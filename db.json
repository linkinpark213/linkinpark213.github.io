{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/files/tensor_chain.py","path":"files/tensor_chain.py","modified":0,"renderable":0},{"_id":"themes/concise/source/CNAME","path":"CNAME","modified":0,"renderable":1},{"_id":"source/images/resnet/downgrade.png","path":"images/resnet/downgrade.png","modified":0,"renderable":0},{"_id":"source/images/resnet/identity_mapping.png","path":"images/resnet/identity_mapping.png","modified":0,"renderable":0},{"_id":"source/images/tftutorial/op.png","path":"images/tftutorial/op.png","modified":0,"renderable":0},{"_id":"source/images/smartypants/apostrophe.png","path":"images/smartypants/apostrophe.png","modified":0,"renderable":0},{"_id":"source/images/tftutorial/oyo.gif","path":"images/tftutorial/oyo.gif","modified":0,"renderable":0},{"_id":"source/images/tftutorial/variable.png","path":"images/tftutorial/variable.png","modified":0,"renderable":0},{"_id":"themes/concise/source/css/archive.less","path":"css/archive.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/category.less","path":"css/category.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/article.less","path":"css/article.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/footer.less","path":"css/footer.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/aside.less","path":"css/aside.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/app.less","path":"css/app.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/icons.less","path":"css/icons.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/markdown.less","path":"css/markdown.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/mixin.less","path":"css/mixin.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/monokai-sublime.less","path":"css/monokai-sublime.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/nav.less","path":"css/nav.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/page.less","path":"css/page.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/gitalk.less","path":"css/gitalk.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/reset.less","path":"css/reset.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/vars.less","path":"css/vars.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/tags.less","path":"css/tags.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/widget.less","path":"css/widget.less","modified":0,"renderable":1},{"_id":"themes/concise/source/css/style.less","path":"css/style.less","modified":0,"renderable":1},{"_id":"themes/concise/source/fonts/icomoon.eot","path":"fonts/icomoon.eot","modified":0,"renderable":1},{"_id":"themes/concise/source/fonts/icomoon.svg","path":"fonts/icomoon.svg","modified":0,"renderable":1},{"_id":"themes/concise/source/fonts/icomoon.ttf","path":"fonts/icomoon.ttf","modified":0,"renderable":1},{"_id":"themes/concise/source/fonts/icomoon.woff","path":"fonts/icomoon.woff","modified":0,"renderable":1},{"_id":"themes/concise/source/js/scrollspy.min.js","path":"js/scrollspy.min.js","modified":0,"renderable":1},{"_id":"source/images/resnet/activations-experiment.png","path":"images/resnet/activations-experiment.png","modified":0,"renderable":0},{"_id":"source/images/resnet/residual_blocks.png","path":"images/resnet/residual_blocks.png","modified":0,"renderable":0},{"_id":"source/images/tftutorial/loss.png","path":"images/tftutorial/loss.png","modified":0,"renderable":0},{"_id":"source/images/tftutorial/resnet56.png","path":"images/tftutorial/resnet56.png","modified":0,"renderable":0},{"_id":"source/images/vgg/conv1.png","path":"images/vgg/conv1.png","modified":0,"renderable":0},{"_id":"source/images/vgg/conv2.png","path":"images/vgg/conv2.png","modified":0,"renderable":0},{"_id":"themes/concise/source/images/favicon.png","path":"images/favicon.png","modified":0,"renderable":1},{"_id":"source/images/resnet/activations.png","path":"images/resnet/activations.png","modified":0,"renderable":0},{"_id":"source/images/resnet/shortcut-connections-experiment.png","path":"images/resnet/shortcut-connections-experiment.png","modified":0,"renderable":0},{"_id":"source/images/resnet/pre-activation.png","path":"images/resnet/pre-activation.png","modified":0,"renderable":0},{"_id":"source/images/resnet/shortcut-connections.png","path":"images/resnet/shortcut-connections.png","modified":0,"renderable":0},{"_id":"source/images/resnet/training.png","path":"images/resnet/training.png","modified":0,"renderable":0},{"_id":"source/images/resnet/resnet-yooo.jpg","path":"images/resnet/resnet-yooo.jpg","modified":0,"renderable":0},{"_id":"source/images/vgg/alexnet.png","path":"images/vgg/alexnet.png","modified":0,"renderable":0},{"_id":"source/images/vgg/lenet.png","path":"images/vgg/lenet.png","modified":0,"renderable":0},{"_id":"source/images/resnet/network_growth.jpg","path":"images/resnet/network_growth.jpg","modified":0,"renderable":0},{"_id":"source/images/vgg/VGG-performance-comparison.png","path":"images/vgg/VGG-performance-comparison.png","modified":0,"renderable":0},{"_id":"source/images/resnet/architectures.png","path":"images/resnet/architectures.png","modified":0,"renderable":0},{"_id":"source/images/vgg/vggnets.png","path":"images/vgg/vggnets.png","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-20.jpg","path":"images/hello-osaka/osaka-20.jpg","modified":0,"renderable":0},{"_id":"source/images/tftutorial/train.png","path":"images/tftutorial/train.png","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-19.jpg","path":"images/hello-osaka/osaka-19.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-6.jpg","path":"images/hello-osaka/osaka-6.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-15.jpg","path":"images/hello-osaka/osaka-15.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-9.jpg","path":"images/hello-osaka/osaka-9.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-13.jpg","path":"images/hello-osaka/osaka-13.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-5.jpg","path":"images/hello-osaka/osaka-5.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-1.jpg","path":"images/hello-osaka/osaka-1.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-12.jpg","path":"images/hello-osaka/osaka-12.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-8.jpg","path":"images/hello-osaka/osaka-8.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-2.jpg","path":"images/hello-osaka/osaka-2.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-18.jpg","path":"images/hello-osaka/osaka-18.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-11.jpg","path":"images/hello-osaka/osaka-11.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-14.jpg","path":"images/hello-osaka/osaka-14.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-3.jpg","path":"images/hello-osaka/osaka-3.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-17.jpg","path":"images/hello-osaka/osaka-17.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-7.jpg","path":"images/hello-osaka/osaka-7.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-10.jpg","path":"images/hello-osaka/osaka-10.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-16.jpg","path":"images/hello-osaka/osaka-16.jpg","modified":0,"renderable":0},{"_id":"source/images/hello-osaka/osaka-4.jpg","path":"images/hello-osaka/osaka-4.jpg","modified":0,"renderable":0}],"Cache":[{"_id":"themes/concise/README.md","hash":"896893bc54b6508e7b3c8b48f4f6d8bac752ee81","modified":1518505949892},{"_id":"themes/concise/.gitignore","hash":"af6004e7a1c722b72af428f4afe08abca4b379b1","modified":1518505949892},{"_id":"themes/concise/LICENSE","hash":"82ce1e15ddeabeaaca60e2186b5a3ce42b1a9c49","modified":1518505949892},{"_id":"themes/concise/_config.example.yml","hash":"dbf598b31ef611d697ec52ab8bc1892395a92052","modified":1518505949892},{"_id":"themes/concise/_config.yml","hash":"71d27bf48e2d01f9787a05612ee59b9f282546ef","modified":1532572106475},{"_id":"source/_drafts/diy-tensorchain.md","hash":"dd9b15b8435da770e57480c25051584410dba118","modified":1524552185125},{"_id":"source/_posts/diy-resnet.md","hash":"ffe1b86a91514fea272f010d5818636aa4ee7bbb","modified":1525010743898},{"_id":"source/_posts/linkinpark213-day.md","hash":"509857f08489f237566160322f4eb7f1a69faabf","modified":1521548592255},{"_id":"source/_posts/mathjax.md","hash":"981d4724a1bb680351efa3c26431bca2820524f1","modified":1524560351947},{"_id":"source/_posts/smartypants.md","hash":"200e0166832cf8d7279ff8be42ca2f7fe3cf82ea","modified":1524410112339},{"_id":"source/_posts/resnet.md","hash":"0a36e1fbf925d7ca2897acd80b1fd3fb162a6071","modified":1524558196165},{"_id":"themes/concise/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1518505949845},{"_id":"themes/concise/.git/config","hash":"1d4b6b4c13fc17099193a55a0459a8326976f85d","modified":1518505949861},{"_id":"themes/concise/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1518505941995},{"_id":"source/about/index.md","hash":"3684f408ee64e1d0d8fce8d4438193153a2f54c6","modified":1532572119095},{"_id":"source/_posts/hello-osaka.md","hash":"a956d7431b85051db62aaba2df3ebebdab102cd7","modified":1532611341650},{"_id":"themes/concise/.git/packed-refs","hash":"67fb03b1dfaa4bee0d2f02d99a3496bf16f9fb70","modified":1518505949814},{"_id":"source/_posts/vgg.md","hash":"3f98cd1289918c8658aaa7382e29a03b27bc1426","modified":1528205883436},{"_id":"themes/concise/.git/index","hash":"3a0b9c14d741bfa64a0dae2da0a8daf1b29aa485","modified":1518505949955},{"_id":"themes/concise/languages/ja-jp.yml","hash":"ad8d3d27b4dba679b46990195af5886fcf1fb734","modified":1524726332903},{"_id":"themes/concise/languages/default.yml","hash":"aa22dc162fc76a36bb62860096e057a52d79196e","modified":1524726349847},{"_id":"themes/concise/languages/zh-CN.yml","hash":"50a8c9b15be3e5512dadf2583a1a38cf94578bb1","modified":1524726625050},{"_id":"themes/concise/layout/categories.ejs","hash":"453ec20e5985487dd2eaa0a32003e2ec4495c391","modified":1518505949908},{"_id":"themes/concise/layout/index.ejs","hash":"e29542322fe798281a40eed476468aa8afba7d50","modified":1518505949923},{"_id":"themes/concise/layout/category.ejs","hash":"e29542322fe798281a40eed476468aa8afba7d50","modified":1518505949908},{"_id":"themes/concise/layout/page.ejs","hash":"f87550d14cc73fadfb95e8e8040d97640cfa4923","modified":1518612137058},{"_id":"themes/concise/layout/tag.ejs","hash":"e29542322fe798281a40eed476468aa8afba7d50","modified":1518505949923},{"_id":"source/files/tensor_chain.py","hash":"bcd3700100ccdda59d29748dac0e6502a6eb0546","modified":1524916672261},{"_id":"themes/concise/layout/archive.ejs","hash":"3e4d0c3fa27aacb7ecf7c7d93b3c603087caf3a8","modified":1518505949908},{"_id":"themes/concise/source/CNAME","hash":"87ad9ee278279eb9ce26622039fc9a96b4b65c81","modified":1521558121955},{"_id":"themes/concise/layout/post.ejs","hash":"df2a2fe6a17bec6a99c6494148517e5610e76e93","modified":1518612156394},{"_id":"themes/concise/preview.png","hash":"3dd563ba6885b21e85e721daa4e038f748585f8c","modified":1518505949923},{"_id":"themes/concise/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1518505941995},{"_id":"themes/concise/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1518505941995},{"_id":"themes/concise/layout/layout.ejs","hash":"20f3e6e680b21af31b734af40ec6eb2b71d47dfd","modified":1518519237734},{"_id":"themes/concise/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1518505941995},{"_id":"themes/concise/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1518505941995},{"_id":"themes/concise/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1518505941995},{"_id":"themes/concise/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1518505941995},{"_id":"themes/concise/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1518505942011},{"_id":"themes/concise/.git/hooks/pre-rebase.sample","hash":"18be3eb275c1decd3614e139f5a311b75f1b0ab8","modified":1518505941995},{"_id":"themes/concise/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1518505941995},{"_id":"themes/concise/.git/hooks/update.sample","hash":"e729cd61b27c128951d139de8e7c63d1a3758dde","modified":1518505942011},{"_id":"source/images/resnet/downgrade.png","hash":"75143520d0b8abc3d071c6bd43341081d5d82072","modified":1523155909947},{"_id":"themes/concise/.git/logs/HEAD","hash":"b1b21ac15a85deda256fb45bc0736c09edf51c10","modified":1518505949845},{"_id":"source/images/resnet/identity_mapping.png","hash":"ff7e65b6867a3975b0fb85a61261282bf0da233f","modified":1524386839391},{"_id":"themes/concise/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1518505942011},{"_id":"source/images/tftutorial/op.png","hash":"09bbaeb37b975d1e0411051f80fb3ac61af01dbe","modified":1525003951213},{"_id":"source/images/smartypants/apostrophe.png","hash":"97219c36fec83eac54499d2211c47f130732d2a2","modified":1524300618701},{"_id":"source/images/tftutorial/oyo.gif","hash":"5d1a50a925b688315425c7f45f0b568aaed9fea2","modified":1502619792819},{"_id":"source/images/tftutorial/variable.png","hash":"09bbaeb37b975d1e0411051f80fb3ac61af01dbe","modified":1525003951213},{"_id":"themes/concise/layout/_partial/archive.ejs","hash":"4f3cdb19da78ac7d42d37fb4d152988f99ea74f4","modified":1518505949892},{"_id":"themes/concise/layout/_partial/articles.ejs","hash":"3ca4869df35f85c39997a99ea32b3366e02de34a","modified":1518612145743},{"_id":"themes/concise/layout/_partial/comments.ejs","hash":"30fbd53f1caa373c09f79be1c8194716cc4ecb38","modified":1525015642768},{"_id":"themes/concise/layout/_partial/footer.ejs","hash":"13d506ac2d63bd649bd5a0c732e36c9f4d2d96f6","modified":1518505949908},{"_id":"themes/concise/layout/_partial/header.ejs","hash":"4b3a9262654ef3bc91fd9067e0db20866bb18da8","modified":1518505949908},{"_id":"themes/concise/layout/_widget/archive.ejs","hash":"e1bdddeaada31c00aa67278fcc50ad845ecf1722","modified":1518505949908},{"_id":"themes/concise/layout/_widget/category.ejs","hash":"866790acc13fed44b7ef74c3e19c300a3d6180d8","modified":1518505949908},{"_id":"themes/concise/layout/_partial/aside.ejs","hash":"69ecfed5f8a877bf7b3eec834d4244b8a38c6e29","modified":1521526722253},{"_id":"themes/concise/layout/_partial/article.ejs","hash":"a46443b099972a06f4a44566f2b04ea4ca44d0b7","modified":1524727653347},{"_id":"themes/concise/layout/_widget/recent_posts.ejs","hash":"16800f85ffb036d2644a26e02facd61acb3706e9","modified":1518505949908},{"_id":"themes/concise/layout/_partial/nav.ejs","hash":"bd79439750262f5ee910e2b7b99060570364d7b1","modified":1524475708531},{"_id":"themes/concise/source/css/archive.less","hash":"acf97e6d82c7bce8591cde5730fcecdc70c9c6c6","modified":1518505949923},{"_id":"themes/concise/source/css/category.less","hash":"9288664fef128c5527078d6c742611c370de08bd","modified":1518505949923},{"_id":"themes/concise/source/css/article.less","hash":"4426fcf2b4143413298b18309cd6be9e683e59d9","modified":1524730845867},{"_id":"themes/concise/layout/_widget/tagcloud.ejs","hash":"7259c179aa0c41c02e467ad892292e90430aaabc","modified":1518505949908},{"_id":"themes/concise/source/css/footer.less","hash":"9798524e6235b96cc0235c77c436c38c41e9b6f7","modified":1518505949923},{"_id":"themes/concise/source/css/aside.less","hash":"7c1a25e887ba25174b3854384cce97d0e70f089e","modified":1518505949923},{"_id":"themes/concise/layout/_widget/tag.ejs","hash":"c000ec9f1479f74ec8d0e9378ba61f23f3778431","modified":1518505949908},{"_id":"themes/concise/source/css/app.less","hash":"048c0c1bfc8034d6f4a824d6c06fe0bfdb5c09d9","modified":1518505949923},{"_id":"themes/concise/source/css/icons.less","hash":"2be8a47cf73ea97e3ca5918f20d97756d340e46d","modified":1518509528594},{"_id":"themes/concise/source/css/markdown.less","hash":"56adc0573ebe6708fae370ade0411124df317c20","modified":1524387564700},{"_id":"themes/concise/source/css/mixin.less","hash":"d3d7673330e60d0294ac8a806ef697fa89e906f0","modified":1518505949939},{"_id":"themes/concise/source/css/monokai-sublime.less","hash":"9b36b183a224d784dcb4655be909368cd68ba1f0","modified":1518505949939},{"_id":"themes/concise/source/css/nav.less","hash":"4fa74995bbaf8bc314f4c1f05de87e1a0a34620f","modified":1518505949939},{"_id":"themes/concise/source/css/page.less","hash":"7610d789fe6f3a43e321682b38159dd405ec299f","modified":1524387599626},{"_id":"themes/concise/source/css/gitalk.less","hash":"e4e0d12e3f08ad15ee68de222045725d00f1036e","modified":1525012797784},{"_id":"themes/concise/source/css/reset.less","hash":"d1535fa425fc1b0aab25397002fb6c1c51538326","modified":1524730830555},{"_id":"themes/concise/source/css/vars.less","hash":"336ac69d2666581716602f3b353c6ec34f6df623","modified":1518505949939},{"_id":"themes/concise/source/css/tags.less","hash":"92edb713370078cf4757e8546555f91ab7632614","modified":1518505949939},{"_id":"themes/concise/source/css/widget.less","hash":"8373c2246dfb0261278003f1985a53707494e0b9","modified":1518505949939},{"_id":"themes/concise/source/css/style.less","hash":"43146606459315a9d27d04c88a1e2d00194d080a","modified":1525012143810},{"_id":"themes/concise/source/fonts/icomoon.eot","hash":"8300564edcd12262c6b8b97ba225bc00b72f5984","modified":1518505949939},{"_id":"themes/concise/source/fonts/icomoon.svg","hash":"821f98d8cdec4dbef208ed6f66c54e7b718fdd6a","modified":1518505949939},{"_id":"themes/concise/source/fonts/icomoon.ttf","hash":"6b577c657ccb5de7c321e51942a52922a4e95ec1","modified":1518505949939},{"_id":"themes/concise/source/fonts/icomoon.woff","hash":"5a3b86fa1122f78d9b285fbd390bf142721c51ed","modified":1518505949939},{"_id":"themes/concise/source/js/scrollspy.min.js","hash":"50fee51d681739bbaf0dd65fddc36d64d951ece9","modified":1518505949955},{"_id":"source/images/resnet/activations-experiment.png","hash":"2d11a22f89779c36f347c5fd9946dcd7df5b2c17","modified":1524404347429},{"_id":"source/images/resnet/residual_blocks.png","hash":"8ba126be7c0bedefa3980c24098ba71190ee50d0","modified":1524383660837},{"_id":"source/images/tftutorial/loss.png","hash":"a459ee1446ae988715e8656ddec27a3bfbb37eb1","modified":1524918087587},{"_id":"source/images/tftutorial/resnet56.png","hash":"800358dc64b3819d9fd6784df7630c427bd9177e","modified":1524920745164},{"_id":"source/images/vgg/conv1.png","hash":"0f400ee8616672b7c5404140a6afcb41bb843bb0","modified":1524309931924},{"_id":"source/images/vgg/conv2.png","hash":"6c28de6b3b875deca0c2bf82e4b977d3b332d8a0","modified":1524309923442},{"_id":"themes/concise/source/images/favicon.png","hash":"e1c0fb88ab6ed405b64009b6f3ce1d9a38f0e5fc","modified":1518511578050},{"_id":"source/images/resnet/activations.png","hash":"484fd30fac3a5ccb641517618e89584101fe2152","modified":1524404320995},{"_id":"source/images/resnet/shortcut-connections-experiment.png","hash":"eb17c12e8e2580fdf6a698465e1a4ba85d4df1e4","modified":1524403538939},{"_id":"source/images/resnet/pre-activation.png","hash":"e5a48d48c13e1aa75cafa6cde3abe38b5e2e65ca","modified":1524398733974},{"_id":"source/images/resnet/shortcut-connections.png","hash":"4da28d1804b67f4a75f58cdcb9cca8a55949f276","modified":1524403518525},{"_id":"source/images/resnet/training.png","hash":"161ca1d9b76900b889f71ff47cd61d1d85930d0d","modified":1524402642561},{"_id":"source/images/resnet/resnet-yooo.jpg","hash":"fd4cd53f877a8855d36a207c87dc7386eb82285b","modified":1524399761065},{"_id":"source/images/vgg/alexnet.png","hash":"33ea253bf43f9bb7530aa3ecae27f8955ebbd4ae","modified":1524410046009},{"_id":"themes/concise/.git/objects/pack/pack-39d23ee8869e43759e3d66dc83ca36e215254fd4.idx","hash":"4158b1e2b91e9a500c0db9c9c08a9760ace4eb80","modified":1518505949720},{"_id":"source/images/vgg/lenet.png","hash":"1ef5f1e5e1f005813599142f42daca34a27627fb","modified":1524311811317},{"_id":"themes/concise/.git/refs/heads/master","hash":"d5da25e7424cc42ca99593e9341b82778f2f7f90","modified":1518505949845},{"_id":"themes/concise/layout/_partial/post/date.ejs","hash":"891db6745bac06df4be5a0f1cd69da1c0f90596a","modified":1518505949908},{"_id":"themes/concise/layout/_partial/post/tags.ejs","hash":"24abef606b55a9dda97dca79bb8ab46968919423","modified":1518505949908},{"_id":"themes/concise/.git/logs/refs/heads/master","hash":"b1b21ac15a85deda256fb45bc0736c09edf51c10","modified":1518505949845},{"_id":"source/images/resnet/network_growth.jpg","hash":"4e3dbad7c70f55fb50f2e2cc3e01cb256f5050d1","modified":1524472630075},{"_id":"themes/concise/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1518505949830},{"_id":"source/images/vgg/VGG-performance-comparison.png","hash":"9415026d608fdaa33d7e602777bf353004f6859e","modified":1524378970853},{"_id":"source/images/resnet/architectures.png","hash":"b9f1a252b195e62fa3c8bb56d23ef9c61d6f05f1","modified":1524401416454},{"_id":"source/images/vgg/vggnets.png","hash":"6261a75847edf0f2284ee6787ab8a9978649c129","modified":1524318873869},{"_id":"themes/concise/.git/logs/refs/remotes/origin/HEAD","hash":"b1b21ac15a85deda256fb45bc0736c09edf51c10","modified":1518505949830},{"_id":"source/images/hello-osaka/osaka-20.jpg","hash":"8adcd789e85e4934bfbac45fd1c4f7e44e41529f","modified":1532571207040},{"_id":"themes/concise/.git/objects/pack/pack-39d23ee8869e43759e3d66dc83ca36e215254fd4.pack","hash":"9a245eedd3d75684fde930dd68c8e430dec9ee52","modified":1518505949736},{"_id":"source/images/tftutorial/train.png","hash":"17607465dd2fefd9179de3f9fb3e38064d440f51","modified":1525004287181},{"_id":"source/images/hello-osaka/osaka-19.jpg","hash":"36a4c7e346df670238c2c19002d20c10f63b8b0a","modified":1532571207337},{"_id":"source/images/hello-osaka/osaka-6.jpg","hash":"24f261be4b186842f7865e3524e24ab4643fdbba","modified":1532571207333},{"_id":"source/images/hello-osaka/osaka-15.jpg","hash":"a441cf1d1b5e09f37c1c68960ef77e6ce3fa6af4","modified":1532571207456},{"_id":"source/images/hello-osaka/osaka-9.jpg","hash":"95ee1e06f82df419622c34353a2994d6d50de5c6","modified":1532571207488},{"_id":"source/images/hello-osaka/osaka-13.jpg","hash":"1c14b4aedc531cc82725176544288d7d7a71eb88","modified":1532571207502},{"_id":"source/images/hello-osaka/osaka-5.jpg","hash":"e51cf4dbb75026235bd83ec44a7e2b8c757322ab","modified":1532571207503},{"_id":"source/images/hello-osaka/osaka-1.jpg","hash":"0405116ebc9737fcbb9d00157ad03dfcd6c24d9f","modified":1532571207520},{"_id":"source/images/hello-osaka/osaka-12.jpg","hash":"f6b475004ca5451f29fdec83e00116c0b831d92a","modified":1532571207552},{"_id":"source/images/hello-osaka/osaka-8.jpg","hash":"0c73562e9aee799d5a0e428c23790781ac3b5abf","modified":1532571207555},{"_id":"source/images/hello-osaka/osaka-2.jpg","hash":"ab5fe9b6f1ee31a3c7e0dc97890c3ccde5bf9781","modified":1532571207571},{"_id":"source/images/hello-osaka/osaka-18.jpg","hash":"29437344c3036c2e8b9814bc4b934f37adbabcfd","modified":1532571207573},{"_id":"source/images/hello-osaka/osaka-11.jpg","hash":"891b139bc73fe270aac0d6c225bb1879a29e15f4","modified":1532571207597},{"_id":"source/images/hello-osaka/osaka-14.jpg","hash":"6844e3e3f866ef60b72f3c30678520ce86a3dcf4","modified":1532571207670},{"_id":"source/images/hello-osaka/osaka-3.jpg","hash":"e3151c54f0b33e0dc11e8644f778a1620ab545ba","modified":1532571207679},{"_id":"source/images/hello-osaka/osaka-17.jpg","hash":"b487fa28bdd805a6127dfa08a5b431a48be018c9","modified":1532571207697},{"_id":"source/images/hello-osaka/osaka-7.jpg","hash":"f3d2b205243d691ff39e584f95bce0e3d5e75fd2","modified":1532571207702},{"_id":"source/images/hello-osaka/osaka-10.jpg","hash":"6129073ca834ba09adf8cf25406d4e2c2d10f0d8","modified":1532571207706},{"_id":"source/images/hello-osaka/osaka-16.jpg","hash":"efc4e6a8ead520e3f9ae1d7dd424cf6735a5b35b","modified":1532571207819},{"_id":"source/images/hello-osaka/osaka-4.jpg","hash":"7576736cc2d65b82e831472536842ee0f1db6fe1","modified":1532571207817},{"_id":"public/atom.xml","hash":"2e7be712cbb9d19d71ec9ed309e4706d3c28a592","modified":1542518315315},{"_id":"public/about/index.html","hash":"d365a163b9e1b3f8cd40ba8cbea95f51d6c6a51f","modified":1542518315668},{"_id":"public/2018/07/26/hello-osaka/index.html","hash":"e1e8999614ab2dc6819ed3a492434e55dc8d0c9b","modified":1542518315708},{"_id":"public/2018/02/13/linkinpark213-day/index.html","hash":"c641bb5b20ddf0ea62707e18f5a450e863199b06","modified":1542518315708},{"_id":"public/2018/03/20/smartypants/index.html","hash":"8cb785c9f49f225f18ea46404c62dc4cbf03fc37","modified":1542518315716},{"_id":"public/archives/index.html","hash":"bfcadb343740652b8125c1ba3cec6d54b909d53e","modified":1542518315716},{"_id":"public/archives/2018/02/index.html","hash":"08a9974a69739ab130633c8db5144ab3f5dd7407","modified":1542518315716},{"_id":"public/archives/2018/index.html","hash":"988938e3b23ad5c014ff69d9929c214e60b35212","modified":1542518315716},{"_id":"public/archives/2018/04/index.html","hash":"501b532ac9d9e24d503ba4532deca3e10e017a9d","modified":1542518315716},{"_id":"public/archives/2018/07/index.html","hash":"25135013b399681802845e27111264158f4da0fa","modified":1542518315716},{"_id":"public/archives/2018/03/index.html","hash":"63cc77b6e9778c09eb21077da357d3ec3d3b2b98","modified":1542518315718},{"_id":"public/2018/04/29/diy-resnet/index.html","hash":"df1221fad7014eb1dc51739eb74ff31557d18013","modified":1542518315718},{"_id":"public/2018/04/21/vgg/index.html","hash":"3818bbd67ceb55281c0d06b68d6a8de784f8c6fb","modified":1542518315718},{"_id":"public/2018/04/22/resnet/index.html","hash":"856411599c77cd9f33bf70dec8d34fad0f0e10d3","modified":1542518315718},{"_id":"public/2018/04/24/mathjax/index.html","hash":"1ca6c4979ea0ed9a87ddb2318140ec5fd2f4c49c","modified":1542518315718},{"_id":"public/index.html","hash":"ba33acb7ad4c3ec48823410873d76bcd6d601fd7","modified":1542518315718},{"_id":"public/tags/Deep-Learning/index.html","hash":"9ebe7c44eaf46f8eb13c0386dc17cb3117e1db2e","modified":1542518315789},{"_id":"public/tags/DIY/index.html","hash":"8d6aa9f072434cc3561f2db40f6e320a0cbfb04d","modified":1542518315789},{"_id":"public/tags/Computer-Vision/index.html","hash":"034e3e77af2d0f6abd0c68bdc079df8683b02351","modified":1542518315789},{"_id":"public/tags/Travel-Gallery/index.html","hash":"6f643cd9b6f8701cfdcdda584734522fc76d1329","modified":1542518315789},{"_id":"public/tags/Blogging/index.html","hash":"b80bf82a512c3cf906826ab408e132ddb5458409","modified":1542518315789},{"_id":"public/tags/Other/index.html","hash":"ff311fef141a4a10aeec1f334d9bbb3a1b79e00e","modified":1542518315789},{"_id":"public/files/tensor_chain.py","hash":"bcd3700100ccdda59d29748dac0e6502a6eb0546","modified":1542518315809},{"_id":"public/CNAME","hash":"87ad9ee278279eb9ce26622039fc9a96b4b65c81","modified":1542518315809},{"_id":"public/images/smartypants/apostrophe.png","hash":"97219c36fec83eac54499d2211c47f130732d2a2","modified":1542518315809},{"_id":"public/images/resnet/downgrade.png","hash":"75143520d0b8abc3d071c6bd43341081d5d82072","modified":1542518315817},{"_id":"public/images/resnet/identity_mapping.png","hash":"ff7e65b6867a3975b0fb85a61261282bf0da233f","modified":1542518315817},{"_id":"public/images/tftutorial/op.png","hash":"09bbaeb37b975d1e0411051f80fb3ac61af01dbe","modified":1542518315817},{"_id":"public/images/tftutorial/oyo.gif","hash":"5d1a50a925b688315425c7f45f0b568aaed9fea2","modified":1542518315817},{"_id":"public/images/tftutorial/variable.png","hash":"09bbaeb37b975d1e0411051f80fb3ac61af01dbe","modified":1542518315817},{"_id":"public/fonts/icomoon.eot","hash":"8300564edcd12262c6b8b97ba225bc00b72f5984","modified":1542518315819},{"_id":"public/fonts/icomoon.svg","hash":"821f98d8cdec4dbef208ed6f66c54e7b718fdd6a","modified":1542518315819},{"_id":"public/fonts/icomoon.ttf","hash":"6b577c657ccb5de7c321e51942a52922a4e95ec1","modified":1542518315819},{"_id":"public/fonts/icomoon.woff","hash":"5a3b86fa1122f78d9b285fbd390bf142721c51ed","modified":1542518315819},{"_id":"public/images/tftutorial/loss.png","hash":"a459ee1446ae988715e8656ddec27a3bfbb37eb1","modified":1542518316051},{"_id":"public/images/resnet/residual_blocks.png","hash":"8ba126be7c0bedefa3980c24098ba71190ee50d0","modified":1542518316051},{"_id":"public/images/tftutorial/resnet56.png","hash":"800358dc64b3819d9fd6784df7630c427bd9177e","modified":1542518316051},{"_id":"public/images/favicon.png","hash":"e1c0fb88ab6ed405b64009b6f3ce1d9a38f0e5fc","modified":1542518316051},{"_id":"public/images/vgg/conv2.png","hash":"6c28de6b3b875deca0c2bf82e4b977d3b332d8a0","modified":1542518316051},{"_id":"public/images/vgg/conv1.png","hash":"0f400ee8616672b7c5404140a6afcb41bb843bb0","modified":1542518316051},{"_id":"public/images/resnet/activations-experiment.png","hash":"2d11a22f89779c36f347c5fd9946dcd7df5b2c17","modified":1542518316051},{"_id":"public/js/scrollspy.min.js","hash":"076f7db44c3a05147144f736cae159baf1612f72","modified":1542518316071},{"_id":"public/images/resnet/activations.png","hash":"484fd30fac3a5ccb641517618e89584101fe2152","modified":1542518316079},{"_id":"public/images/resnet/shortcut-connections-experiment.png","hash":"eb17c12e8e2580fdf6a698465e1a4ba85d4df1e4","modified":1542518316079},{"_id":"public/images/resnet/pre-activation.png","hash":"e5a48d48c13e1aa75cafa6cde3abe38b5e2e65ca","modified":1542518316081},{"_id":"public/images/resnet/shortcut-connections.png","hash":"4da28d1804b67f4a75f58cdcb9cca8a55949f276","modified":1542518316081},{"_id":"public/images/vgg/alexnet.png","hash":"33ea253bf43f9bb7530aa3ecae27f8955ebbd4ae","modified":1542518316081},{"_id":"public/images/resnet/training.png","hash":"161ca1d9b76900b889f71ff47cd61d1d85930d0d","modified":1542518316081},{"_id":"public/images/resnet/resnet-yooo.jpg","hash":"fd4cd53f877a8855d36a207c87dc7386eb82285b","modified":1542518316081},{"_id":"public/images/vgg/lenet.png","hash":"1ef5f1e5e1f005813599142f42daca34a27627fb","modified":1542518316081},{"_id":"public/css/icons.css","hash":"73d6e22911def7d325314e382aada50f7dcff31b","modified":1542518316099},{"_id":"public/css/markdown.css","hash":"2103453fdf4ff9eda29ca689ad037ea91d412dbd","modified":1542518316099},{"_id":"public/css/mixin.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1542518316101},{"_id":"public/css/monokai-sublime.css","hash":"f087de3a7a0f2e3aae36633eb4a46e38e1da3ea2","modified":1542518316101},{"_id":"public/css/vars.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1542518316101},{"_id":"public/css/gitalk.css","hash":"c1471df162585d0cfc10113e5a7c7d7560e7d642","modified":1542518316101},{"_id":"public/images/resnet/network_growth.jpg","hash":"4e3dbad7c70f55fb50f2e2cc3e01cb256f5050d1","modified":1542518316121},{"_id":"public/images/vgg/VGG-performance-comparison.png","hash":"9415026d608fdaa33d7e602777bf353004f6859e","modified":1542518316121},{"_id":"public/images/resnet/architectures.png","hash":"b9f1a252b195e62fa3c8bb56d23ef9c61d6f05f1","modified":1542518316141},{"_id":"public/images/vgg/vggnets.png","hash":"6261a75847edf0f2284ee6787ab8a9978649c129","modified":1542518316141},{"_id":"public/images/hello-osaka/osaka-20.jpg","hash":"8adcd789e85e4934bfbac45fd1c4f7e44e41529f","modified":1542518316159},{"_id":"public/images/tftutorial/train.png","hash":"17607465dd2fefd9179de3f9fb3e38064d440f51","modified":1542518316342},{"_id":"public/css/category.css","hash":"b6059959855d73fc8eac634939be5322927589b1","modified":1542518316373},{"_id":"public/css/archive.css","hash":"b81b3eb3940353d87245e3edd01b479edb59064e","modified":1542518316373},{"_id":"public/css/footer.css","hash":"0f50543ca0f5c0c29d0a11b934c61467ddfb7fc6","modified":1542518316373},{"_id":"public/css/article.css","hash":"368ef785bd19398366603a847a2bd15ba5ce9da7","modified":1542518316373},{"_id":"public/css/aside.css","hash":"bff16cf949d13cedc0c03a68f1c1109183732a97","modified":1542518316373},{"_id":"public/css/app.css","hash":"85f5d34a44d3cf9b241d9a0de10490ed32865b6c","modified":1542518316373},{"_id":"public/css/nav.css","hash":"f25dcc8be7f31f71f84c67d62a41577238e5974d","modified":1542518316373},{"_id":"public/css/tags.css","hash":"c6618b8da67259a3bb105adb9841b6b5e3dda51f","modified":1542518316373},{"_id":"public/css/page.css","hash":"b6b0c0c9d2d5753b5f8c7e63c88c7c15ca8e570e","modified":1542518316381},{"_id":"public/css/widget.css","hash":"0becbaab3f03488d4bfe57d0f4c759592b241bc0","modified":1542518316383},{"_id":"public/css/reset.css","hash":"7fb3892fc2bb3f612afe6ad0eacd53df1183fcc9","modified":1542518316393},{"_id":"public/css/style.css","hash":"34ad40cbd517da584e04444ca0b2eafe94b2ec1d","modified":1542518316673},{"_id":"public/images/hello-osaka/osaka-19.jpg","hash":"36a4c7e346df670238c2c19002d20c10f63b8b0a","modified":1542518316874},{"_id":"public/images/hello-osaka/osaka-6.jpg","hash":"24f261be4b186842f7865e3524e24ab4643fdbba","modified":1542518316874},{"_id":"public/images/hello-osaka/osaka-15.jpg","hash":"a441cf1d1b5e09f37c1c68960ef77e6ce3fa6af4","modified":1542518316987},{"_id":"public/images/hello-osaka/osaka-9.jpg","hash":"95ee1e06f82df419622c34353a2994d6d50de5c6","modified":1542518317088},{"_id":"public/images/hello-osaka/osaka-13.jpg","hash":"1c14b4aedc531cc82725176544288d7d7a71eb88","modified":1542518317106},{"_id":"public/images/hello-osaka/osaka-5.jpg","hash":"e51cf4dbb75026235bd83ec44a7e2b8c757322ab","modified":1542518317118},{"_id":"public/images/hello-osaka/osaka-1.jpg","hash":"0405116ebc9737fcbb9d00157ad03dfcd6c24d9f","modified":1542518317128},{"_id":"public/images/hello-osaka/osaka-12.jpg","hash":"f6b475004ca5451f29fdec83e00116c0b831d92a","modified":1542518317209},{"_id":"public/images/hello-osaka/osaka-8.jpg","hash":"0c73562e9aee799d5a0e428c23790781ac3b5abf","modified":1542518317227},{"_id":"public/images/hello-osaka/osaka-2.jpg","hash":"ab5fe9b6f1ee31a3c7e0dc97890c3ccde5bf9781","modified":1542518317237},{"_id":"public/images/hello-osaka/osaka-18.jpg","hash":"29437344c3036c2e8b9814bc4b934f37adbabcfd","modified":1542518317249},{"_id":"public/images/hello-osaka/osaka-11.jpg","hash":"891b139bc73fe270aac0d6c225bb1879a29e15f4","modified":1542518317259},{"_id":"public/images/hello-osaka/osaka-14.jpg","hash":"6844e3e3f866ef60b72f3c30678520ce86a3dcf4","modified":1542518317269},{"_id":"public/images/hello-osaka/osaka-3.jpg","hash":"e3151c54f0b33e0dc11e8644f778a1620ab545ba","modified":1542518317360},{"_id":"public/images/hello-osaka/osaka-17.jpg","hash":"b487fa28bdd805a6127dfa08a5b431a48be018c9","modified":1542518317371},{"_id":"public/images/hello-osaka/osaka-7.jpg","hash":"f3d2b205243d691ff39e584f95bce0e3d5e75fd2","modified":1542518317389},{"_id":"public/images/hello-osaka/osaka-10.jpg","hash":"6129073ca834ba09adf8cf25406d4e2c2d10f0d8","modified":1542518317401},{"_id":"public/images/hello-osaka/osaka-16.jpg","hash":"efc4e6a8ead520e3f9ae1d7dd424cf6735a5b35b","modified":1542518317492},{"_id":"public/images/hello-osaka/osaka-4.jpg","hash":"7576736cc2d65b82e831472536842ee0f1db6fe1","modified":1542518317510}],"Category":[],"Data":[],"Page":[{"title":"About Me","_content":"\n## Info\n| Nickname          | Harper Long                                                     |\n|-------------------|-----------------------------------------------------------------|\n| Date of Birth     | November 25th, 1996                                             |\n| E-mail            | <linkinpark213@outlook.com>                                     |\n| Occupation        | Student                                                         |\n| Field of Interest | Computer Vision, Deep Learning, Software Engineering, Robotics  |\n| Relationship      | Available                                                       |\n| Motto             | All I want to do is trade this life for something new           |\n\n## Education\n* Currently - M1 student in the Robotics Lab & Robotics Vision Lab of Nara Institute of Science and Technology.\n* Bachelor - Software Engineering School of Xi'an Jiaotong University, June 2018.\n\n## Skills & Abilities\n* Programming in Java, C++ and Python\n* Basic usage of Javascript, PHP and MATLAB\n* Web app programming, from HTML to React\n* Solving problems using basic data structures and algorithms\n* Analyzing and designing software systems\n* Basic knowledge in machine learning and computer vision\n* Designing and training neural networks using Tensorflow\n\n## Experience\n* Internship, Qihoo 360 Corps (Beijing), Summer 2016 & 2017\n* Research Assistant, XJTU Spacial Vision Collaborative Laboratory, 2017","source":"about/index.md","raw":"---\ntitle: About Me\n---\n\n## Info\n| Nickname          | Harper Long                                                     |\n|-------------------|-----------------------------------------------------------------|\n| Date of Birth     | November 25th, 1996                                             |\n| E-mail            | <linkinpark213@outlook.com>                                     |\n| Occupation        | Student                                                         |\n| Field of Interest | Computer Vision, Deep Learning, Software Engineering, Robotics  |\n| Relationship      | Available                                                       |\n| Motto             | All I want to do is trade this life for something new           |\n\n## Education\n* Currently - M1 student in the Robotics Lab & Robotics Vision Lab of Nara Institute of Science and Technology.\n* Bachelor - Software Engineering School of Xi'an Jiaotong University, June 2018.\n\n## Skills & Abilities\n* Programming in Java, C++ and Python\n* Basic usage of Javascript, PHP and MATLAB\n* Web app programming, from HTML to React\n* Solving problems using basic data structures and algorithms\n* Analyzing and designing software systems\n* Basic knowledge in machine learning and computer vision\n* Designing and training neural networks using Tensorflow\n\n## Experience\n* Internship, Qihoo 360 Corps (Beijing), Summer 2016 & 2017\n* Research Assistant, XJTU Spacial Vision Collaborative Laboratory, 2017","date":"2018-07-26T02:28:39.095Z","updated":"2018-07-26T02:28:39.095Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjomfkrfh0001zcvj6gfx9qga","content":"<h2 id=\"Info\"><a href=\"#Info\" class=\"headerlink\" title=\"Info\"></a>Info</h2><table>\n<thead>\n<tr>\n<th>Nickname</th>\n<th>Harper Long</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Date of Birth</td>\n<td>November 25th, 1996</td>\n</tr>\n<tr>\n<td>E-mail</td>\n<td><a href=\"&#109;&#97;&#x69;&#108;&#x74;&#111;&#x3a;&#108;&#105;&#x6e;&#107;&#105;&#110;&#112;&#97;&#x72;&#107;&#50;&#49;&#x33;&#x40;&#x6f;&#x75;&#x74;&#x6c;&#x6f;&#x6f;&#x6b;&#46;&#99;&#x6f;&#109;\">&#108;&#105;&#x6e;&#107;&#105;&#110;&#112;&#97;&#x72;&#107;&#50;&#49;&#x33;&#x40;&#x6f;&#x75;&#x74;&#x6c;&#x6f;&#x6f;&#x6b;&#46;&#99;&#x6f;&#109;</a></td>\n</tr>\n<tr>\n<td>Occupation</td>\n<td>Student</td>\n</tr>\n<tr>\n<td>Field of Interest</td>\n<td>Computer Vision, Deep Learning, Software Engineering, Robotics</td>\n</tr>\n<tr>\n<td>Relationship</td>\n<td>Available</td>\n</tr>\n<tr>\n<td>Motto</td>\n<td>All I want to do is trade this life for something new</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Education\"><a href=\"#Education\" class=\"headerlink\" title=\"Education\"></a>Education</h2><ul>\n<li>Currently - M1 student in the Robotics Lab &amp; Robotics Vision Lab of Nara Institute of Science and Technology.</li>\n<li>Bachelor - Software Engineering School of Xi&#39;an Jiaotong University, June 2018.</li>\n</ul>\n<h2 id=\"Skills-amp-Abilities\"><a href=\"#Skills-amp-Abilities\" class=\"headerlink\" title=\"Skills &amp; Abilities\"></a>Skills &amp; Abilities</h2><ul>\n<li>Programming in Java, C++ and Python</li>\n<li>Basic usage of Javascript, PHP and MATLAB</li>\n<li>Web app programming, from HTML to React</li>\n<li>Solving problems using basic data structures and algorithms</li>\n<li>Analyzing and designing software systems</li>\n<li>Basic knowledge in machine learning and computer vision</li>\n<li>Designing and training neural networks using Tensorflow</li>\n</ul>\n<h2 id=\"Experience\"><a href=\"#Experience\" class=\"headerlink\" title=\"Experience\"></a>Experience</h2><ul>\n<li>Internship, Qihoo 360 Corps (Beijing), Summer 2016 &amp; 2017</li>\n<li>Research Assistant, XJTU Spacial Vision Collaborative Laboratory, 2017</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Info\"><a href=\"#Info\" class=\"headerlink\" title=\"Info\"></a>Info</h2><table>\n<thead>\n<tr>\n<th>Nickname</th>\n<th>Harper Long</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Date of Birth</td>\n<td>November 25th, 1996</td>\n</tr>\n<tr>\n<td>E-mail</td>\n<td><a href=\"&#109;&#97;&#x69;&#108;&#x74;&#111;&#x3a;&#108;&#105;&#x6e;&#107;&#105;&#110;&#112;&#97;&#x72;&#107;&#50;&#49;&#x33;&#x40;&#x6f;&#x75;&#x74;&#x6c;&#x6f;&#x6f;&#x6b;&#46;&#99;&#x6f;&#109;\">&#108;&#105;&#x6e;&#107;&#105;&#110;&#112;&#97;&#x72;&#107;&#50;&#49;&#x33;&#x40;&#x6f;&#x75;&#x74;&#x6c;&#x6f;&#x6f;&#x6b;&#46;&#99;&#x6f;&#109;</a></td>\n</tr>\n<tr>\n<td>Occupation</td>\n<td>Student</td>\n</tr>\n<tr>\n<td>Field of Interest</td>\n<td>Computer Vision, Deep Learning, Software Engineering, Robotics</td>\n</tr>\n<tr>\n<td>Relationship</td>\n<td>Available</td>\n</tr>\n<tr>\n<td>Motto</td>\n<td>All I want to do is trade this life for something new</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Education\"><a href=\"#Education\" class=\"headerlink\" title=\"Education\"></a>Education</h2><ul>\n<li>Currently - M1 student in the Robotics Lab &amp; Robotics Vision Lab of Nara Institute of Science and Technology.</li>\n<li>Bachelor - Software Engineering School of Xi&#39;an Jiaotong University, June 2018.</li>\n</ul>\n<h2 id=\"Skills-amp-Abilities\"><a href=\"#Skills-amp-Abilities\" class=\"headerlink\" title=\"Skills &amp; Abilities\"></a>Skills &amp; Abilities</h2><ul>\n<li>Programming in Java, C++ and Python</li>\n<li>Basic usage of Javascript, PHP and MATLAB</li>\n<li>Web app programming, from HTML to React</li>\n<li>Solving problems using basic data structures and algorithms</li>\n<li>Analyzing and designing software systems</li>\n<li>Basic knowledge in machine learning and computer vision</li>\n<li>Designing and training neural networks using Tensorflow</li>\n</ul>\n<h2 id=\"Experience\"><a href=\"#Experience\" class=\"headerlink\" title=\"Experience\"></a>Experience</h2><ul>\n<li>Internship, Qihoo 360 Corps (Beijing), Summer 2016 &amp; 2017</li>\n<li>Research Assistant, XJTU Spacial Vision Collaborative Laboratory, 2017</li>\n</ul>\n"}],"Post":[{"title":"\\[Do it Yourself\\] Tensor Chain - A Simple Chain-Styled TensorFlow Encapsulation","_content":"","source":"_drafts/diy-tensorchain.md","raw":"---\ntitle: \\[Do it Yourself\\] Tensor Chain - A Simple Chain-Styled TensorFlow Encapsulation\ntags: [Deep Learning, DIY]\n---\n","slug":"diy-tensorchain","published":0,"date":"2018-04-23T13:32:03.584Z","updated":"2018-04-24T06:43:05.125Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomfkrf70000zcvjyam020ez","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"A Painless Tensorflow Basic Tutorial - Take ResNet-56 as an Example","langs":["zh-cn","en-us"],"date":"2018-04-29T13:05:43.000Z","_content":"\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n<br>\n<p lang=\"en-us\">\nTensorFlow is a powerful open-source deep learning framework, supporting various languages including Python. However, its APIs are far too complicated for a beginner in deep learning(especially those who are new to Python). In order to ease the pain of having to understand the mess of various elements in TensorFlow computation graphs, I made this tutorial to help beginners take the first bite of the cake.\n\nResNets are one of the greatest works in the deep learning field. Although they look scary with extreme depths, it's not a hard job to implement one. Now let's build one of the simplest ResNets - ResNet-56, and train it on the CIFAR-10 dataset.\n\n</p>\n<!-- more -->\n<p lang=\"zh-cn\">\nTensorFlow是一个强大的开源深度学习软件库，它支持包括Python在内的多种语言。然而，由于API过于复杂（实际上还有点混乱），它往往使得一个深度学习的初学者（尤其是为此初学Python的那些）望而却步——老虎吃天，无从下口。为了减轻初学者不得不尝试理解TensorFlow中的大量概念的痛苦，我213今天带各位尝尝深度学习这片天的第一口。\nResNet是深度学习领域的一个重磅炸弹，尽管它们（ResNet有不同层数的多个模型）的深度看上去有点吓人，但实际上实现一个ResNet并不难。接下来，我们来实现一个较为简单的ResNet——ResNet-56，并在CIFAR-10数据集上训练一下，看看效果如何。\n</p>\n<div align=\"center\" class=\"figure\">\n![Let's Rock!](/images/tftutorial/oyo.gif)\n</div>\n<p lang=\"en-us\">\nFirst let's take a look at ResNet-56. It's proposed by Kaiming He et al., and is designed to confirm the effect of residual networks. It has 56 weighted layers, deep but simple. The structure is shown in the figure below:\n</p>\n<p lang=\"zh-cn\">\n首先来看一下ResNet-56这个神经网络。它是何凯明等在ResNet论文中提出的、用于验证残差网络效果的一个相对简单的残差网络（尽管它很深，深度达到了56个权重层）。图示如下：\n</p>\n<div align=\"center\" class=\"figure\">\n<img src=\"/images/tftutorial/resnet56.png\" alt=\"ResNet-56\" width=\"80%\"/>\n\nFig. 1 The structure of ResNet-56\n</div>\n<br>\n<p lang=\"en-us\">\nSeems a little bit long? Don't worry, let's do this step by step.\n</p>\n<p lang=\"zh-cn\">\n看起来有点长了是不是？别担心，我们一步一步来做。\n</p>\n\n## 1 Ingredients\nPython 3.6\n\nTensorFlow 1.4.0\n\nNumpy 1.13.3\n\nOpenCV 3.2.0\n\n[CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)\n<p lang=\"en-us\">\nAlso prepare some basic knowledge on Python programming, digital image processing and convolutional neural networks. If you are already capable of building, training and validating your own neural networks with TensorFlow, you don't have to read this post.\n</p>\n<p lang=\"zh-cn\">\n另外，请确保自己有一点点Python编程、数字图像处理和卷积神经网络的知识储备。如果你已经具备用TensorFlow自行搭建神经网络并进行训练、测试的能力，就不必阅读本文了。\n</p>\n\n## 2 Recipe\n### 2.0 Prepare the tools\n<p lang=\"en-us\">\nPrepare(import) the tools for our project, including all that I mentioned above. Like this :P\n</p>\n<p lang=\"zh-cn\">\n准(i)备(m)所(p)需(o)工(r)具(t)，上一部分已提到过。如下：\n</p>\n```python\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nimport pickle\nfrom tensor_chain import TensorChain\n```\n<p lang=\"en-us\">\nWait... What's this? TensorChain? Another deep learning framework like TensorFlow?\n\nUh, nope. This is my own encapsulation of some TensorFlow APIs, for the sake of easing your pain. You'll only have to focus on \"what's what\" in the beginning. We'll look into my implementation of this encapsulation later, when you are clear how everything goes. Please [download this file](/files/tensor_chain.py) and put it where your code file is, and import it.\n</p>\n<p lang=\"zh-cn\">\n等等...最后这个是个什么鬼？ TensorChain？另一个深度学习框架吗？\n\n呃...并不是。这个是我对一些TensorFlow API的封装，为了减轻你的痛苦才做的。作为初学者，你只需要关注用TensorFlow搭建网络模型的这个过程，分清东西南北。回头等你弄清了大体流程后，我们再来看这个的实现细节。请先下载[这个文件](/files/tensor_chain.py)并把它与你的代码放在同一文件夹下，然后就可以import了。\n</p>\n\n### 2.1 Decide the input\n<p lang=\"en-us\">\nEvery neural network requires an input - you always have to identify the details of a question, before asking the computer to solve it. All of the variable, constant in TensorFlow are objects of type <em>tf.Tensors</em>. And the <em>tf.placeholder</em> of our input(s) is a special one. Images in CIFAR-10 dataset are RGB images(3 channels) of 32x32(really small), so our input should shaped like [32, 32, 3]. Also, we want to input a little <em>batch</em> of multiple images. Therefore, our input data should be an array of shape <em>[?, 32, 32, 3]</em>. Unknown dimension size can be marked as None, and it will be clear when we feed the model with the actual images. It's coded like this:\n</p>\n<p lang=\"zh-cn\">\n每个神经网络都需要有输入——毕竟你想找电脑解决一些问题的话，你总得告诉它问题的一些细节吧？TensorFlow中所有的变量、常量都是<em>tf.Tensor</em>类型的对象，作为输入内容的占位符<em>tf.placeholder</em>也是（只不过比较特殊而已）。CIFAR-10数据集的图像都是32x32尺寸（好小哇）的RGB图像（RGB即彩色图像的三个通道），因此我们的输入给神经网络的内容将会像是[32, 32, 3]这个样子。另外呢，我们需要输入的是一个小<em>batch</em>（批）的图像，因此，输入网络的图像数据将会是一个<em>[?, 32, 32, 3]</em>的数组（也可以是numpy数组）。未知的维度大小用None代指就好，我们之后给模型喂实际图像batch时，它自然就清楚了。代码如下：\n</p>\n```python\ninput_tensor = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n```\n<p lang=\"en-us\">\n*Ground truth* data also need to be known in supervised learning, so we also have to define a placeholder for the ground truth data: \n</p>\n<p lang=\"zh-cn\">\n监督学习中，正确标注的数据（英文为*ground truth*，目前貌似没有对这个名词的合理翻译）也是需要输入到模型中的。因此再给ground truth定义一个placeholder：\n</p>\n```python\nground_truth = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n```\n<p lang=\"en-us\">\nWe want the label data to be in the one-hot encoding format, which means an array of length 10, denoting 10 classes. Only on one position is a '1', and on other positions are '0's.\n</p>\n<p lang=\"zh-cn\">\n我们需要标记的数据呈One-Hot编码格式（又称为一位有效编码），意思是如果有10个类别，那么数组长度就是10，每一位代表一个类别。只有一个位置上是1（代表图片被分为这个类），其他位上都是0。\n</p>\n\n### 2.2 Do some operations\n<p lang=\"en-us\">\nFor now, let's use our TensorChain to build it fast. Under most circumstances that we may face, the computations are based on the input data or the result of the former computation, so our network(or say, the most of it) look more like a chain than a web. Every time we add some new operation(layer), we add it to our <em>TensorChain</em> object. Just remember to get the <em>output_tensor</em> of this object(denoting the output tensor of the last operation on the chain) when you need to ue native TensorFlow API.\nThe construction function of TensorChain class requires a Tensor object as the parameter, which is also the input tensor of this chain. As we mentioned earlier, all we have to do is add operations. See my ResNet-56 code:\n</p>\n<p lang=\"zh-cn\">\n现在呢，我们先用TensorChain来快速盖楼。因为我们遇到的大多数情况下，所有的计算都是在输入数据或者这个计算的前一个计算结果基础上进行的，所以我们的网络（至少是它的绝大部分）会看起来像个链而不是所谓的网。每次我们添加一个新的运算（层），我们会把它加到这个独一无二的TensorChain对象。只要记得在使用原生TensorFlow API前把它的<em>output_tensor</em>属性（也就是这条链上最后一个运算的输出Tensor）取出来就好了。\nTensorChain类的构造函数需要一个Tensor对象作为参数，这个对象也正是被拿来作为这个链的输入层。正如我们之前所说的，只要在这个对象上添加运算即可。写个ResNet-56，代码很简单：\n</p>\n```python\nchain = TensorChain(input_tensor) \\\n        .convolution_layer_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 32, stride=2) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 64, stride=2) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .flatten() \\\n        .fully_connected_layer(10)\n```\n<p lang=\"en-us\">\nThis is it? Right, this is it! Isn't it cool? Didn't seem that high, huh? That's because I encapsulated that huge mess of weights and biases, only leaving a few parameters that decide the structure of the network. Later in this pose we'll talk about the actual work that these functions do.\n</p>\n<p lang=\"zh-cn\">\n就这？没错呀，就这！稳不稳？似乎看起来也没56层那么高呀？毕竟这些函数被我封装得太严实了，只留出几个决定网络结构的几个参数供修改。这篇博客后边就会讲到这些函数究竟干了点什么事儿。\n</p>\n\n### 2.3 Define the loss\n<p lang=\"en-us\">\nIn supervised learning, you always have to tell the learning target to the model. To tell the model how to optimize, you have to let it know how, how much, on which direction should it change its parameters. This is done by using a loss function. Therefore, we need to define a loss function for our ResNet-56 model(which we designed for this classification problem) so that it will learn and optimize.\nA commonly used loss function in classification problems is cross entropy. It's defined below:\n</p>\n<p lang=\"zh-cn\">\n搞监督学习，总是要让模型按照“参考答案”去改的。要改就得让它知道怎么改、改多少、往什么方向改，这也就是*loss*（损失函数）的功劳。因此，像我们这个拿来做分类问题的ResNet-56，我们要给它定义一个损失函数来让它学习、优化。\n分类问题上一个常用的损失函数是交叉熵。定义如下式：\n</p>\n$$C=-\\frac{1}{n}\\sum_x{y\\ln a+(1-y)\\ln(1-a)}$$\n<p lang=\"en-us\">\nin which \\\\(y\\\\) is the expected(or say correct) output and \\\\(a\\\\) is the actual output.\nThis seems a little bit complicated. But it's not a hard job to implement, since TensorFlow implemented it already! You can also try and implement it yourself within one line if you want. For now we use the pre-defined cross entropy loss function:\n</p>\n<p lang=\"zh-cn\">\n其中\\\\(y\\\\)为期望输出（或者说参考答案），\\\\(a\\\\)为实际输出。\n略复杂呀...这个用程序怎么写？其实也不难。。。毕竟TensorFlow都帮我们实现好啦！（有兴趣的话也可以自己尝试着写一下，同样一行代码即可搞定）现在你只需要来这么一句：\n</p>\n```python\nloss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))\n```\n<p lang=\"en-us\">\nand it returns a tf.Tensor that denotes an average of cross entropies(don't forget that this is a batch). As for the 'softmax' before the 'cross_entropy', it's a function that project the data in an array to range 0~1, which allows us to do a comparison between our prediction and the ground truth(in one-hot code). The definition is simple too:\\\n</p>\n<p lang=\"zh-cn\">\n就可以创建一个表示交叉熵平均值（别忘了这可是一个batch）的Tensor了。至于cross_entropy前边的那个*softmax*呢，它的作用是把输入的数组内数据归一化，投射到0~1的范围内（实际上就是相当于把exp(数组各项的值)的当做频数，求出一个概率），这样子才能跟实际数据做一个比较。定义也比较简单：\\\n</p>\n$$S_i=\\frac{e^{V_i}}{\\sum_j{e^{V_j}}}$$\n<p></p>\n\n### 2.4 Define the train op\n<p lang=\"en-us\">\nNow we have the loss function. We'll have to tell its value to an *optimizer*, which make our model learn and optimize in order to minimize the loss value. Gradient Descent Optimizer, Adagrad Optimizer, Adam Optimizer and Momentum Optimizer are commonly used optimizers. Here we use an Adam Optimizer for instance. You're free to try any other one here. When\n</p>\n<p lang=\"zh-cn\">\n现在误差函数已经有了，我们需要把它的值告诉一个优化器（*optimizer*），并让它去尽可能向着缩小误差函数值得方向努力。这样，模型才能去学习、优化。常用的优化器包括Gradient Descent Optimizer，Adagrad Optimizer，Adam Optimizer以及Momentum Optimizer等等等等。选择优化器时，我们需要给它一个初始的学习速率。这里我用了一个\\\\(10^-3\\\\)，如果需要提高准确率，可能后期微调还需要进一步减小。代码如下：\n</p>\n```python\noptimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n```\n<p lang=\"en-us\">\nAlso, tell the optimizer that what the loss tensor is. The returned object is a train operation.\n</p>\n<p lang=\"zh-cn\">\n当然还要告诉它要减小的损失函数是哪个Tensor，这个函数返回的是一个训练操作（*train op*，一种特殊的运算，或者说操作）：\n</p>\n```python\ntrain = optimizer.minimize(loss)\n```\n<p lang=\"en-us\">\nThe neural network is finished. It's time to grab some data and train it.\n</p>\n<p lang=\"zh-cn\">\n其实到这里为止，神经网络已经搭建好了。是时候搞点数据来训练它了。\n</p>\n\n### 2.5 Feed the model with data, and train it!\n<p lang=\"en-us\">\nRemember how we defined the placeholders? It's time to fetch some data that fits the placeholders and train it. See how CIFAR-10 dataset can be fetched on its [website](https://www.cs.toronto.edu/~kriz/cifar.html).\n</p>\n<p lang=\"zh-cn\">\n还记得我们怎么定义那些placeholder吗？现在我们要把符合它们口径的数据灌进模型。那么来看一下CIFAR-10数据集[官网](https://www.cs.toronto.edu/~kriz/cifar.html)上是怎么描述的吧。它给了这么一段代码：\n</p>\n```python\ndef unpickle(file):\n    import pickle\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict\n```\n<p lang=\"en-us\">\nThe returned value *dict* is a Python dictionary. Every time we unpickle a file, a dictionary would be returned. Its 'data' key leads to 10000 RGB images of size 32x32, which is stored in a [10000, 3072] array(3072=32*32*3, I guess you know how it's stored now). The 'label' key leads to 10000 values in range 0~9. Obviously we have to reshape the data so as to fit it into the network model:\n</p>\n<p lang=\"zh-cn\">\n返回值*dict*是一个字典（Python的dict类型）。每读一个batch文件（比如data_batch_1），就会返回这样一个字典，它的“data”键值是10000张32x32的RGB图像（数组维数居然是[10000, 3072]，而3072=10000x32x32x3！实际上就是直接把所有像素、所有通道的值罗列在这里了）；“label”键值是10000个0-9之间的整数（代表类别）。显然，为了让数据能够成功放进模型，还需要对它进行一点处理：\n</p>\n```python\nbatch = unpickle(DATA_PATH + 'data_batch_{}'.format(i))  # 'i' is the loop variable\n\n# Read the image data\nimage_data = np.reshape(batch[b'data'], (10000, 32, 32, 3), 'F').astype(np.float32)   \nimage_data = image_data / 255                            # Cast range(0, 255) to range(0, 1)\nimage_data = np.transpose(image_data, (0, 2, 1, 3))      # Exchange row and column\n\n# Read the label data and convert into one-hot code\nlabel_data = batch[b'labels']\nnew_label_data = np.zeros((10000, 10))                   \nfor j in range(10000):\n    new_label_data[j][label_data[j]] = 1\n```\n<p lang=\"en-us\">\nThe details for data processing are not covered here. Try doing step-by-step to see the results.\nThe *image_data* and *new_label_data* are contain 10000 pieces of data each. Let's divide them into 100 small batches(100 elements each, including image and label) and feed it into the model. Do this on all the 5 batch files:\n</p>\n<p lang=\"zh-cn\">\n处理的细节不再赘述。你可以尝试一步一步运行来看看每一步的结果。\n这样我们拿到的*image_data*和*new_label_data*都是长度为10000的大batch，我们把它们各自分成100份，每次取100个图像+标记数据来塞进模型。对全部5个大batch文件来一遍：\n</p>\n```python\nwith tf.Session() as session:\n    session.run(tf.global_variables_initializer())\n    for j in range(100): # 10000 / BATCH_SIZE\n        # Divide them and get one part\n        image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]\n        label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]\n        \n        # Feed the model\n        session.run(train, feed_dict={\n            input_tensor: image_batch,\n            ground_truth: label_batch\n        })\n```\n<p lang=\"en-us\">\nA *session* - created with *tf.Session()* - is required every time we run a TensorFlow model, no matter when we're training it or evaluating it. The first time you run a model, you'll need to run *session.run(tf.global_variables_initializer())* to initialize the values of the TensorFlow variables defined previously.\nWhen running *session.run()*, you must first decide a TensorFlow operation(or a list of operations) that you need. If its result is dependent on some actual data(which means that some data in one or more placeholders flow to this operation), it's also required that you feed it the actual data by adding a *feed_dict* parameter. For example, I'm training this ResNet-56 model, in which a loss will be calculated with my *ground_truth* and the prediction result that comes from the *input_tensor*. Therefore, I'll have to give a value for each placeholder given above(format: \"placeholder name: corresponding data\"), and fold them in one Python dictionary.\n</p>\n<p lang=\"zh-cn\">\n每次运行一个TensorFlow模型（无论是训练还是测试）时，都需要通过tf.Session()创建一个*session*。第一次运行模型（而不是载入之前保存的模型）时，需要使用*session.run(tf.global_variables_initializer())*来初始化之前定义的一些可训练的TensorFlow变量。\n运行*session.run()*时，需要指定一个或一组你要执行的operation，作为这个函数唯一一个必要的参数。如果它的结果依赖于一些实际数据（也就是说在计算图中，一些数据会从placeholder流向这个operation），那么就需要通过填入*feed_dict*参数的值来填装训练或测试数据。以此模型为例，我在训练它时需要算误差函数值，这需要*ground_truth*数据和预测结果来计算，而预测结果又需要用输入图像*input_tensor*来计算得到。因此，我需要给这两个占位符分别给出对应的数据（格式：“占位符名：对应数据”），并把它们封在同一个Python字典中作为feed_dict参数的值。\n</p>\n<p lang=\"en-us\">\nI'm also interested in the loss function value in each iteration(which means feeding a batch of data and executing one forward-propagation and one back-propagation) in the training process. Therefore, what I'll fill in the parameter is not just the train op, but also the loss tensor. And the session.run() above should be modified to:\n</p>\n<p lang=\"zh-cn\">\n然而呢，我还想看看每次迭代（即把一个batch送进去，执行一次正向传播与反向传播这个过程）中损失函数变成了多大，来监控一下训练的效果。这样，需要session.run()的就不仅是那个train运算，还要加上loss运算。将上边的session.run()部分改为：\n</p>\n```python\n        [train_, loss_value] = session.run([train, loss],\n            feed_dict={\n                input_tensor: image_batch,\n                ground_truth: label_batch\n            })\n        print(\"Loss = {}\".format(loss_value)\n```\n<p lang=\"en-us\">\nThis is when the return value of session.run() becomes useful. Its value(s) - corresponding to the first parameter of run() - is/are the actual value(s) of the tensor(s) in the first parameter. In our example, *loss_value* is the actual output of the loss tensor. As for train_, we don't care what it is. Just add it to match the dimensions.\n</p>\n<p lang=\"zh-cn\">\n这时候，session.run()函数的返回值就有意义了。它与第一个参数的内容一一对应，分别是该参数中各个operation的实际输出值。像这个例子里边，*loss_value*接收的就是loss运算的输出内容。而train运算的输出我们并不关心，但是为了保证参数维度数与返回值一致，用一个train_变量来接收而已（实际上它的值是None）。\n</p>\n<p lang=\"en-us\">\nActually, one epoch(train the model once with the whole dataset) is not enough for the model to fully optimize. I trained this model for 40 epochs and added some loop variables to display the result. You can see my code and my output below. It's highly recommended that you train this with a high-performance GPU, or it would be a century before you train your model to a satisfactory degree.\n</p>\n<p lang=\"zh-cn\">\n实际上，一个epoch（把整个数据集都在模型里过一遍的周期）并不足以让模型充分学习。我把这个模型训练了40个epoch并且加了一些循环变量来输出结果。我的代码和结果如下。强烈建议用一个高性能GPU训练（如果手头没有，可以租一个GPU服务器），不然等别人把毕设论文逗写完的时候，你还在训练就很尴尬了。\n</p>\n```python\nimport tensorflow as tf\nimport numpy as np\nimport pickle\nfrom tensor_chain import TensorChain\n\ndef unpickle(file):\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict\n\nif __name__ == '__main__':\n    input_tensor = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n    ground_truth = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n\n    chain = TensorChain(input_tensor) \\\n            .convolution_layer_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 32, stride=2) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 64, stride=2) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .flatten() \\\n            .fully_connected_layer(10)\n\n    prediction = chain.output_tensor\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train = optimizer.minimize(loss)\n\n    with tf.Session() as session:\n        session.run(tf.global_variables_initializer())\n        iteration = 1\n        BATCH_SIZE = 100\n        DATA_PATH = '../data/cifar-10-batches-py/'\n        for epoch in range(1, 41):\n            for i in range(1, 6):\n                data = unpickle(DATA_PATH + 'data_batch_{}'.format(i))\n                image_data = np.reshape(data[b'data'], (10000, 32, 32, 3), 'F').astype(np.float32)\n                image_data = image_data / 255\n                image_data = np.transpose(image_data, (0, 2, 1, 3))\n                label_data = data[b'labels']\n                new_label_data = np.zeros((10000, 10))\n                for j in range(10000):\n                    new_label_data[j][label_data[j]] = 1\n                for j in range(int(10000 / BATCH_SIZE)):\n                    image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]\n                    label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]\n                    [train_, loss_] = session.run(\n                        [train, loss],\n                        feed_dict={\n                            input_tensor: image_batch,\n                            ground_truth: label_batch\n                        })\n                    iteration += 1\n                    print(\"Epoch {}, Iteration {}, Loss = {}\".format(epoch, iteration, loss_))\n```\n\n<div align=\"center\" class=\"figure\">\n<img src=\"/images/tftutorial/train.png\" alt=\"Training result\" width=\"40%\">\n\nFig. 2 Training result: cross entropy has dropped below 0.5\n</div>\n<p></p>\n\n### 2.6 Conclusion\n<p lang=\"en-us\">\nIn a word, building & training neural network models with TensorFlow involves the following steps:\n\n1. Decide the *input tensor*\n\n2. Add operations(*op*s) based on existing tensors\n\n3. Define the *loss* tensor, just like other tensors\n\n4. Select an *optimizer* and define the *train* op\n\n5. Process *data* and feed the model with them\n</p>\n<p lang=\"zh-cn\">\n总而言之，用TensorFlow建立、训练一个神经网络模型分以下几步：\n\n1. 定义*输入*Tensor\n\n2. 在已有的Tensor上添加运算（*op*）\n\n3. 像之前添加的那些运算一样，定义*损失*Tensor\n\n4. 选择一个*优化器*并定义*训练*操作\n\n5. 把*数据*处理为合适的shape，并喂进模型训练\n</p>\n\n## 3 A Closer Look\n<p lang=\"en-us\">\nWait, it's too late to leave now!\nTensorChain saved you from having to deal with a mess of TensorFlow classes and functions. Now it's time that we take a closer look at how TensorChain is implemented, thus understanding the native TensorFlow APIs.\n</p>\n<p lang=\"zh-cn\">\n别走呢喂！\nTensorChain让你不至于面对TensorFlow中乱糟糟的类型和函数而不知所措被水淹没。现在是时候近距离观察一下TensorChain是如何实现的，以便理解TensorFlowAPI了。\n</p>\n\n### 3.1 TensorFlow variables\n<p lang=\"en-us\">\nLet's begin with TensorFlow variables. Variables in TensorFlow are similar to variables in C, Java or any other strong typed programming languages - they have a type, though not necessarily explicitly decided upon definition. Usually them will change as the training process goes on, getting close to a best value.\nThe most commonly used variables in TensorFlow are weights and biases. I guess that you have seen formulae like:\n</p>\n<p lang=\"zh-cn\">\n先说TensorFlow的变量。TensorFlow的变量和C，Java以及其他强类型语言类似——都有一个类型，尽管不一定在它的定义时就显式地声明。通常它们会随着训练的进行而不断变化，达到一个最佳的值附近。\nTensorFlow中最常用的变量就是weights和biases（权重和偏置）。想必你应该见过这样的式子吧：\n</p>\n$$y=Wx+b$$\n<p lang=\"en-us\">\nThe \\\\(W\\\\) here is the weight, and the \\\\(b\\\\) here is the bias. When implementing some common network layers, they two are always used as the parameters in the layers. For instance, at the very beginning of our ResNet-56, we had a 3x3 sized convolution layer with 16 channels. Its implementation in TensorChain is:\n</p>\n<p lang=\"zh-cn\">\n这里\\\\(W\\\\)就是权重，\\\\(b\\\\)就是偏置。在定义一些常用的层时，我们往往也是用这两个变量作为这些层中的参数。比如说，在我们ResNet-56最开始，我们用到了一个3x3大小、16个通道的卷积层，TensorChain中，它的实现如下：\n</p>\n```python\n    def convolution_layer_2d(self, filter_size: int, num_channels: int, stride: int = 1, name: str = None,\n                             disable_log: bool = False):\n        \"\"\"\n        Add a 2D convolution layer\n        :param filter_size: Filter size(width and height) for this operation\n        :param num_channels: Channel number of this filter\n        :param stride: Stride for this convolution operation\n        :param name: The name of the tensor\n        :param disable_log: Set it True if you don't want this layer to be recorded\n        :return: This object itself\n        \"\"\"\n        filter = self._weights([filter_size, filter_size, self.num_channels, num_channels], layer_name=name,\n                               suffix='filter')\n        bias = self._bias([num_channels], layer_name=name)\n        self.num_channels = num_channels\n        self.output_tensor = tf.nn.conv2d(self.output_tensor, filter,\n                                          [1, stride, stride, 1], 'SAME', name=name)\n        self.output_tensor = tf.add(self.output_tensor, bias)\n        self._log_layer(\n            '2D Convolution layer, filter size = {}x{}, stride = {}, {} channels'.format(filter_size, filter_size,\n                                                                                         stride,\n                                                                                         num_channels),\n            disable=disable_log)\n        return self\n```\n<p lang=\"en-us\">\nSee? On line 16, we used a *tf.nn.conv2d()* function, the parameters of which are *input*, *filter*, *strides*, *padding*, etc. As can be guessed from the names, this function does a convolution operation with out input and the weights(the convolution *filter* here). A *bias* is added to the result as the final output. There are also many people who argue that the bias here is meaningless and should removed. One line of code is sufficient for defining a variable:\n</p>\n<p lang=\"zh-cn\">\n看见了吧？16行上，我们用了一个*tf.nn.conv2d()*函数，它的参数是*input*，*filter*，*strides*，*padding*等等。顾名思义，这个函数就是用我们定义的权重Tensor*filter*（在这里称之为卷积核）来与这一层的输入input做了一次运算。运算的结果加上了偏置Tensor*bias*，作为这个卷积层的最终输出。很多人认为这里的偏置bias意义不明，因此他们在卷积之后没有加上这样的一个bias变量。定义一个变量只需要这样一个语句：\n</p>\n```python\ntf.Variable(tf.truncated_normal(shape, stddev=sigma), dtype=tf.float32, name=suffix)\n```\n<p lang=\"en-us\">\nTo define weight or bias variables, create a *tf.Variable* object. Usually you'll need to give the *initial_value* which also decides the shape of this tensor. *tf.truncated_normal()* and *tf.constant()* are usually used as the initial values. Also, other APIs - function *tf.get_variable()* and package *tf.initializers* are frequently used when using some more methods for initialization. I strongly recommend that you try using these APIs yourself.\n</p>\n<p lang=\"zh-cn\">\n要定义权重或者偏置变量，请创建一个*tf.Variable*对象。通常情况下，你会需要给出*initial_value*（TF变量的初始值），这将顺便定义了这个变量的shape（因为初始值的shape是确定的）。另外，一些新的API——*tf.get_variable()*函数和*tf.initializers*包也常用与的参数初始化，以实现更多样的初始化方法。我强烈建议自己动手实践一下，试一试这些API。\n</p>\n\n### 3.2 Tensors and operations\n<p lang=\"en-us\">\nGoing on with the parameters of the *tf.nn.conv2d()* function. The required parameters also include *strides* and *padding*. You should have already learned about what strides mean in convolution, and I'll only talk about their formats. *strides* require a 1-D vector with a length of 4, like [1, 2, 2, 1]. The 1st and the 4th number is always 1(in order to match dimensions with the input), while the 2nd and the 3rd means the vertical stride and the horizonal stride. \nThe 4th parameter *padding* is a little bit different from its definition in convolution operation. It requires 'SAME' of 'VALID', denoting 'with' or 'without' zero paddings. When it's 'SAME', zero padding is introduced to make the shapes match as needed, equally on every side of the input map.\n</p>\n<p lang=\"zh-cn\">\n接着说*tf.nn.conv2d()*函数的参数。需要的参数还包括*strides*和*padding*。你应该在了解卷积运算时学过stride（步幅）含义，我只说一下它的格式吧。*strides*参数需要是一个1维、长度为4的向量。第一位和第四位永远都是1，第二位和第三位分别是竖直方向和水平方向的步幅长。维持这个形式只是为了与输入的数据维度匹配，因此API看起来非常蹩脚。\n第四个参数*padding*和卷积运算里的padding不太相同。它的值只能是'SAME'或'VALID'，分别代表“带”和“不带”零补全。如果是'SAME'的话，函数会均匀地在图像的上下左右使用零补全来使得运算结果与之前尽可能保持一致。（stride>1时有可能输出尺寸不是正好等于原来的尺寸/stride，因为补全问题）\n</p>\n<p lang=\"en-us\">\ntf.nn.conv2d() is just an example of TensorFlow *operations*. Other functions like *tf.matmul()*, *tf.reduce_mean()*, *tf.global_variables_initializer()*, *tf.losses.softmax_cross_entropy()*, *tf.truncated_normal()* are all operations. Operation functions return tensors(*tf.truncated_normal* also return a tensor, a tensor with initializers).\n</p>\n<p lang=\"zh-cn\">\ntf.nn.conv2d()只是TensorFlow运算（*operation*）的一个例子。其他例如*tf.matmul()*，*tf.reduce_mean()*，*tf.nn.relu()*，*tf.batch_normalization()*，*tf.global_variables_initializer()*，*tf.losses.softmax_cross_entropy()*，*tf.truncated_normal()*之类的函数也都是TensorFlow的运算。TensorFlow的运算函数会返回一个Tensor对象（包括*tf.truncated_normal()*也是！它只不过返回的是一个带初始化器的Tensor而已）。\n</p>\n<p lang=\"en-us\">\nAll the functions in the TensorChain class are based on the most basic TensorFlow operations and variables. After learning about these basic TensorFlow concepts, actually you can already abandon TensorChain, go and try implementing your own neural networks yourself!\n</p>\n<p lang=\"zh-cn\">\nTensorChain类中的所有成员函数都是基于最基本的TensorFlow运算和变量的。实际上，了解了这些，你现在已经可以抛开TensorChain的束缚，去尝试实现你自己的神经网络了！\n</p>\n\n## 4 Spices\n<p lang=\"en-us\">\nI'm not joking just now! But I know that there are a lot of things that you still don't understand about using TensorFlow - like \"how do I visualize my computation graph\", \"how do I save/load my model to/from files\", \"how do I record some tensors' values while training\" or \"how do I view the loss curves\" - after all TensorFlow APIs are far more complicated than just building those nets. Those are also important techniques in your research. If you'd rather ask me than spending some time experimenting, please go on with reading.\n</p>\n<p lang=\"zh-cn\">\n我，我真没开玩笑！但是我知道关于如何使用TensorFlow，你还有许许多多的问题，好比“如何可视化地查看我的计算图结构”、“如何存储/读取模型文件”、“如何记录训练过程中某些Tensor的真实值”、“如何查看损失函数的变化曲线”——毕竟TensorFlow的API太复杂了，远比搭建神经网络那点函数复杂得多。上边说的那些是你使用TensorFlow研究过程中的重要技巧。如果你愿意听我讲而不想花些时间尝试的话，请继续读下去。\n</p>\n\n### 4.1 Saving and loading your model\n<p lang=\"en-us\">\nThe very first thing that you may want to do - after training a network model with nice outcomes - would be saving it. Saving a model is fairly easy - just use a *tf.train.Saver* object. See my code below:\n</p>\n<p lang=\"zh-cn\">\n训练出一个看起来输出还不错的神经网络模型后你想做的第一件事恐怕就是把它存下来了吧？保存模型其实非常简单：只要用一个*tf.train.Saver*类的对象。代码示例：\n</p>\n```python\nwith tf.Session() as session:\n    # Train it for some iterations\n    # Train it for some iterations\n    # Train it for some iterations\n    saver = tf.train.Saver()\n    saver.save(session, 'models/model.ckpt')\n```\n<p lang=\"en-us\">\nI saved my model and variable values to 'models/model.ckpt'. But actually, you'll find 3 files in the 'models' directory - *model.ckpt.data-00000-of-00001*, *model.ckpt.meta* and *model.ckpt.index* - none of which is 'model.ckpt'! That's because TensorFlow stores the graph structure separately from variables values. The *.meta* file describes the saved graph structure; the *.index* file records the mappings between tensor names and tensor metadata; and the *.data-00000-of-00001* file - which is always the biggest one - saves all the variable values. If you need the graph data together with the variable values to be loaded, use a Saver to load after creating a session:\n</p>\n<p lang=\"zh-cn\">\n我把我的模型和变量值存到了'models/model.ckpt'文件里。但是！实际上在models目录里你会找到三个文件：*model.ckpt.data-00000-of-00001*，*model.ckpt.meta*和*model.ckpt.index*——哪个也不是model.ckpt呀？那是因为TensorFlow把计算图的结构和图中各种变量的值分开存放了。*.meta*文件描述计算图的结构；*.index*文件记录各个Tensor名称（是name属性，而不是变量名）与Tensor元信息之间的映射；*.data-00000-of-00001*文件往往是最大的一个，它存储的是各个TensorFlow变量的实际值。如果读取时需要把图结构和变量值都读进来，在session创建以后，同样用一个Saver来读取即可：\n</p>\n```python\nwith tf.Session() as session:\n    saver = tf.train.Saver()\n    saver.restore(session, 'models/model.ckpt')\n    # Then continue doing everything just like the model is just trained\n```\n<p lang=\"en-us\">\nRemember that session.run(tf.global_variables_initializer()) shouldn't be executed, since variables are already initialized with your saved *.data-0000-of-00001* file.\nIf you only need the graph to be loaded, only use the *.meta* file:\n</p>\n<p lang=\"zh-cn\">\n记住，这时候就不要再去执行session.run(tf.global_variables_initializer())了，因为变量已经用存储的checkpoint文件内容初始化过了。\n如果只需要读取计算图结构，只要读取*.meta*文件：\n</p>\n```python\nwith tf.Session() as session:\n    tf.train.import_meta_graph('models/model.ckpt.meta')\n    # Then continue doing everything just like the model is just built\n```\n<p lang=\"en-us\">\nFunction *tf.train.import_meta_graph()* loads(appends) the graph to your current computation graph. The values of tensors are still uninitialized so you'll have to execute session.run(tf.global_variables_initializer()) again. The tensors that you defined in the model can be retrieved by their names(property of the Tensor objects, instead of Python variable names). For example:\n</p>\n<p lang=\"zh-cn\">\n*tf.train.import_meta_graph()*函数将文件里的计算图读到（添加到）你当前的计算图中。其中所有Tensor的值仍未初始化，所以有必要执行一下session.run(tf.global_variables_initializer())了。之前定义的变量可以按照名称取回，示例：\n</p>\n```python\nwith tf.Session() as session:\n    # Recover the model here\n\n    graph = tf.get_default_graph()\n    image_tensor = graph.get_tensor_by_name('input_image:0')\n    loss = graph.get_tensor_by_name('loss:0')\n    train = graph.get_operation_by_name('train)\n```\n<p lang=\"en-us\">\nTo retrieve normal tensors, you'll have to append a *':0'* to the name of the op. This means getting the associated tensor of the op. *train* is a little special - we only need the op, so the function is *get_operation_by_name()* so the ':0' is not necessary.\n</p>\n<p lang=\"zh-cn\">\n要取回一般的Tensor，需要在Tensor的name属性值后边加一个*':0'*，意思是取这个运算对应的Tensor。训练操作*train*略有不同——我们要的就只是这个op，所以用的函数*get_operation_by_name()*跟其他Tensor不一样，而且':0'也不需要加。\n</p>\n\n<p lang=\"en-us\" align=\"center\">\n[THIS SECTION IS UNDER CONSTRUCTION]\n</p>\n<p lang=\"zh-cn\" align=\"center\">\n[本部分内容施工中]\n</p>","source":"_posts/diy-resnet.md","raw":"---\ntitle: A Painless Tensorflow Basic Tutorial - Take ResNet-56 as an Example\ntags:\n  - Deep Learning\n  - DIY\nlangs:\n  - zh-cn\n  - en-us\ndate: 2018-04-29 22:05:43\n---\n\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n<br>\n<p lang=\"en-us\">\nTensorFlow is a powerful open-source deep learning framework, supporting various languages including Python. However, its APIs are far too complicated for a beginner in deep learning(especially those who are new to Python). In order to ease the pain of having to understand the mess of various elements in TensorFlow computation graphs, I made this tutorial to help beginners take the first bite of the cake.\n\nResNets are one of the greatest works in the deep learning field. Although they look scary with extreme depths, it's not a hard job to implement one. Now let's build one of the simplest ResNets - ResNet-56, and train it on the CIFAR-10 dataset.\n\n</p>\n<!-- more -->\n<p lang=\"zh-cn\">\nTensorFlow是一个强大的开源深度学习软件库，它支持包括Python在内的多种语言。然而，由于API过于复杂（实际上还有点混乱），它往往使得一个深度学习的初学者（尤其是为此初学Python的那些）望而却步——老虎吃天，无从下口。为了减轻初学者不得不尝试理解TensorFlow中的大量概念的痛苦，我213今天带各位尝尝深度学习这片天的第一口。\nResNet是深度学习领域的一个重磅炸弹，尽管它们（ResNet有不同层数的多个模型）的深度看上去有点吓人，但实际上实现一个ResNet并不难。接下来，我们来实现一个较为简单的ResNet——ResNet-56，并在CIFAR-10数据集上训练一下，看看效果如何。\n</p>\n<div align=\"center\" class=\"figure\">\n![Let's Rock!](/images/tftutorial/oyo.gif)\n</div>\n<p lang=\"en-us\">\nFirst let's take a look at ResNet-56. It's proposed by Kaiming He et al., and is designed to confirm the effect of residual networks. It has 56 weighted layers, deep but simple. The structure is shown in the figure below:\n</p>\n<p lang=\"zh-cn\">\n首先来看一下ResNet-56这个神经网络。它是何凯明等在ResNet论文中提出的、用于验证残差网络效果的一个相对简单的残差网络（尽管它很深，深度达到了56个权重层）。图示如下：\n</p>\n<div align=\"center\" class=\"figure\">\n<img src=\"/images/tftutorial/resnet56.png\" alt=\"ResNet-56\" width=\"80%\"/>\n\nFig. 1 The structure of ResNet-56\n</div>\n<br>\n<p lang=\"en-us\">\nSeems a little bit long? Don't worry, let's do this step by step.\n</p>\n<p lang=\"zh-cn\">\n看起来有点长了是不是？别担心，我们一步一步来做。\n</p>\n\n## 1 Ingredients\nPython 3.6\n\nTensorFlow 1.4.0\n\nNumpy 1.13.3\n\nOpenCV 3.2.0\n\n[CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)\n<p lang=\"en-us\">\nAlso prepare some basic knowledge on Python programming, digital image processing and convolutional neural networks. If you are already capable of building, training and validating your own neural networks with TensorFlow, you don't have to read this post.\n</p>\n<p lang=\"zh-cn\">\n另外，请确保自己有一点点Python编程、数字图像处理和卷积神经网络的知识储备。如果你已经具备用TensorFlow自行搭建神经网络并进行训练、测试的能力，就不必阅读本文了。\n</p>\n\n## 2 Recipe\n### 2.0 Prepare the tools\n<p lang=\"en-us\">\nPrepare(import) the tools for our project, including all that I mentioned above. Like this :P\n</p>\n<p lang=\"zh-cn\">\n准(i)备(m)所(p)需(o)工(r)具(t)，上一部分已提到过。如下：\n</p>\n```python\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nimport pickle\nfrom tensor_chain import TensorChain\n```\n<p lang=\"en-us\">\nWait... What's this? TensorChain? Another deep learning framework like TensorFlow?\n\nUh, nope. This is my own encapsulation of some TensorFlow APIs, for the sake of easing your pain. You'll only have to focus on \"what's what\" in the beginning. We'll look into my implementation of this encapsulation later, when you are clear how everything goes. Please [download this file](/files/tensor_chain.py) and put it where your code file is, and import it.\n</p>\n<p lang=\"zh-cn\">\n等等...最后这个是个什么鬼？ TensorChain？另一个深度学习框架吗？\n\n呃...并不是。这个是我对一些TensorFlow API的封装，为了减轻你的痛苦才做的。作为初学者，你只需要关注用TensorFlow搭建网络模型的这个过程，分清东西南北。回头等你弄清了大体流程后，我们再来看这个的实现细节。请先下载[这个文件](/files/tensor_chain.py)并把它与你的代码放在同一文件夹下，然后就可以import了。\n</p>\n\n### 2.1 Decide the input\n<p lang=\"en-us\">\nEvery neural network requires an input - you always have to identify the details of a question, before asking the computer to solve it. All of the variable, constant in TensorFlow are objects of type <em>tf.Tensors</em>. And the <em>tf.placeholder</em> of our input(s) is a special one. Images in CIFAR-10 dataset are RGB images(3 channels) of 32x32(really small), so our input should shaped like [32, 32, 3]. Also, we want to input a little <em>batch</em> of multiple images. Therefore, our input data should be an array of shape <em>[?, 32, 32, 3]</em>. Unknown dimension size can be marked as None, and it will be clear when we feed the model with the actual images. It's coded like this:\n</p>\n<p lang=\"zh-cn\">\n每个神经网络都需要有输入——毕竟你想找电脑解决一些问题的话，你总得告诉它问题的一些细节吧？TensorFlow中所有的变量、常量都是<em>tf.Tensor</em>类型的对象，作为输入内容的占位符<em>tf.placeholder</em>也是（只不过比较特殊而已）。CIFAR-10数据集的图像都是32x32尺寸（好小哇）的RGB图像（RGB即彩色图像的三个通道），因此我们的输入给神经网络的内容将会像是[32, 32, 3]这个样子。另外呢，我们需要输入的是一个小<em>batch</em>（批）的图像，因此，输入网络的图像数据将会是一个<em>[?, 32, 32, 3]</em>的数组（也可以是numpy数组）。未知的维度大小用None代指就好，我们之后给模型喂实际图像batch时，它自然就清楚了。代码如下：\n</p>\n```python\ninput_tensor = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n```\n<p lang=\"en-us\">\n*Ground truth* data also need to be known in supervised learning, so we also have to define a placeholder for the ground truth data: \n</p>\n<p lang=\"zh-cn\">\n监督学习中，正确标注的数据（英文为*ground truth*，目前貌似没有对这个名词的合理翻译）也是需要输入到模型中的。因此再给ground truth定义一个placeholder：\n</p>\n```python\nground_truth = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n```\n<p lang=\"en-us\">\nWe want the label data to be in the one-hot encoding format, which means an array of length 10, denoting 10 classes. Only on one position is a '1', and on other positions are '0's.\n</p>\n<p lang=\"zh-cn\">\n我们需要标记的数据呈One-Hot编码格式（又称为一位有效编码），意思是如果有10个类别，那么数组长度就是10，每一位代表一个类别。只有一个位置上是1（代表图片被分为这个类），其他位上都是0。\n</p>\n\n### 2.2 Do some operations\n<p lang=\"en-us\">\nFor now, let's use our TensorChain to build it fast. Under most circumstances that we may face, the computations are based on the input data or the result of the former computation, so our network(or say, the most of it) look more like a chain than a web. Every time we add some new operation(layer), we add it to our <em>TensorChain</em> object. Just remember to get the <em>output_tensor</em> of this object(denoting the output tensor of the last operation on the chain) when you need to ue native TensorFlow API.\nThe construction function of TensorChain class requires a Tensor object as the parameter, which is also the input tensor of this chain. As we mentioned earlier, all we have to do is add operations. See my ResNet-56 code:\n</p>\n<p lang=\"zh-cn\">\n现在呢，我们先用TensorChain来快速盖楼。因为我们遇到的大多数情况下，所有的计算都是在输入数据或者这个计算的前一个计算结果基础上进行的，所以我们的网络（至少是它的绝大部分）会看起来像个链而不是所谓的网。每次我们添加一个新的运算（层），我们会把它加到这个独一无二的TensorChain对象。只要记得在使用原生TensorFlow API前把它的<em>output_tensor</em>属性（也就是这条链上最后一个运算的输出Tensor）取出来就好了。\nTensorChain类的构造函数需要一个Tensor对象作为参数，这个对象也正是被拿来作为这个链的输入层。正如我们之前所说的，只要在这个对象上添加运算即可。写个ResNet-56，代码很简单：\n</p>\n```python\nchain = TensorChain(input_tensor) \\\n        .convolution_layer_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 32, stride=2) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 64, stride=2) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .flatten() \\\n        .fully_connected_layer(10)\n```\n<p lang=\"en-us\">\nThis is it? Right, this is it! Isn't it cool? Didn't seem that high, huh? That's because I encapsulated that huge mess of weights and biases, only leaving a few parameters that decide the structure of the network. Later in this pose we'll talk about the actual work that these functions do.\n</p>\n<p lang=\"zh-cn\">\n就这？没错呀，就这！稳不稳？似乎看起来也没56层那么高呀？毕竟这些函数被我封装得太严实了，只留出几个决定网络结构的几个参数供修改。这篇博客后边就会讲到这些函数究竟干了点什么事儿。\n</p>\n\n### 2.3 Define the loss\n<p lang=\"en-us\">\nIn supervised learning, you always have to tell the learning target to the model. To tell the model how to optimize, you have to let it know how, how much, on which direction should it change its parameters. This is done by using a loss function. Therefore, we need to define a loss function for our ResNet-56 model(which we designed for this classification problem) so that it will learn and optimize.\nA commonly used loss function in classification problems is cross entropy. It's defined below:\n</p>\n<p lang=\"zh-cn\">\n搞监督学习，总是要让模型按照“参考答案”去改的。要改就得让它知道怎么改、改多少、往什么方向改，这也就是*loss*（损失函数）的功劳。因此，像我们这个拿来做分类问题的ResNet-56，我们要给它定义一个损失函数来让它学习、优化。\n分类问题上一个常用的损失函数是交叉熵。定义如下式：\n</p>\n$$C=-\\frac{1}{n}\\sum_x{y\\ln a+(1-y)\\ln(1-a)}$$\n<p lang=\"en-us\">\nin which \\\\(y\\\\) is the expected(or say correct) output and \\\\(a\\\\) is the actual output.\nThis seems a little bit complicated. But it's not a hard job to implement, since TensorFlow implemented it already! You can also try and implement it yourself within one line if you want. For now we use the pre-defined cross entropy loss function:\n</p>\n<p lang=\"zh-cn\">\n其中\\\\(y\\\\)为期望输出（或者说参考答案），\\\\(a\\\\)为实际输出。\n略复杂呀...这个用程序怎么写？其实也不难。。。毕竟TensorFlow都帮我们实现好啦！（有兴趣的话也可以自己尝试着写一下，同样一行代码即可搞定）现在你只需要来这么一句：\n</p>\n```python\nloss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))\n```\n<p lang=\"en-us\">\nand it returns a tf.Tensor that denotes an average of cross entropies(don't forget that this is a batch). As for the 'softmax' before the 'cross_entropy', it's a function that project the data in an array to range 0~1, which allows us to do a comparison between our prediction and the ground truth(in one-hot code). The definition is simple too:\\\n</p>\n<p lang=\"zh-cn\">\n就可以创建一个表示交叉熵平均值（别忘了这可是一个batch）的Tensor了。至于cross_entropy前边的那个*softmax*呢，它的作用是把输入的数组内数据归一化，投射到0~1的范围内（实际上就是相当于把exp(数组各项的值)的当做频数，求出一个概率），这样子才能跟实际数据做一个比较。定义也比较简单：\\\n</p>\n$$S_i=\\frac{e^{V_i}}{\\sum_j{e^{V_j}}}$$\n<p></p>\n\n### 2.4 Define the train op\n<p lang=\"en-us\">\nNow we have the loss function. We'll have to tell its value to an *optimizer*, which make our model learn and optimize in order to minimize the loss value. Gradient Descent Optimizer, Adagrad Optimizer, Adam Optimizer and Momentum Optimizer are commonly used optimizers. Here we use an Adam Optimizer for instance. You're free to try any other one here. When\n</p>\n<p lang=\"zh-cn\">\n现在误差函数已经有了，我们需要把它的值告诉一个优化器（*optimizer*），并让它去尽可能向着缩小误差函数值得方向努力。这样，模型才能去学习、优化。常用的优化器包括Gradient Descent Optimizer，Adagrad Optimizer，Adam Optimizer以及Momentum Optimizer等等等等。选择优化器时，我们需要给它一个初始的学习速率。这里我用了一个\\\\(10^-3\\\\)，如果需要提高准确率，可能后期微调还需要进一步减小。代码如下：\n</p>\n```python\noptimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n```\n<p lang=\"en-us\">\nAlso, tell the optimizer that what the loss tensor is. The returned object is a train operation.\n</p>\n<p lang=\"zh-cn\">\n当然还要告诉它要减小的损失函数是哪个Tensor，这个函数返回的是一个训练操作（*train op*，一种特殊的运算，或者说操作）：\n</p>\n```python\ntrain = optimizer.minimize(loss)\n```\n<p lang=\"en-us\">\nThe neural network is finished. It's time to grab some data and train it.\n</p>\n<p lang=\"zh-cn\">\n其实到这里为止，神经网络已经搭建好了。是时候搞点数据来训练它了。\n</p>\n\n### 2.5 Feed the model with data, and train it!\n<p lang=\"en-us\">\nRemember how we defined the placeholders? It's time to fetch some data that fits the placeholders and train it. See how CIFAR-10 dataset can be fetched on its [website](https://www.cs.toronto.edu/~kriz/cifar.html).\n</p>\n<p lang=\"zh-cn\">\n还记得我们怎么定义那些placeholder吗？现在我们要把符合它们口径的数据灌进模型。那么来看一下CIFAR-10数据集[官网](https://www.cs.toronto.edu/~kriz/cifar.html)上是怎么描述的吧。它给了这么一段代码：\n</p>\n```python\ndef unpickle(file):\n    import pickle\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict\n```\n<p lang=\"en-us\">\nThe returned value *dict* is a Python dictionary. Every time we unpickle a file, a dictionary would be returned. Its 'data' key leads to 10000 RGB images of size 32x32, which is stored in a [10000, 3072] array(3072=32*32*3, I guess you know how it's stored now). The 'label' key leads to 10000 values in range 0~9. Obviously we have to reshape the data so as to fit it into the network model:\n</p>\n<p lang=\"zh-cn\">\n返回值*dict*是一个字典（Python的dict类型）。每读一个batch文件（比如data_batch_1），就会返回这样一个字典，它的“data”键值是10000张32x32的RGB图像（数组维数居然是[10000, 3072]，而3072=10000x32x32x3！实际上就是直接把所有像素、所有通道的值罗列在这里了）；“label”键值是10000个0-9之间的整数（代表类别）。显然，为了让数据能够成功放进模型，还需要对它进行一点处理：\n</p>\n```python\nbatch = unpickle(DATA_PATH + 'data_batch_{}'.format(i))  # 'i' is the loop variable\n\n# Read the image data\nimage_data = np.reshape(batch[b'data'], (10000, 32, 32, 3), 'F').astype(np.float32)   \nimage_data = image_data / 255                            # Cast range(0, 255) to range(0, 1)\nimage_data = np.transpose(image_data, (0, 2, 1, 3))      # Exchange row and column\n\n# Read the label data and convert into one-hot code\nlabel_data = batch[b'labels']\nnew_label_data = np.zeros((10000, 10))                   \nfor j in range(10000):\n    new_label_data[j][label_data[j]] = 1\n```\n<p lang=\"en-us\">\nThe details for data processing are not covered here. Try doing step-by-step to see the results.\nThe *image_data* and *new_label_data* are contain 10000 pieces of data each. Let's divide them into 100 small batches(100 elements each, including image and label) and feed it into the model. Do this on all the 5 batch files:\n</p>\n<p lang=\"zh-cn\">\n处理的细节不再赘述。你可以尝试一步一步运行来看看每一步的结果。\n这样我们拿到的*image_data*和*new_label_data*都是长度为10000的大batch，我们把它们各自分成100份，每次取100个图像+标记数据来塞进模型。对全部5个大batch文件来一遍：\n</p>\n```python\nwith tf.Session() as session:\n    session.run(tf.global_variables_initializer())\n    for j in range(100): # 10000 / BATCH_SIZE\n        # Divide them and get one part\n        image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]\n        label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]\n        \n        # Feed the model\n        session.run(train, feed_dict={\n            input_tensor: image_batch,\n            ground_truth: label_batch\n        })\n```\n<p lang=\"en-us\">\nA *session* - created with *tf.Session()* - is required every time we run a TensorFlow model, no matter when we're training it or evaluating it. The first time you run a model, you'll need to run *session.run(tf.global_variables_initializer())* to initialize the values of the TensorFlow variables defined previously.\nWhen running *session.run()*, you must first decide a TensorFlow operation(or a list of operations) that you need. If its result is dependent on some actual data(which means that some data in one or more placeholders flow to this operation), it's also required that you feed it the actual data by adding a *feed_dict* parameter. For example, I'm training this ResNet-56 model, in which a loss will be calculated with my *ground_truth* and the prediction result that comes from the *input_tensor*. Therefore, I'll have to give a value for each placeholder given above(format: \"placeholder name: corresponding data\"), and fold them in one Python dictionary.\n</p>\n<p lang=\"zh-cn\">\n每次运行一个TensorFlow模型（无论是训练还是测试）时，都需要通过tf.Session()创建一个*session*。第一次运行模型（而不是载入之前保存的模型）时，需要使用*session.run(tf.global_variables_initializer())*来初始化之前定义的一些可训练的TensorFlow变量。\n运行*session.run()*时，需要指定一个或一组你要执行的operation，作为这个函数唯一一个必要的参数。如果它的结果依赖于一些实际数据（也就是说在计算图中，一些数据会从placeholder流向这个operation），那么就需要通过填入*feed_dict*参数的值来填装训练或测试数据。以此模型为例，我在训练它时需要算误差函数值，这需要*ground_truth*数据和预测结果来计算，而预测结果又需要用输入图像*input_tensor*来计算得到。因此，我需要给这两个占位符分别给出对应的数据（格式：“占位符名：对应数据”），并把它们封在同一个Python字典中作为feed_dict参数的值。\n</p>\n<p lang=\"en-us\">\nI'm also interested in the loss function value in each iteration(which means feeding a batch of data and executing one forward-propagation and one back-propagation) in the training process. Therefore, what I'll fill in the parameter is not just the train op, but also the loss tensor. And the session.run() above should be modified to:\n</p>\n<p lang=\"zh-cn\">\n然而呢，我还想看看每次迭代（即把一个batch送进去，执行一次正向传播与反向传播这个过程）中损失函数变成了多大，来监控一下训练的效果。这样，需要session.run()的就不仅是那个train运算，还要加上loss运算。将上边的session.run()部分改为：\n</p>\n```python\n        [train_, loss_value] = session.run([train, loss],\n            feed_dict={\n                input_tensor: image_batch,\n                ground_truth: label_batch\n            })\n        print(\"Loss = {}\".format(loss_value)\n```\n<p lang=\"en-us\">\nThis is when the return value of session.run() becomes useful. Its value(s) - corresponding to the first parameter of run() - is/are the actual value(s) of the tensor(s) in the first parameter. In our example, *loss_value* is the actual output of the loss tensor. As for train_, we don't care what it is. Just add it to match the dimensions.\n</p>\n<p lang=\"zh-cn\">\n这时候，session.run()函数的返回值就有意义了。它与第一个参数的内容一一对应，分别是该参数中各个operation的实际输出值。像这个例子里边，*loss_value*接收的就是loss运算的输出内容。而train运算的输出我们并不关心，但是为了保证参数维度数与返回值一致，用一个train_变量来接收而已（实际上它的值是None）。\n</p>\n<p lang=\"en-us\">\nActually, one epoch(train the model once with the whole dataset) is not enough for the model to fully optimize. I trained this model for 40 epochs and added some loop variables to display the result. You can see my code and my output below. It's highly recommended that you train this with a high-performance GPU, or it would be a century before you train your model to a satisfactory degree.\n</p>\n<p lang=\"zh-cn\">\n实际上，一个epoch（把整个数据集都在模型里过一遍的周期）并不足以让模型充分学习。我把这个模型训练了40个epoch并且加了一些循环变量来输出结果。我的代码和结果如下。强烈建议用一个高性能GPU训练（如果手头没有，可以租一个GPU服务器），不然等别人把毕设论文逗写完的时候，你还在训练就很尴尬了。\n</p>\n```python\nimport tensorflow as tf\nimport numpy as np\nimport pickle\nfrom tensor_chain import TensorChain\n\ndef unpickle(file):\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict\n\nif __name__ == '__main__':\n    input_tensor = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n    ground_truth = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n\n    chain = TensorChain(input_tensor) \\\n            .convolution_layer_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 32, stride=2) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 64, stride=2) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .flatten() \\\n            .fully_connected_layer(10)\n\n    prediction = chain.output_tensor\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train = optimizer.minimize(loss)\n\n    with tf.Session() as session:\n        session.run(tf.global_variables_initializer())\n        iteration = 1\n        BATCH_SIZE = 100\n        DATA_PATH = '../data/cifar-10-batches-py/'\n        for epoch in range(1, 41):\n            for i in range(1, 6):\n                data = unpickle(DATA_PATH + 'data_batch_{}'.format(i))\n                image_data = np.reshape(data[b'data'], (10000, 32, 32, 3), 'F').astype(np.float32)\n                image_data = image_data / 255\n                image_data = np.transpose(image_data, (0, 2, 1, 3))\n                label_data = data[b'labels']\n                new_label_data = np.zeros((10000, 10))\n                for j in range(10000):\n                    new_label_data[j][label_data[j]] = 1\n                for j in range(int(10000 / BATCH_SIZE)):\n                    image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]\n                    label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]\n                    [train_, loss_] = session.run(\n                        [train, loss],\n                        feed_dict={\n                            input_tensor: image_batch,\n                            ground_truth: label_batch\n                        })\n                    iteration += 1\n                    print(\"Epoch {}, Iteration {}, Loss = {}\".format(epoch, iteration, loss_))\n```\n\n<div align=\"center\" class=\"figure\">\n<img src=\"/images/tftutorial/train.png\" alt=\"Training result\" width=\"40%\">\n\nFig. 2 Training result: cross entropy has dropped below 0.5\n</div>\n<p></p>\n\n### 2.6 Conclusion\n<p lang=\"en-us\">\nIn a word, building & training neural network models with TensorFlow involves the following steps:\n\n1. Decide the *input tensor*\n\n2. Add operations(*op*s) based on existing tensors\n\n3. Define the *loss* tensor, just like other tensors\n\n4. Select an *optimizer* and define the *train* op\n\n5. Process *data* and feed the model with them\n</p>\n<p lang=\"zh-cn\">\n总而言之，用TensorFlow建立、训练一个神经网络模型分以下几步：\n\n1. 定义*输入*Tensor\n\n2. 在已有的Tensor上添加运算（*op*）\n\n3. 像之前添加的那些运算一样，定义*损失*Tensor\n\n4. 选择一个*优化器*并定义*训练*操作\n\n5. 把*数据*处理为合适的shape，并喂进模型训练\n</p>\n\n## 3 A Closer Look\n<p lang=\"en-us\">\nWait, it's too late to leave now!\nTensorChain saved you from having to deal with a mess of TensorFlow classes and functions. Now it's time that we take a closer look at how TensorChain is implemented, thus understanding the native TensorFlow APIs.\n</p>\n<p lang=\"zh-cn\">\n别走呢喂！\nTensorChain让你不至于面对TensorFlow中乱糟糟的类型和函数而不知所措被水淹没。现在是时候近距离观察一下TensorChain是如何实现的，以便理解TensorFlowAPI了。\n</p>\n\n### 3.1 TensorFlow variables\n<p lang=\"en-us\">\nLet's begin with TensorFlow variables. Variables in TensorFlow are similar to variables in C, Java or any other strong typed programming languages - they have a type, though not necessarily explicitly decided upon definition. Usually them will change as the training process goes on, getting close to a best value.\nThe most commonly used variables in TensorFlow are weights and biases. I guess that you have seen formulae like:\n</p>\n<p lang=\"zh-cn\">\n先说TensorFlow的变量。TensorFlow的变量和C，Java以及其他强类型语言类似——都有一个类型，尽管不一定在它的定义时就显式地声明。通常它们会随着训练的进行而不断变化，达到一个最佳的值附近。\nTensorFlow中最常用的变量就是weights和biases（权重和偏置）。想必你应该见过这样的式子吧：\n</p>\n$$y=Wx+b$$\n<p lang=\"en-us\">\nThe \\\\(W\\\\) here is the weight, and the \\\\(b\\\\) here is the bias. When implementing some common network layers, they two are always used as the parameters in the layers. For instance, at the very beginning of our ResNet-56, we had a 3x3 sized convolution layer with 16 channels. Its implementation in TensorChain is:\n</p>\n<p lang=\"zh-cn\">\n这里\\\\(W\\\\)就是权重，\\\\(b\\\\)就是偏置。在定义一些常用的层时，我们往往也是用这两个变量作为这些层中的参数。比如说，在我们ResNet-56最开始，我们用到了一个3x3大小、16个通道的卷积层，TensorChain中，它的实现如下：\n</p>\n```python\n    def convolution_layer_2d(self, filter_size: int, num_channels: int, stride: int = 1, name: str = None,\n                             disable_log: bool = False):\n        \"\"\"\n        Add a 2D convolution layer\n        :param filter_size: Filter size(width and height) for this operation\n        :param num_channels: Channel number of this filter\n        :param stride: Stride for this convolution operation\n        :param name: The name of the tensor\n        :param disable_log: Set it True if you don't want this layer to be recorded\n        :return: This object itself\n        \"\"\"\n        filter = self._weights([filter_size, filter_size, self.num_channels, num_channels], layer_name=name,\n                               suffix='filter')\n        bias = self._bias([num_channels], layer_name=name)\n        self.num_channels = num_channels\n        self.output_tensor = tf.nn.conv2d(self.output_tensor, filter,\n                                          [1, stride, stride, 1], 'SAME', name=name)\n        self.output_tensor = tf.add(self.output_tensor, bias)\n        self._log_layer(\n            '2D Convolution layer, filter size = {}x{}, stride = {}, {} channels'.format(filter_size, filter_size,\n                                                                                         stride,\n                                                                                         num_channels),\n            disable=disable_log)\n        return self\n```\n<p lang=\"en-us\">\nSee? On line 16, we used a *tf.nn.conv2d()* function, the parameters of which are *input*, *filter*, *strides*, *padding*, etc. As can be guessed from the names, this function does a convolution operation with out input and the weights(the convolution *filter* here). A *bias* is added to the result as the final output. There are also many people who argue that the bias here is meaningless and should removed. One line of code is sufficient for defining a variable:\n</p>\n<p lang=\"zh-cn\">\n看见了吧？16行上，我们用了一个*tf.nn.conv2d()*函数，它的参数是*input*，*filter*，*strides*，*padding*等等。顾名思义，这个函数就是用我们定义的权重Tensor*filter*（在这里称之为卷积核）来与这一层的输入input做了一次运算。运算的结果加上了偏置Tensor*bias*，作为这个卷积层的最终输出。很多人认为这里的偏置bias意义不明，因此他们在卷积之后没有加上这样的一个bias变量。定义一个变量只需要这样一个语句：\n</p>\n```python\ntf.Variable(tf.truncated_normal(shape, stddev=sigma), dtype=tf.float32, name=suffix)\n```\n<p lang=\"en-us\">\nTo define weight or bias variables, create a *tf.Variable* object. Usually you'll need to give the *initial_value* which also decides the shape of this tensor. *tf.truncated_normal()* and *tf.constant()* are usually used as the initial values. Also, other APIs - function *tf.get_variable()* and package *tf.initializers* are frequently used when using some more methods for initialization. I strongly recommend that you try using these APIs yourself.\n</p>\n<p lang=\"zh-cn\">\n要定义权重或者偏置变量，请创建一个*tf.Variable*对象。通常情况下，你会需要给出*initial_value*（TF变量的初始值），这将顺便定义了这个变量的shape（因为初始值的shape是确定的）。另外，一些新的API——*tf.get_variable()*函数和*tf.initializers*包也常用与的参数初始化，以实现更多样的初始化方法。我强烈建议自己动手实践一下，试一试这些API。\n</p>\n\n### 3.2 Tensors and operations\n<p lang=\"en-us\">\nGoing on with the parameters of the *tf.nn.conv2d()* function. The required parameters also include *strides* and *padding*. You should have already learned about what strides mean in convolution, and I'll only talk about their formats. *strides* require a 1-D vector with a length of 4, like [1, 2, 2, 1]. The 1st and the 4th number is always 1(in order to match dimensions with the input), while the 2nd and the 3rd means the vertical stride and the horizonal stride. \nThe 4th parameter *padding* is a little bit different from its definition in convolution operation. It requires 'SAME' of 'VALID', denoting 'with' or 'without' zero paddings. When it's 'SAME', zero padding is introduced to make the shapes match as needed, equally on every side of the input map.\n</p>\n<p lang=\"zh-cn\">\n接着说*tf.nn.conv2d()*函数的参数。需要的参数还包括*strides*和*padding*。你应该在了解卷积运算时学过stride（步幅）含义，我只说一下它的格式吧。*strides*参数需要是一个1维、长度为4的向量。第一位和第四位永远都是1，第二位和第三位分别是竖直方向和水平方向的步幅长。维持这个形式只是为了与输入的数据维度匹配，因此API看起来非常蹩脚。\n第四个参数*padding*和卷积运算里的padding不太相同。它的值只能是'SAME'或'VALID'，分别代表“带”和“不带”零补全。如果是'SAME'的话，函数会均匀地在图像的上下左右使用零补全来使得运算结果与之前尽可能保持一致。（stride>1时有可能输出尺寸不是正好等于原来的尺寸/stride，因为补全问题）\n</p>\n<p lang=\"en-us\">\ntf.nn.conv2d() is just an example of TensorFlow *operations*. Other functions like *tf.matmul()*, *tf.reduce_mean()*, *tf.global_variables_initializer()*, *tf.losses.softmax_cross_entropy()*, *tf.truncated_normal()* are all operations. Operation functions return tensors(*tf.truncated_normal* also return a tensor, a tensor with initializers).\n</p>\n<p lang=\"zh-cn\">\ntf.nn.conv2d()只是TensorFlow运算（*operation*）的一个例子。其他例如*tf.matmul()*，*tf.reduce_mean()*，*tf.nn.relu()*，*tf.batch_normalization()*，*tf.global_variables_initializer()*，*tf.losses.softmax_cross_entropy()*，*tf.truncated_normal()*之类的函数也都是TensorFlow的运算。TensorFlow的运算函数会返回一个Tensor对象（包括*tf.truncated_normal()*也是！它只不过返回的是一个带初始化器的Tensor而已）。\n</p>\n<p lang=\"en-us\">\nAll the functions in the TensorChain class are based on the most basic TensorFlow operations and variables. After learning about these basic TensorFlow concepts, actually you can already abandon TensorChain, go and try implementing your own neural networks yourself!\n</p>\n<p lang=\"zh-cn\">\nTensorChain类中的所有成员函数都是基于最基本的TensorFlow运算和变量的。实际上，了解了这些，你现在已经可以抛开TensorChain的束缚，去尝试实现你自己的神经网络了！\n</p>\n\n## 4 Spices\n<p lang=\"en-us\">\nI'm not joking just now! But I know that there are a lot of things that you still don't understand about using TensorFlow - like \"how do I visualize my computation graph\", \"how do I save/load my model to/from files\", \"how do I record some tensors' values while training\" or \"how do I view the loss curves\" - after all TensorFlow APIs are far more complicated than just building those nets. Those are also important techniques in your research. If you'd rather ask me than spending some time experimenting, please go on with reading.\n</p>\n<p lang=\"zh-cn\">\n我，我真没开玩笑！但是我知道关于如何使用TensorFlow，你还有许许多多的问题，好比“如何可视化地查看我的计算图结构”、“如何存储/读取模型文件”、“如何记录训练过程中某些Tensor的真实值”、“如何查看损失函数的变化曲线”——毕竟TensorFlow的API太复杂了，远比搭建神经网络那点函数复杂得多。上边说的那些是你使用TensorFlow研究过程中的重要技巧。如果你愿意听我讲而不想花些时间尝试的话，请继续读下去。\n</p>\n\n### 4.1 Saving and loading your model\n<p lang=\"en-us\">\nThe very first thing that you may want to do - after training a network model with nice outcomes - would be saving it. Saving a model is fairly easy - just use a *tf.train.Saver* object. See my code below:\n</p>\n<p lang=\"zh-cn\">\n训练出一个看起来输出还不错的神经网络模型后你想做的第一件事恐怕就是把它存下来了吧？保存模型其实非常简单：只要用一个*tf.train.Saver*类的对象。代码示例：\n</p>\n```python\nwith tf.Session() as session:\n    # Train it for some iterations\n    # Train it for some iterations\n    # Train it for some iterations\n    saver = tf.train.Saver()\n    saver.save(session, 'models/model.ckpt')\n```\n<p lang=\"en-us\">\nI saved my model and variable values to 'models/model.ckpt'. But actually, you'll find 3 files in the 'models' directory - *model.ckpt.data-00000-of-00001*, *model.ckpt.meta* and *model.ckpt.index* - none of which is 'model.ckpt'! That's because TensorFlow stores the graph structure separately from variables values. The *.meta* file describes the saved graph structure; the *.index* file records the mappings between tensor names and tensor metadata; and the *.data-00000-of-00001* file - which is always the biggest one - saves all the variable values. If you need the graph data together with the variable values to be loaded, use a Saver to load after creating a session:\n</p>\n<p lang=\"zh-cn\">\n我把我的模型和变量值存到了'models/model.ckpt'文件里。但是！实际上在models目录里你会找到三个文件：*model.ckpt.data-00000-of-00001*，*model.ckpt.meta*和*model.ckpt.index*——哪个也不是model.ckpt呀？那是因为TensorFlow把计算图的结构和图中各种变量的值分开存放了。*.meta*文件描述计算图的结构；*.index*文件记录各个Tensor名称（是name属性，而不是变量名）与Tensor元信息之间的映射；*.data-00000-of-00001*文件往往是最大的一个，它存储的是各个TensorFlow变量的实际值。如果读取时需要把图结构和变量值都读进来，在session创建以后，同样用一个Saver来读取即可：\n</p>\n```python\nwith tf.Session() as session:\n    saver = tf.train.Saver()\n    saver.restore(session, 'models/model.ckpt')\n    # Then continue doing everything just like the model is just trained\n```\n<p lang=\"en-us\">\nRemember that session.run(tf.global_variables_initializer()) shouldn't be executed, since variables are already initialized with your saved *.data-0000-of-00001* file.\nIf you only need the graph to be loaded, only use the *.meta* file:\n</p>\n<p lang=\"zh-cn\">\n记住，这时候就不要再去执行session.run(tf.global_variables_initializer())了，因为变量已经用存储的checkpoint文件内容初始化过了。\n如果只需要读取计算图结构，只要读取*.meta*文件：\n</p>\n```python\nwith tf.Session() as session:\n    tf.train.import_meta_graph('models/model.ckpt.meta')\n    # Then continue doing everything just like the model is just built\n```\n<p lang=\"en-us\">\nFunction *tf.train.import_meta_graph()* loads(appends) the graph to your current computation graph. The values of tensors are still uninitialized so you'll have to execute session.run(tf.global_variables_initializer()) again. The tensors that you defined in the model can be retrieved by their names(property of the Tensor objects, instead of Python variable names). For example:\n</p>\n<p lang=\"zh-cn\">\n*tf.train.import_meta_graph()*函数将文件里的计算图读到（添加到）你当前的计算图中。其中所有Tensor的值仍未初始化，所以有必要执行一下session.run(tf.global_variables_initializer())了。之前定义的变量可以按照名称取回，示例：\n</p>\n```python\nwith tf.Session() as session:\n    # Recover the model here\n\n    graph = tf.get_default_graph()\n    image_tensor = graph.get_tensor_by_name('input_image:0')\n    loss = graph.get_tensor_by_name('loss:0')\n    train = graph.get_operation_by_name('train)\n```\n<p lang=\"en-us\">\nTo retrieve normal tensors, you'll have to append a *':0'* to the name of the op. This means getting the associated tensor of the op. *train* is a little special - we only need the op, so the function is *get_operation_by_name()* so the ':0' is not necessary.\n</p>\n<p lang=\"zh-cn\">\n要取回一般的Tensor，需要在Tensor的name属性值后边加一个*':0'*，意思是取这个运算对应的Tensor。训练操作*train*略有不同——我们要的就只是这个op，所以用的函数*get_operation_by_name()*跟其他Tensor不一样，而且':0'也不需要加。\n</p>\n\n<p lang=\"en-us\" align=\"center\">\n[THIS SECTION IS UNDER CONSTRUCTION]\n</p>\n<p lang=\"zh-cn\" align=\"center\">\n[本部分内容施工中]\n</p>","slug":"diy-resnet","published":1,"updated":"2018-04-29T14:05:43.898Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomfkrfh0002zcvj3iyyw0rm","content":"<p><script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML\" async></script>\n<br></p>\n<p lang=\"en-us\">\nTensorFlow is a powerful open-source deep learning framework, supporting various languages including Python. However, its APIs are far too complicated for a beginner in deep learning(especially those who are new to Python). In order to ease the pain of having to understand the mess of various elements in TensorFlow computation graphs, I made this tutorial to help beginners take the first bite of the cake.<br>ResNets are one of the greatest works in the deep learning field. Although they look scary with extreme depths, it&#39;s not a hard job to implement one. Now let&#39;s build one of the simplest ResNets - ResNet-56, and train it on the CIFAR-10 dataset.<br></p>\n<a id=\"more\"></a>\n<p lang=\"zh-cn\">\nTensorFlow是一个强大的开源深度学习软件库，它支持包括Python在内的多种语言。然而，由于API过于复杂（实际上还有点混乱），它往往使得一个深度学习的初学者（尤其是为此初学Python的那些）望而却步——老虎吃天，无从下口。为了减轻初学者不得不尝试理解TensorFlow中的大量概念的痛苦，我213今天带各位尝尝深度学习这片天的第一口。\nResNet是深度学习领域的一个重磅炸弹，尽管它们（ResNet有不同层数的多个模型）的深度看上去有点吓人，但实际上实现一个ResNet并不难。接下来，我们来实现一个较为简单的ResNet——ResNet-56，并在CIFAR-10数据集上训练一下，看看效果如何。\n</p>\n<div align=\"center\" class=\"figure\">\n<img src=\"/images/tftutorial/oyo.gif\" alt=\"Let&#39;s Rock!\">\n</div>\n<p lang=\"en-us\">\nFirst let&#39;s take a look at ResNet-56. It&#39;s proposed by Kaiming He et al., and is designed to confirm the effect of residual networks. It has 56 weighted layers, deep but simple. The structure is shown in the figure below:\n</p>\n<p lang=\"zh-cn\">\n首先来看一下ResNet-56这个神经网络。它是何凯明等在ResNet论文中提出的、用于验证残差网络效果的一个相对简单的残差网络（尽管它很深，深度达到了56个权重层）。图示如下：\n</p>\n<div align=\"center\" class=\"figure\">\n<img src=\"/images/tftutorial/resnet56.png\" alt=\"ResNet-56\" width=\"80%\"><br>Fig. 1 The structure of ResNet-56\n</div>\n<br>\n<p lang=\"en-us\">\nSeems a little bit long? Don&#39;t worry, let&#39;s do this step by step.\n</p>\n<p lang=\"zh-cn\">\n看起来有点长了是不是？别担心，我们一步一步来做。\n</p>\n\n<h2 id=\"1-Ingredients\"><a href=\"#1-Ingredients\" class=\"headerlink\" title=\"1 Ingredients\"></a>1 Ingredients</h2><p>Python 3.6</p>\n<p>TensorFlow 1.4.0</p>\n<p>Numpy 1.13.3</p>\n<p>OpenCV 3.2.0</p>\n<p><a href=\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\" target=\"_blank\" rel=\"noopener\">CIFAR-10 Dataset</a></p>\n<p lang=\"en-us\">\nAlso prepare some basic knowledge on Python programming, digital image processing and convolutional neural networks. If you are already capable of building, training and validating your own neural networks with TensorFlow, you don&#39;t have to read this post.\n</p>\n<p lang=\"zh-cn\">\n另外，请确保自己有一点点Python编程、数字图像处理和卷积神经网络的知识储备。如果你已经具备用TensorFlow自行搭建神经网络并进行训练、测试的能力，就不必阅读本文了。\n</p>\n\n<h2 id=\"2-Recipe\"><a href=\"#2-Recipe\" class=\"headerlink\" title=\"2 Recipe\"></a>2 Recipe</h2><h3 id=\"2-0-Prepare-the-tools\"><a href=\"#2-0-Prepare-the-tools\" class=\"headerlink\" title=\"2.0 Prepare the tools\"></a>2.0 Prepare the tools</h3><p lang=\"en-us\">\nPrepare(import) the tools for our project, including all that I mentioned above. Like this :P\n</p>\n<p lang=\"zh-cn\">\n准(i)备(m)所(p)需(o)工(r)具(t)，上一部分已提到过。如下：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> cv2</span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensor_chain <span class=\"keyword\">import</span> TensorChain</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nWait... What&#39;s this? TensorChain? Another deep learning framework like TensorFlow?<br>Uh, nope. This is my own encapsulation of some TensorFlow APIs, for the sake of easing your pain. You&#39;ll only have to focus on &quot;what&#39;s what&quot; in the beginning. We&#39;ll look into my implementation of this encapsulation later, when you are clear how everything goes. Please <a href=\"/files/tensor_chain.py\">download this file</a> and put it where your code file is, and import it.\n</p>\n<p lang=\"zh-cn\">\n等等...最后这个是个什么鬼？ TensorChain？另一个深度学习框架吗？<br>呃...并不是。这个是我对一些TensorFlow API的封装，为了减轻你的痛苦才做的。作为初学者，你只需要关注用TensorFlow搭建网络模型的这个过程，分清东西南北。回头等你弄清了大体流程后，我们再来看这个的实现细节。请先下载<a href=\"/files/tensor_chain.py\">这个文件</a>并把它与你的代码放在同一文件夹下，然后就可以import了。\n</p>\n\n<h3 id=\"2-1-Decide-the-input\"><a href=\"#2-1-Decide-the-input\" class=\"headerlink\" title=\"2.1 Decide the input\"></a>2.1 Decide the input</h3><p lang=\"en-us\">\nEvery neural network requires an input - you always have to identify the details of a question, before asking the computer to solve it. All of the variable, constant in TensorFlow are objects of type <em>tf.Tensors</em>. And the <em>tf.placeholder</em> of our input(s) is a special one. Images in CIFAR-10 dataset are RGB images(3 channels) of 32x32(really small), so our input should shaped like [32, 32, 3]. Also, we want to input a little <em>batch</em> of multiple images. Therefore, our input data should be an array of shape <em>[?, 32, 32, 3]</em>. Unknown dimension size can be marked as None, and it will be clear when we feed the model with the actual images. It&#39;s coded like this:\n</p>\n<p lang=\"zh-cn\">\n每个神经网络都需要有输入——毕竟你想找电脑解决一些问题的话，你总得告诉它问题的一些细节吧？TensorFlow中所有的变量、常量都是<em>tf.Tensor</em>类型的对象，作为输入内容的占位符<em>tf.placeholder</em>也是（只不过比较特殊而已）。CIFAR-10数据集的图像都是32x32尺寸（好小哇）的RGB图像（RGB即彩色图像的三个通道），因此我们的输入给神经网络的内容将会像是[32, 32, 3]这个样子。另外呢，我们需要输入的是一个小<em>batch</em>（批）的图像，因此，输入网络的图像数据将会是一个<em>[?, 32, 32, 3]</em>的数组（也可以是numpy数组）。未知的维度大小用None代指就好，我们之后给模型喂实际图像batch时，它自然就清楚了。代码如下：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input_tensor = tf.placeholder(dtype=tf.float32, shape=[<span class=\"keyword\">None</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>])</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\n<em>Ground truth</em> data also need to be known in supervised learning, so we also have to define a placeholder for the ground truth data:<br></p>\n<p lang=\"zh-cn\">\n监督学习中，正确标注的数据（英文为<em>ground truth</em>，目前貌似没有对这个名词的合理翻译）也是需要输入到模型中的。因此再给ground truth定义一个placeholder：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ground_truth = tf.placeholder(dtype=tf.float32, shape=[<span class=\"keyword\">None</span>, <span class=\"number\">10</span>])</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nWe want the label data to be in the one-hot encoding format, which means an array of length 10, denoting 10 classes. Only on one position is a &#39;1&#39;, and on other positions are &#39;0&#39;s.\n</p>\n<p lang=\"zh-cn\">\n我们需要标记的数据呈One-Hot编码格式（又称为一位有效编码），意思是如果有10个类别，那么数组长度就是10，每一位代表一个类别。只有一个位置上是1（代表图片被分为这个类），其他位上都是0。\n</p>\n\n<h3 id=\"2-2-Do-some-operations\"><a href=\"#2-2-Do-some-operations\" class=\"headerlink\" title=\"2.2 Do some operations\"></a>2.2 Do some operations</h3><p lang=\"en-us\">\nFor now, let&#39;s use our TensorChain to build it fast. Under most circumstances that we may face, the computations are based on the input data or the result of the former computation, so our network(or say, the most of it) look more like a chain than a web. Every time we add some new operation(layer), we add it to our <em>TensorChain</em> object. Just remember to get the <em>output_tensor</em> of this object(denoting the output tensor of the last operation on the chain) when you need to ue native TensorFlow API.\nThe construction function of TensorChain class requires a Tensor object as the parameter, which is also the input tensor of this chain. As we mentioned earlier, all we have to do is add operations. See my ResNet-56 code:\n</p>\n<p lang=\"zh-cn\">\n现在呢，我们先用TensorChain来快速盖楼。因为我们遇到的大多数情况下，所有的计算都是在输入数据或者这个计算的前一个计算结果基础上进行的，所以我们的网络（至少是它的绝大部分）会看起来像个链而不是所谓的网。每次我们添加一个新的运算（层），我们会把它加到这个独一无二的TensorChain对象。只要记得在使用原生TensorFlow API前把它的<em>output_tensor</em>属性（也就是这条链上最后一个运算的输出Tensor）取出来就好了。\nTensorChain类的构造函数需要一个Tensor对象作为参数，这个对象也正是被拿来作为这个链的输入层。正如我们之前所说的，只要在这个对象上添加运算即可。写个ResNet-56，代码很简单：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chain = TensorChain(input_tensor) \\</span><br><span class=\"line\">        .convolution_layer_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>, stride=<span class=\"number\">2</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>, stride=<span class=\"number\">2</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .flatten() \\</span><br><span class=\"line\">        .fully_connected_layer(<span class=\"number\">10</span>)</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nThis is it? Right, this is it! Isn&#39;t it cool? Didn&#39;t seem that high, huh? That&#39;s because I encapsulated that huge mess of weights and biases, only leaving a few parameters that decide the structure of the network. Later in this pose we&#39;ll talk about the actual work that these functions do.\n</p>\n<p lang=\"zh-cn\">\n就这？没错呀，就这！稳不稳？似乎看起来也没56层那么高呀？毕竟这些函数被我封装得太严实了，只留出几个决定网络结构的几个参数供修改。这篇博客后边就会讲到这些函数究竟干了点什么事儿。\n</p>\n\n<h3 id=\"2-3-Define-the-loss\"><a href=\"#2-3-Define-the-loss\" class=\"headerlink\" title=\"2.3 Define the loss\"></a>2.3 Define the loss</h3><p lang=\"en-us\">\nIn supervised learning, you always have to tell the learning target to the model. To tell the model how to optimize, you have to let it know how, how much, on which direction should it change its parameters. This is done by using a loss function. Therefore, we need to define a loss function for our ResNet-56 model(which we designed for this classification problem) so that it will learn and optimize.\nA commonly used loss function in classification problems is cross entropy. It&#39;s defined below:\n</p>\n<p lang=\"zh-cn\">\n搞监督学习，总是要让模型按照“参考答案”去改的。要改就得让它知道怎么改、改多少、往什么方向改，这也就是<em>loss</em>（损失函数）的功劳。因此，像我们这个拿来做分类问题的ResNet-56，我们要给它定义一个损失函数来让它学习、优化。\n分类问题上一个常用的损失函数是交叉熵。定义如下式：\n</p>\n$$C=-\\frac{1}{n}\\sum_x{y\\ln a+(1-y)\\ln(1-a)}$$\n<p lang=\"en-us\">\nin which \\(y\\) is the expected(or say correct) output and \\(a\\) is the actual output.\nThis seems a little bit complicated. But it&#39;s not a hard job to implement, since TensorFlow implemented it already! You can also try and implement it yourself within one line if you want. For now we use the pre-defined cross entropy loss function:\n</p>\n<p lang=\"zh-cn\">\n其中\\(y\\)为期望输出（或者说参考答案），\\(a\\)为实际输出。\n略复杂呀...这个用程序怎么写？其实也不难。。。毕竟TensorFlow都帮我们实现好啦！（有兴趣的话也可以自己尝试着写一下，同样一行代码即可搞定）现在你只需要来这么一句：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nand it returns a tf.Tensor that denotes an average of cross entropies(don&#39;t forget that this is a batch). As for the &#39;softmax&#39; before the &#39;cross_entropy&#39;, it&#39;s a function that project the data in an array to range 0~1, which allows us to do a comparison between our prediction and the ground truth(in one-hot code). The definition is simple too:<br></p>\n<p lang=\"zh-cn\">\n就可以创建一个表示交叉熵平均值（别忘了这可是一个batch）的Tensor了。至于cross_entropy前边的那个<em>softmax</em>呢，它的作用是把输入的数组内数据归一化，投射到0~1的范围内（实际上就是相当于把exp(数组各项的值)的当做频数，求出一个概率），这样子才能跟实际数据做一个比较。定义也比较简单：<br></p>\n$$S_i=\\frac{e^{V_i}}{\\sum_j{e^{V_j}}}$$\n<p></p>\n\n<h3 id=\"2-4-Define-the-train-op\"><a href=\"#2-4-Define-the-train-op\" class=\"headerlink\" title=\"2.4 Define the train op\"></a>2.4 Define the train op</h3><p lang=\"en-us\">\nNow we have the loss function. We&#39;ll have to tell its value to an <em>optimizer</em>, which make our model learn and optimize in order to minimize the loss value. Gradient Descent Optimizer, Adagrad Optimizer, Adam Optimizer and Momentum Optimizer are commonly used optimizers. Here we use an Adam Optimizer for instance. You&#39;re free to try any other one here. When\n</p>\n<p lang=\"zh-cn\">\n现在误差函数已经有了，我们需要把它的值告诉一个优化器（<em>optimizer</em>），并让它去尽可能向着缩小误差函数值得方向努力。这样，模型才能去学习、优化。常用的优化器包括Gradient Descent Optimizer，Adagrad Optimizer，Adam Optimizer以及Momentum Optimizer等等等等。选择优化器时，我们需要给它一个初始的学习速率。这里我用了一个\\(10^-3\\)，如果需要提高准确率，可能后期微调还需要进一步减小。代码如下：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">optimizer = tf.train.AdamOptimizer(learning_rate=<span class=\"number\">0.001</span>)</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nAlso, tell the optimizer that what the loss tensor is. The returned object is a train operation.\n</p>\n<p lang=\"zh-cn\">\n当然还要告诉它要减小的损失函数是哪个Tensor，这个函数返回的是一个训练操作（<em>train op</em>，一种特殊的运算，或者说操作）：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train = optimizer.minimize(loss)</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nThe neural network is finished. It&#39;s time to grab some data and train it.\n</p>\n<p lang=\"zh-cn\">\n其实到这里为止，神经网络已经搭建好了。是时候搞点数据来训练它了。\n</p>\n\n<h3 id=\"2-5-Feed-the-model-with-data-and-train-it\"><a href=\"#2-5-Feed-the-model-with-data-and-train-it\" class=\"headerlink\" title=\"2.5 Feed the model with data, and train it!\"></a>2.5 Feed the model with data, and train it!</h3><p lang=\"en-us\">\nRemember how we defined the placeholders? It&#39;s time to fetch some data that fits the placeholders and train it. See how CIFAR-10 dataset can be fetched on its <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\" target=\"_blank\" rel=\"noopener\">website</a><br></p>\n<p lang=\"zh-cn\">\n还记得我们怎么定义那些placeholder吗？现在我们要把符合它们口径的数据灌进模型。那么来看一下CIFAR-10数据集<a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\" target=\"_blank\" rel=\"noopener\">官网</a>上是怎么描述的吧。它给了这么一段代码：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">unpickle</span><span class=\"params\">(file)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">import</span> pickle</span><br><span class=\"line\">    <span class=\"keyword\">with</span> open(file, <span class=\"string\">'rb'</span>) <span class=\"keyword\">as</span> fo:</span><br><span class=\"line\">        dict = pickle.load(fo, encoding=<span class=\"string\">'bytes'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dict</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nThe returned value <em>dict</em> is a Python dictionary. Every time we unpickle a file, a dictionary would be returned. Its &#39;data&#39; key leads to 10000 RGB images of size 32x32, which is stored in a [10000, 3072] array(3072=32<em>32</em>3, I guess you know how it&#39;s stored now). The &#39;label&#39; key leads to 10000 values in range 0~9. Obviously we have to reshape the data so as to fit it into the network model:\n</p>\n<p lang=\"zh-cn\">\n返回值<em>dict</em>是一个字典（Python的dict类型）。每读一个batch文件（比如data_batch_1），就会返回这样一个字典，它的“data”键值是10000张32x32的RGB图像（数组维数居然是[10000, 3072]，而3072=10000x32x32x3！实际上就是直接把所有像素、所有通道的值罗列在这里了）；“label”键值是10000个0-9之间的整数（代表类别）。显然，为了让数据能够成功放进模型，还需要对它进行一点处理：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch = unpickle(DATA_PATH + <span class=\"string\">'data_batch_&#123;&#125;'</span>.format(i))  <span class=\"comment\"># 'i' is the loop variable</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Read the image data</span></span><br><span class=\"line\">image_data = np.reshape(batch[<span class=\"string\">b'data'</span>], (<span class=\"number\">10000</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>), <span class=\"string\">'F'</span>).astype(np.float32)   </span><br><span class=\"line\">image_data = image_data / <span class=\"number\">255</span>                            <span class=\"comment\"># Cast range(0, 255) to range(0, 1)</span></span><br><span class=\"line\">image_data = np.transpose(image_data, (<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">3</span>))      <span class=\"comment\"># Exchange row and column</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Read the label data and convert into one-hot code</span></span><br><span class=\"line\">label_data = batch[<span class=\"string\">b'labels'</span>]</span><br><span class=\"line\">new_label_data = np.zeros((<span class=\"number\">10000</span>, <span class=\"number\">10</span>))                   </span><br><span class=\"line\"><span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">10000</span>):</span><br><span class=\"line\">    new_label_data[j][label_data[j]] = <span class=\"number\">1</span></span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nThe details for data processing are not covered here. Try doing step-by-step to see the results.\nThe <em>image_data</em> and <em>new_label_data</em> are contain 10000 pieces of data each. Let&#39;s divide them into 100 small batches(100 elements each, including image and label) and feed it into the model. Do this on all the 5 batch files:\n</p>\n<p lang=\"zh-cn\">\n处理的细节不再赘述。你可以尝试一步一步运行来看看每一步的结果。\n这样我们拿到的<em>image_data</em>和<em>new_label_data</em>都是长度为10000的大batch，我们把它们各自分成100份，每次取100个图像+标记数据来塞进模型。对全部5个大batch文件来一遍：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    session.run(tf.global_variables_initializer())</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">100</span>): <span class=\"comment\"># 10000 / BATCH_SIZE</span></span><br><span class=\"line\">        <span class=\"comment\"># Divide them and get one part</span></span><br><span class=\"line\">        image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class=\"line\">        label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># Feed the model</span></span><br><span class=\"line\">        session.run(train, feed_dict=&#123;</span><br><span class=\"line\">            input_tensor: image_batch,</span><br><span class=\"line\">            ground_truth: label_batch</span><br><span class=\"line\">        &#125;)</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nA <em>session</em> - created with <em>tf.Session()</em> - is required every time we run a TensorFlow model, no matter when we&#39;re training it or evaluating it. The first time you run a model, you&#39;ll need to run <em>session.run(tf.global_variables_initializer())</em> to initialize the values of the TensorFlow variables defined previously.\nWhen running <em>session.run()</em>, you must first decide a TensorFlow operation(or a list of operations) that you need. If its result is dependent on some actual data(which means that some data in one or more placeholders flow to this operation), it&#39;s also required that you feed it the actual data by adding a <em>feed_dict</em> parameter. For example, I&#39;m training this ResNet-56 model, in which a loss will be calculated with my <em>ground_truth</em> and the prediction result that comes from the <em>input_tensor</em>. Therefore, I&#39;ll have to give a value for each placeholder given above(format: &quot;placeholder name: corresponding data&quot;), and fold them in one Python dictionary.\n</p>\n<p lang=\"zh-cn\">\n每次运行一个TensorFlow模型（无论是训练还是测试）时，都需要通过tf.Session()创建一个<em>session</em>。第一次运行模型（而不是载入之前保存的模型）时，需要使用<em>session.run(tf.global_variables_initializer())</em>来初始化之前定义的一些可训练的TensorFlow变量。\n运行<em>session.run()</em>时，需要指定一个或一组你要执行的operation，作为这个函数唯一一个必要的参数。如果它的结果依赖于一些实际数据（也就是说在计算图中，一些数据会从placeholder流向这个operation），那么就需要通过填入<em>feed_dict</em>参数的值来填装训练或测试数据。以此模型为例，我在训练它时需要算误差函数值，这需要<em>ground_truth</em>数据和预测结果来计算，而预测结果又需要用输入图像<em>input_tensor</em>来计算得到。因此，我需要给这两个占位符分别给出对应的数据（格式：“占位符名：对应数据”），并把它们封在同一个Python字典中作为feed_dict参数的值。\n</p>\n<p lang=\"en-us\">\nI&#39;m also interested in the loss function value in each iteration(which means feeding a batch of data and executing one forward-propagation and one back-propagation) in the training process. Therefore, what I&#39;ll fill in the parameter is not just the train op, but also the loss tensor. And the session.run() above should be modified to:\n</p>\n<p lang=\"zh-cn\">\n然而呢，我还想看看每次迭代（即把一个batch送进去，执行一次正向传播与反向传播这个过程）中损失函数变成了多大，来监控一下训练的效果。这样，需要session.run()的就不仅是那个train运算，还要加上loss运算。将上边的session.run()部分改为：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[train_, loss_value] = session.run([train, loss],</span><br><span class=\"line\">    feed_dict=&#123;</span><br><span class=\"line\">        input_tensor: image_batch,</span><br><span class=\"line\">        ground_truth: label_batch</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">print(<span class=\"string\">\"Loss = &#123;&#125;\"</span>.format(loss_value)</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nThis is when the return value of session.run() becomes useful. Its value(s) - corresponding to the first parameter of run() - is/are the actual value(s) of the tensor(s) in the first parameter. In our example, <em>loss_value</em> is the actual output of the loss tensor. As for train_, we don&#39;t care what it is. Just add it to match the dimensions.\n</p>\n<p lang=\"zh-cn\">\n这时候，session.run()函数的返回值就有意义了。它与第一个参数的内容一一对应，分别是该参数中各个operation的实际输出值。像这个例子里边，<em>loss_value</em>接收的就是loss运算的输出内容。而train运算的输出我们并不关心，但是为了保证参数维度数与返回值一致，用一个train_变量来接收而已（实际上它的值是None）。\n</p>\n<p lang=\"en-us\">\nActually, one epoch(train the model once with the whole dataset) is not enough for the model to fully optimize. I trained this model for 40 epochs and added some loop variables to display the result. You can see my code and my output below. It&#39;s highly recommended that you train this with a high-performance GPU, or it would be a century before you train your model to a satisfactory degree.\n</p>\n<p lang=\"zh-cn\">\n实际上，一个epoch（把整个数据集都在模型里过一遍的周期）并不足以让模型充分学习。我把这个模型训练了40个epoch并且加了一些循环变量来输出结果。我的代码和结果如下。强烈建议用一个高性能GPU训练（如果手头没有，可以租一个GPU服务器），不然等别人把毕设论文逗写完的时候，你还在训练就很尴尬了。\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensor_chain <span class=\"keyword\">import</span> TensorChain</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">unpickle</span><span class=\"params\">(file)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> open(file, <span class=\"string\">'rb'</span>) <span class=\"keyword\">as</span> fo:</span><br><span class=\"line\">        dict = pickle.load(fo, encoding=<span class=\"string\">'bytes'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dict</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    input_tensor = tf.placeholder(dtype=tf.float32, shape=[<span class=\"keyword\">None</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>])</span><br><span class=\"line\">    ground_truth = tf.placeholder(dtype=tf.float32, shape=[<span class=\"keyword\">None</span>, <span class=\"number\">10</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    chain = TensorChain(input_tensor) \\</span><br><span class=\"line\">            .convolution_layer_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>, stride=<span class=\"number\">2</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>, stride=<span class=\"number\">2</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .flatten() \\</span><br><span class=\"line\">            .fully_connected_layer(<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    prediction = chain.output_tensor</span><br><span class=\"line\">    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))</span><br><span class=\"line\"></span><br><span class=\"line\">    optimizer = tf.train.AdamOptimizer(learning_rate=<span class=\"number\">0.001</span>)</span><br><span class=\"line\">    train = optimizer.minimize(loss)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">        session.run(tf.global_variables_initializer())</span><br><span class=\"line\">        iteration = <span class=\"number\">1</span></span><br><span class=\"line\">        BATCH_SIZE = <span class=\"number\">100</span></span><br><span class=\"line\">        DATA_PATH = <span class=\"string\">'../data/cifar-10-batches-py/'</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, <span class=\"number\">41</span>):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, <span class=\"number\">6</span>):</span><br><span class=\"line\">                data = unpickle(DATA_PATH + <span class=\"string\">'data_batch_&#123;&#125;'</span>.format(i))</span><br><span class=\"line\">                image_data = np.reshape(data[<span class=\"string\">b'data'</span>], (<span class=\"number\">10000</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>), <span class=\"string\">'F'</span>).astype(np.float32)</span><br><span class=\"line\">                image_data = image_data / <span class=\"number\">255</span></span><br><span class=\"line\">                image_data = np.transpose(image_data, (<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">3</span>))</span><br><span class=\"line\">                label_data = data[<span class=\"string\">b'labels'</span>]</span><br><span class=\"line\">                new_label_data = np.zeros((<span class=\"number\">10000</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\">                <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">10000</span>):</span><br><span class=\"line\">                    new_label_data[j][label_data[j]] = <span class=\"number\">1</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(int(<span class=\"number\">10000</span> / BATCH_SIZE)):</span><br><span class=\"line\">                    image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class=\"line\">                    label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class=\"line\">                    [train_, loss_] = session.run(</span><br><span class=\"line\">                        [train, loss],</span><br><span class=\"line\">                        feed_dict=&#123;</span><br><span class=\"line\">                            input_tensor: image_batch,</span><br><span class=\"line\">                            ground_truth: label_batch</span><br><span class=\"line\">                        &#125;)</span><br><span class=\"line\">                    iteration += <span class=\"number\">1</span></span><br><span class=\"line\">                    print(<span class=\"string\">\"Epoch &#123;&#125;, Iteration &#123;&#125;, Loss = &#123;&#125;\"</span>.format(epoch, iteration, loss_))</span><br></pre></td></tr></table></figure><br><div align=\"center\" class=\"figure\">\n<img src=\"/images/tftutorial/train.png\" alt=\"Training result\" width=\"40%\"><br>Fig. 2 Training result: cross entropy has dropped below 0.5\n</div>\n<p></p>\n\n<h3 id=\"2-6-Conclusion\"><a href=\"#2-6-Conclusion\" class=\"headerlink\" title=\"2.6 Conclusion\"></a>2.6 Conclusion</h3><p lang=\"en-us\">\nIn a word, building &amp; training neural network models with TensorFlow involves the following steps:<br>1. Decide the <em>input tensor</em><br>2. Add operations(<em>op</em>s) based on existing tensors<br>3. Define the <em>loss</em> tensor, just like other tensors<br>4. Select an <em>optimizer</em> and define the <em>train</em> op<br>5. Process <em>data</em> and feed the model with them\n</p>\n<p lang=\"zh-cn\">\n总而言之，用TensorFlow建立、训练一个神经网络模型分以下几步：<br>1. 定义<em>输入</em>Tensor<br>2. 在已有的Tensor上添加运算（<em>op</em><br>\n3. 像之前添加的那些运算一样，定义<em>损失</em>Tensor<br>4. 选择一个<em>优化器</em>并定义<em>训练</em>操作<br>5. 把<em>数据</em>处理为合适的shape，并喂进模型训练\n</p>\n\n<h2 id=\"3-A-Closer-Look\"><a href=\"#3-A-Closer-Look\" class=\"headerlink\" title=\"3 A Closer Look\"></a>3 A Closer Look</h2><p lang=\"en-us\">\nWait, it&#39;s too late to leave now<br>TensorChain saved you from having to deal with a mess of TensorFlow classes and functions. Now it&#39;s time that we take a closer look at how TensorChain is implemented, thus understanding the native TensorFlow APIs.\n</p>\n<p lang=\"zh-cn\">\n别走呢喂！\nTensorChain让你不至于面对TensorFlow中乱糟糟的类型和函数而不知所措被水淹没。现在是时候近距离观察一下TensorChain是如何实现的，以便理解TensorFlowAPI了。\n</p>\n\n<h3 id=\"3-1-TensorFlow-variables\"><a href=\"#3-1-TensorFlow-variables\" class=\"headerlink\" title=\"3.1 TensorFlow variables\"></a>3.1 TensorFlow variables</h3><p lang=\"en-us\">\nLet&#39;s begin with TensorFlow variables. Variables in TensorFlow are similar to variables in C, Java or any other strong typed programming languages - they have a type, though not necessarily explicitly decided upon definition. Usually them will change as the training process goes on, getting close to a best value.\nThe most commonly used variables in TensorFlow are weights and biases. I guess that you have seen formulae like:\n</p>\n<p lang=\"zh-cn\">\n先说TensorFlow的变量。TensorFlow的变量和C，Java以及其他强类型语言类似——都有一个类型，尽管不一定在它的定义时就显式地声明。通常它们会随着训练的进行而不断变化，达到一个最佳的值附近。\nTensorFlow中最常用的变量就是weights和biases（权重和偏置）。想必你应该见过这样的式子吧：\n</p>\n$$y=Wx+b$$\n<p lang=\"en-us\">\nThe \\(W\\) here is the weight, and the \\(b\\) here is the bias. When implementing some common network layers, they two are always used as the parameters in the layers. For instance, at the very beginning of our ResNet-56, we had a 3x3 sized convolution layer with 16 channels. Its implementation in TensorChain is:\n</p>\n<p lang=\"zh-cn\">\n这里\\(W\\)就是权重，\\(b\\)就是偏置。在定义一些常用的层时，我们往往也是用这两个变量作为这些层中的参数。比如说，在我们ResNet-56最开始，我们用到了一个3x3大小、16个通道的卷积层，TensorChain中，它的实现如下：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">convolution_layer_2d</span><span class=\"params\">(self, filter_size: int, num_channels: int, stride: int = <span class=\"number\">1</span>, name: str = None,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                         disable_log: bool = False)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Add a 2D convolution layer</span></span><br><span class=\"line\"><span class=\"string\">    :param filter_size: Filter size(width and height) for this operation</span></span><br><span class=\"line\"><span class=\"string\">    :param num_channels: Channel number of this filter</span></span><br><span class=\"line\"><span class=\"string\">    :param stride: Stride for this convolution operation</span></span><br><span class=\"line\"><span class=\"string\">    :param name: The name of the tensor</span></span><br><span class=\"line\"><span class=\"string\">    :param disable_log: Set it True if you don't want this layer to be recorded</span></span><br><span class=\"line\"><span class=\"string\">    :return: This object itself</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    filter = self._weights([filter_size, filter_size, self.num_channels, num_channels], layer_name=name,</span><br><span class=\"line\">                           suffix=<span class=\"string\">'filter'</span>)</span><br><span class=\"line\">    bias = self._bias([num_channels], layer_name=name)</span><br><span class=\"line\">    self.num_channels = num_channels</span><br><span class=\"line\">    self.output_tensor = tf.nn.conv2d(self.output_tensor, filter,</span><br><span class=\"line\">                                      [<span class=\"number\">1</span>, stride, stride, <span class=\"number\">1</span>], <span class=\"string\">'SAME'</span>, name=name)</span><br><span class=\"line\">    self.output_tensor = tf.add(self.output_tensor, bias)</span><br><span class=\"line\">    self._log_layer(</span><br><span class=\"line\">        <span class=\"string\">'2D Convolution layer, filter size = &#123;&#125;x&#123;&#125;, stride = &#123;&#125;, &#123;&#125; channels'</span>.format(filter_size, filter_size,</span><br><span class=\"line\">                                                                                     stride,</span><br><span class=\"line\">                                                                                     num_channels),</span><br><span class=\"line\">        disable=disable_log)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> self</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nSee? On line 16, we used a <em>tf.nn.conv2d()</em> function, the parameters of which are <em>input</em>, <em>filter</em>, <em>strides</em>, <em>padding</em>, etc. As can be guessed from the names, this function does a convolution operation with out input and the weights(the convolution <em>filter</em> here). A <em>bias</em> is added to the result as the final output. There are also many people who argue that the bias here is meaningless and should removed. One line of code is sufficient for defining a variable:\n</p>\n<p lang=\"zh-cn\">\n看见了吧？16行上，我们用了一个<em>tf.nn.conv2d()</em>函数，它的参数是<em>input</em>，<em>filter</em>，<em>strides</em>，<em>padding</em>等等。顾名思义，这个函数就是用我们定义的权重Tensor<em>filter</em>（在这里称之为卷积核）来与这一层的输入input做了一次运算。运算的结果加上了偏置Tensor<em>bias</em>，作为这个卷积层的最终输出。很多人认为这里的偏置bias意义不明，因此他们在卷积之后没有加上这样的一个bias变量。定义一个变量只需要这样一个语句：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tf.Variable(tf.truncated_normal(shape, stddev=sigma), dtype=tf.float32, name=suffix)</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nTo define weight or bias variables, create a <em>tf.Variable</em> object. Usually you&#39;ll need to give the <em>initial_value</em> which also decides the shape of this tensor. <em>tf.truncated_normal()</em> and <em>tf.constant()</em> are usually used as the initial values. Also, other APIs - function <em>tf.get_variable()</em> and package <em>tf.initializers</em> are frequently used when using some more methods for initialization. I strongly recommend that you try using these APIs yourself.\n</p>\n<p lang=\"zh-cn\">\n要定义权重或者偏置变量，请创建一个<em>tf.Variable</em>对象。通常情况下，你会需要给出<em>initial_value</em>（TF变量的初始值），这将顺便定义了这个变量的shape（因为初始值的shape是确定的）。另外，一些新的API——<em>tf.get_variable()</em>函数和<em>tf.initializers</em>包也常用与的参数初始化，以实现更多样的初始化方法。我强烈建议自己动手实践一下，试一试这些API。\n</p>\n\n<h3 id=\"3-2-Tensors-and-operations\"><a href=\"#3-2-Tensors-and-operations\" class=\"headerlink\" title=\"3.2 Tensors and operations\"></a>3.2 Tensors and operations</h3><p lang=\"en-us\">\nGoing on with the parameters of the <em>tf.nn.conv2d()</em> function. The required parameters also include <em>strides</em> and <em>padding</em>. You should have already learned about what strides mean in convolution, and I&#39;ll only talk about their formats. <em>strides</em> require a 1-D vector with a length of 4, like [1, 2, 2, 1]. The 1st and the 4th number is always 1(in order to match dimensions with the input), while the 2nd and the 3rd means the vertical stride and the horizonal stride.<br>The 4th parameter <em>padding</em> is a little bit different from its definition in convolution operation. It requires &#39;SAME&#39; of &#39;VALID&#39;, denoting &#39;with&#39; or &#39;without&#39; zero paddings. When it&#39;s &#39;SAME&#39;, zero padding is introduced to make the shapes match as needed, equally on every side of the input map.\n</p>\n<p lang=\"zh-cn\">\n接着说<em>tf.nn.conv2d()</em>函数的参数。需要的参数还包括<em>strides</em>和<em>padding</em>。你应该在了解卷积运算时学过stride（步幅）含义，我只说一下它的格式吧。<em>strides</em>参数需要是一个1维、长度为4的向量。第一位和第四位永远都是1，第二位和第三位分别是竖直方向和水平方向的步幅长。维持这个形式只是为了与输入的数据维度匹配，因此API看起来非常蹩脚。\n第四个参数<em>padding</em>和卷积运算里的padding不太相同。它的值只能是&#39;SAME&#39;或&#39;VALID&#39;，分别代表“带”和“不带”零补全。如果是&#39;SAME&#39;的话，函数会均匀地在图像的上下左右使用零补全来使得运算结果与之前尽可能保持一致。（stride&gt;1时有可能输出尺寸不是正好等于原来的尺寸/stride，因为补全问题）\n</p>\n<p lang=\"en-us\">\ntf.nn.conv2d() is just an example of TensorFlow <em>operations</em>. Other functions like <em>tf.matmul()</em>, <em>tf.reduce_mean()</em>, <em>tf.global_variables_initializer()</em>, <em>tf.losses.softmax_cross_entropy()</em>, <em>tf.truncated_normal()</em> are all operations. Operation functions return tensors(<em>tf.truncated_normal</em> also return a tensor, a tensor with initializers).\n</p>\n<p lang=\"zh-cn\">\ntf.nn.conv2d()只是TensorFlow运算（<em>operation</em>）的一个例子。其他例如<em>tf.matmul()</em>，<em>tf.reduce_mean()</em>，<em>tf.nn.relu()</em>，<em>tf.batch_normalization()</em>，<em>tf.global_variables_initializer()</em>，<em>tf.losses.softmax_cross_entropy()</em>，<em>tf.truncated_normal()</em>之类的函数也都是TensorFlow的运算。TensorFlow的运算函数会返回一个Tensor对象（包括<em>tf.truncated_normal()</em>也是！它只不过返回的是一个带初始化器的Tensor而已）。\n</p>\n<p lang=\"en-us\">\nAll the functions in the TensorChain class are based on the most basic TensorFlow operations and variables. After learning about these basic TensorFlow concepts, actually you can already abandon TensorChain, go and try implementing your own neural networks yourself<br></p>\n<p lang=\"zh-cn\">\nTensorChain类中的所有成员函数都是基于最基本的TensorFlow运算和变量的。实际上，了解了这些，你现在已经可以抛开TensorChain的束缚，去尝试实现你自己的神经网络了！\n</p>\n\n<h2 id=\"4-Spices\"><a href=\"#4-Spices\" class=\"headerlink\" title=\"4 Spices\"></a>4 Spices</h2><p lang=\"en-us\">\nI&#39;m not joking just now! But I know that there are a lot of things that you still don&#39;t understand about using TensorFlow - like &quot;how do I visualize my computation graph&quot;, &quot;how do I save/load my model to/from files&quot;, &quot;how do I record some tensors&#39; values while training&quot; or &quot;how do I view the loss curves&quot; - after all TensorFlow APIs are far more complicated than just building those nets. Those are also important techniques in your research. If you&#39;d rather ask me than spending some time experimenting, please go on with reading.\n</p>\n<p lang=\"zh-cn\">\n我，我真没开玩笑！但是我知道关于如何使用TensorFlow，你还有许许多多的问题，好比“如何可视化地查看我的计算图结构”、“如何存储/读取模型文件”、“如何记录训练过程中某些Tensor的真实值”、“如何查看损失函数的变化曲线”——毕竟TensorFlow的API太复杂了，远比搭建神经网络那点函数复杂得多。上边说的那些是你使用TensorFlow研究过程中的重要技巧。如果你愿意听我讲而不想花些时间尝试的话，请继续读下去。\n</p>\n\n<h3 id=\"4-1-Saving-and-loading-your-model\"><a href=\"#4-1-Saving-and-loading-your-model\" class=\"headerlink\" title=\"4.1 Saving and loading your model\"></a>4.1 Saving and loading your model</h3><p lang=\"en-us\">\nThe very first thing that you may want to do - after training a network model with nice outcomes - would be saving it. Saving a model is fairly easy - just use a <em>tf.train.Saver</em> object. See my code below:\n</p>\n<p lang=\"zh-cn\">\n训练出一个看起来输出还不错的神经网络模型后你想做的第一件事恐怕就是把它存下来了吧？保存模型其实非常简单：只要用一个<em>tf.train.Saver</em>类的对象。代码示例：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    <span class=\"comment\"># Train it for some iterations</span></span><br><span class=\"line\">    <span class=\"comment\"># Train it for some iterations</span></span><br><span class=\"line\">    <span class=\"comment\"># Train it for some iterations</span></span><br><span class=\"line\">    saver = tf.train.Saver()</span><br><span class=\"line\">    saver.save(session, <span class=\"string\">'models/model.ckpt'</span>)</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nI saved my model and variable values to &#39;models/model.ckpt&#39;. But actually, you&#39;ll find 3 files in the &#39;models&#39; directory - <em>model.ckpt.data-00000-of-00001</em>, <em>model.ckpt.meta</em> and <em>model.ckpt.index</em> - none of which is &#39;model.ckpt&#39;! That&#39;s because TensorFlow stores the graph structure separately from variables values. The <em>.meta</em> file describes the saved graph structure; the <em>.index</em> file records the mappings between tensor names and tensor metadata; and the <em>.data-00000-of-00001</em> file - which is always the biggest one - saves all the variable values. If you need the graph data together with the variable values to be loaded, use a Saver to load after creating a session:\n</p>\n<p lang=\"zh-cn\">\n我把我的模型和变量值存到了&#39;models/model.ckpt&#39;文件里。但是！实际上在models目录里你会找到三个文件：<em>model.ckpt.data-00000-of-00001</em>，<em>model.ckpt.meta</em>和<em>model.ckpt.index</em>——哪个也不是model.ckpt呀？那是因为TensorFlow把计算图的结构和图中各种变量的值分开存放了。<em>.meta</em>文件描述计算图的结构；<em>.index</em>文件记录各个Tensor名称（是name属性，而不是变量名）与Tensor元信息之间的映射；<em>.data-00000-of-00001</em>文件往往是最大的一个，它存储的是各个TensorFlow变量的实际值。如果读取时需要把图结构和变量值都读进来，在session创建以后，同样用一个Saver来读取即可：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    saver = tf.train.Saver()</span><br><span class=\"line\">    saver.restore(session, <span class=\"string\">'models/model.ckpt'</span>)</span><br><span class=\"line\">    <span class=\"comment\"># Then continue doing everything just like the model is just trained</span></span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nRemember that session.run(tf.global_variables_initializer()) shouldn&#39;t be executed, since variables are already initialized with your saved <em>.data-0000-of-00001</em> file.\nIf you only need the graph to be loaded, only use the <em>.meta</em> file:\n</p>\n<p lang=\"zh-cn\">\n记住，这时候就不要再去执行session.run(tf.global_variables_initializer())了，因为变量已经用存储的checkpoint文件内容初始化过了。\n如果只需要读取计算图结构，只要读取<em>.meta</em>文件：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    tf.train.import_meta_graph(<span class=\"string\">'models/model.ckpt.meta'</span>)</span><br><span class=\"line\">    <span class=\"comment\"># Then continue doing everything just like the model is just built</span></span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nFunction <em>tf.train.import_meta_graph()</em> loads(appends) the graph to your current computation graph. The values of tensors are still uninitialized so you&#39;ll have to execute session.run(tf.global_variables_initializer()) again. The tensors that you defined in the model can be retrieved by their names(property of the Tensor objects, instead of Python variable names). For example:\n</p>\n<p lang=\"zh-cn\">\n<em>tf.train.import_meta_graph()</em>函数将文件里的计算图读到（添加到）你当前的计算图中。其中所有Tensor的值仍未初始化，所以有必要执行一下session.run(tf.global_variables_initializer())了。之前定义的变量可以按照名称取回，示例：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    <span class=\"comment\"># Recover the model here</span></span><br><span class=\"line\"></span><br><span class=\"line\">    graph = tf.get_default_graph()</span><br><span class=\"line\">    image_tensor = graph.get_tensor_by_name(<span class=\"string\">'input_image:0'</span>)</span><br><span class=\"line\">    loss = graph.get_tensor_by_name(<span class=\"string\">'loss:0'</span>)</span><br><span class=\"line\">    train = graph.get_operation_by_name(<span class=\"string\">'train)</span></span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nTo retrieve normal tensors, you&#39;ll have to append a <em>&#39;:0&#39;</em> to the name of the op. This means getting the associated tensor of the op. <em>train</em> is a little special - we only need the op, so the function is <em>get_operation_by_name()</em> so the &#39;:0&#39; is not necessary.\n</p>\n<p lang=\"zh-cn\">\n要取回一般的Tensor，需要在Tensor的name属性值后边加一个<em>&#39;:0&#39;</em>，意思是取这个运算对应的Tensor。训练操作<em>train</em>略有不同——我们要的就只是这个op，所以用的函数<em>get_operation_by_name()</em>跟其他Tensor不一样，而且&#39;:0&#39;也不需要加。\n</p>\n\n<p lang=\"en-us\" align=\"center\">\n[THIS SECTION IS UNDER CONSTRUCTION]\n</p>\n<p lang=\"zh-cn\" align=\"center\">\n[本部分内容施工中]\n</p>","site":{"data":{}},"excerpt":"<p><script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML\" async></script>\n<br></p>\n<p lang=\"en-us\">\nTensorFlow is a powerful open-source deep learning framework, supporting various languages including Python. However, its APIs are far too complicated for a beginner in deep learning(especially those who are new to Python). In order to ease the pain of having to understand the mess of various elements in TensorFlow computation graphs, I made this tutorial to help beginners take the first bite of the cake.<br>ResNets are one of the greatest works in the deep learning field. Although they look scary with extreme depths, it&#39;s not a hard job to implement one. Now let&#39;s build one of the simplest ResNets - ResNet-56, and train it on the CIFAR-10 dataset.<br></p>","more":"<p lang=\"zh-cn\">\nTensorFlow是一个强大的开源深度学习软件库，它支持包括Python在内的多种语言。然而，由于API过于复杂（实际上还有点混乱），它往往使得一个深度学习的初学者（尤其是为此初学Python的那些）望而却步——老虎吃天，无从下口。为了减轻初学者不得不尝试理解TensorFlow中的大量概念的痛苦，我213今天带各位尝尝深度学习这片天的第一口。\nResNet是深度学习领域的一个重磅炸弹，尽管它们（ResNet有不同层数的多个模型）的深度看上去有点吓人，但实际上实现一个ResNet并不难。接下来，我们来实现一个较为简单的ResNet——ResNet-56，并在CIFAR-10数据集上训练一下，看看效果如何。\n</p>\n<div align=\"center\" class=\"figure\">\n<img src=\"/images/tftutorial/oyo.gif\" alt=\"Let&#39;s Rock!\">\n</div>\n<p lang=\"en-us\">\nFirst let&#39;s take a look at ResNet-56. It&#39;s proposed by Kaiming He et al., and is designed to confirm the effect of residual networks. It has 56 weighted layers, deep but simple. The structure is shown in the figure below:\n</p>\n<p lang=\"zh-cn\">\n首先来看一下ResNet-56这个神经网络。它是何凯明等在ResNet论文中提出的、用于验证残差网络效果的一个相对简单的残差网络（尽管它很深，深度达到了56个权重层）。图示如下：\n</p>\n<div align=\"center\" class=\"figure\">\n<img src=\"/images/tftutorial/resnet56.png\" alt=\"ResNet-56\" width=\"80%\"><br>Fig. 1 The structure of ResNet-56\n</div>\n<br>\n<p lang=\"en-us\">\nSeems a little bit long? Don&#39;t worry, let&#39;s do this step by step.\n</p>\n<p lang=\"zh-cn\">\n看起来有点长了是不是？别担心，我们一步一步来做。\n</p>\n\n<h2 id=\"1-Ingredients\"><a href=\"#1-Ingredients\" class=\"headerlink\" title=\"1 Ingredients\"></a>1 Ingredients</h2><p>Python 3.6</p>\n<p>TensorFlow 1.4.0</p>\n<p>Numpy 1.13.3</p>\n<p>OpenCV 3.2.0</p>\n<p><a href=\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\" target=\"_blank\" rel=\"noopener\">CIFAR-10 Dataset</a></p>\n<p lang=\"en-us\">\nAlso prepare some basic knowledge on Python programming, digital image processing and convolutional neural networks. If you are already capable of building, training and validating your own neural networks with TensorFlow, you don&#39;t have to read this post.\n</p>\n<p lang=\"zh-cn\">\n另外，请确保自己有一点点Python编程、数字图像处理和卷积神经网络的知识储备。如果你已经具备用TensorFlow自行搭建神经网络并进行训练、测试的能力，就不必阅读本文了。\n</p>\n\n<h2 id=\"2-Recipe\"><a href=\"#2-Recipe\" class=\"headerlink\" title=\"2 Recipe\"></a>2 Recipe</h2><h3 id=\"2-0-Prepare-the-tools\"><a href=\"#2-0-Prepare-the-tools\" class=\"headerlink\" title=\"2.0 Prepare the tools\"></a>2.0 Prepare the tools</h3><p lang=\"en-us\">\nPrepare(import) the tools for our project, including all that I mentioned above. Like this :P\n</p>\n<p lang=\"zh-cn\">\n准(i)备(m)所(p)需(o)工(r)具(t)，上一部分已提到过。如下：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> cv2</span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensor_chain <span class=\"keyword\">import</span> TensorChain</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nWait... What&#39;s this? TensorChain? Another deep learning framework like TensorFlow?<br>Uh, nope. This is my own encapsulation of some TensorFlow APIs, for the sake of easing your pain. You&#39;ll only have to focus on &quot;what&#39;s what&quot; in the beginning. We&#39;ll look into my implementation of this encapsulation later, when you are clear how everything goes. Please <a href=\"/files/tensor_chain.py\">download this file</a> and put it where your code file is, and import it.\n</p>\n<p lang=\"zh-cn\">\n等等...最后这个是个什么鬼？ TensorChain？另一个深度学习框架吗？<br>呃...并不是。这个是我对一些TensorFlow API的封装，为了减轻你的痛苦才做的。作为初学者，你只需要关注用TensorFlow搭建网络模型的这个过程，分清东西南北。回头等你弄清了大体流程后，我们再来看这个的实现细节。请先下载<a href=\"/files/tensor_chain.py\">这个文件</a>并把它与你的代码放在同一文件夹下，然后就可以import了。\n</p>\n\n<h3 id=\"2-1-Decide-the-input\"><a href=\"#2-1-Decide-the-input\" class=\"headerlink\" title=\"2.1 Decide the input\"></a>2.1 Decide the input</h3><p lang=\"en-us\">\nEvery neural network requires an input - you always have to identify the details of a question, before asking the computer to solve it. All of the variable, constant in TensorFlow are objects of type <em>tf.Tensors</em>. And the <em>tf.placeholder</em> of our input(s) is a special one. Images in CIFAR-10 dataset are RGB images(3 channels) of 32x32(really small), so our input should shaped like [32, 32, 3]. Also, we want to input a little <em>batch</em> of multiple images. Therefore, our input data should be an array of shape <em>[?, 32, 32, 3]</em>. Unknown dimension size can be marked as None, and it will be clear when we feed the model with the actual images. It&#39;s coded like this:\n</p>\n<p lang=\"zh-cn\">\n每个神经网络都需要有输入——毕竟你想找电脑解决一些问题的话，你总得告诉它问题的一些细节吧？TensorFlow中所有的变量、常量都是<em>tf.Tensor</em>类型的对象，作为输入内容的占位符<em>tf.placeholder</em>也是（只不过比较特殊而已）。CIFAR-10数据集的图像都是32x32尺寸（好小哇）的RGB图像（RGB即彩色图像的三个通道），因此我们的输入给神经网络的内容将会像是[32, 32, 3]这个样子。另外呢，我们需要输入的是一个小<em>batch</em>（批）的图像，因此，输入网络的图像数据将会是一个<em>[?, 32, 32, 3]</em>的数组（也可以是numpy数组）。未知的维度大小用None代指就好，我们之后给模型喂实际图像batch时，它自然就清楚了。代码如下：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input_tensor = tf.placeholder(dtype=tf.float32, shape=[<span class=\"keyword\">None</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>])</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\n<em>Ground truth</em> data also need to be known in supervised learning, so we also have to define a placeholder for the ground truth data:<br></p>\n<p lang=\"zh-cn\">\n监督学习中，正确标注的数据（英文为<em>ground truth</em>，目前貌似没有对这个名词的合理翻译）也是需要输入到模型中的。因此再给ground truth定义一个placeholder：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ground_truth = tf.placeholder(dtype=tf.float32, shape=[<span class=\"keyword\">None</span>, <span class=\"number\">10</span>])</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nWe want the label data to be in the one-hot encoding format, which means an array of length 10, denoting 10 classes. Only on one position is a &#39;1&#39;, and on other positions are &#39;0&#39;s.\n</p>\n<p lang=\"zh-cn\">\n我们需要标记的数据呈One-Hot编码格式（又称为一位有效编码），意思是如果有10个类别，那么数组长度就是10，每一位代表一个类别。只有一个位置上是1（代表图片被分为这个类），其他位上都是0。\n</p>\n\n<h3 id=\"2-2-Do-some-operations\"><a href=\"#2-2-Do-some-operations\" class=\"headerlink\" title=\"2.2 Do some operations\"></a>2.2 Do some operations</h3><p lang=\"en-us\">\nFor now, let&#39;s use our TensorChain to build it fast. Under most circumstances that we may face, the computations are based on the input data or the result of the former computation, so our network(or say, the most of it) look more like a chain than a web. Every time we add some new operation(layer), we add it to our <em>TensorChain</em> object. Just remember to get the <em>output_tensor</em> of this object(denoting the output tensor of the last operation on the chain) when you need to ue native TensorFlow API.\nThe construction function of TensorChain class requires a Tensor object as the parameter, which is also the input tensor of this chain. As we mentioned earlier, all we have to do is add operations. See my ResNet-56 code:\n</p>\n<p lang=\"zh-cn\">\n现在呢，我们先用TensorChain来快速盖楼。因为我们遇到的大多数情况下，所有的计算都是在输入数据或者这个计算的前一个计算结果基础上进行的，所以我们的网络（至少是它的绝大部分）会看起来像个链而不是所谓的网。每次我们添加一个新的运算（层），我们会把它加到这个独一无二的TensorChain对象。只要记得在使用原生TensorFlow API前把它的<em>output_tensor</em>属性（也就是这条链上最后一个运算的输出Tensor）取出来就好了。\nTensorChain类的构造函数需要一个Tensor对象作为参数，这个对象也正是被拿来作为这个链的输入层。正如我们之前所说的，只要在这个对象上添加运算即可。写个ResNet-56，代码很简单：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chain = TensorChain(input_tensor) \\</span><br><span class=\"line\">        .convolution_layer_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>, stride=<span class=\"number\">2</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>, stride=<span class=\"number\">2</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .flatten() \\</span><br><span class=\"line\">        .fully_connected_layer(<span class=\"number\">10</span>)</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nThis is it? Right, this is it! Isn&#39;t it cool? Didn&#39;t seem that high, huh? That&#39;s because I encapsulated that huge mess of weights and biases, only leaving a few parameters that decide the structure of the network. Later in this pose we&#39;ll talk about the actual work that these functions do.\n</p>\n<p lang=\"zh-cn\">\n就这？没错呀，就这！稳不稳？似乎看起来也没56层那么高呀？毕竟这些函数被我封装得太严实了，只留出几个决定网络结构的几个参数供修改。这篇博客后边就会讲到这些函数究竟干了点什么事儿。\n</p>\n\n<h3 id=\"2-3-Define-the-loss\"><a href=\"#2-3-Define-the-loss\" class=\"headerlink\" title=\"2.3 Define the loss\"></a>2.3 Define the loss</h3><p lang=\"en-us\">\nIn supervised learning, you always have to tell the learning target to the model. To tell the model how to optimize, you have to let it know how, how much, on which direction should it change its parameters. This is done by using a loss function. Therefore, we need to define a loss function for our ResNet-56 model(which we designed for this classification problem) so that it will learn and optimize.\nA commonly used loss function in classification problems is cross entropy. It&#39;s defined below:\n</p>\n<p lang=\"zh-cn\">\n搞监督学习，总是要让模型按照“参考答案”去改的。要改就得让它知道怎么改、改多少、往什么方向改，这也就是<em>loss</em>（损失函数）的功劳。因此，像我们这个拿来做分类问题的ResNet-56，我们要给它定义一个损失函数来让它学习、优化。\n分类问题上一个常用的损失函数是交叉熵。定义如下式：\n</p>\n$$C=-\\frac{1}{n}\\sum_x{y\\ln a+(1-y)\\ln(1-a)}$$\n<p lang=\"en-us\">\nin which \\(y\\) is the expected(or say correct) output and \\(a\\) is the actual output.\nThis seems a little bit complicated. But it&#39;s not a hard job to implement, since TensorFlow implemented it already! You can also try and implement it yourself within one line if you want. For now we use the pre-defined cross entropy loss function:\n</p>\n<p lang=\"zh-cn\">\n其中\\(y\\)为期望输出（或者说参考答案），\\(a\\)为实际输出。\n略复杂呀...这个用程序怎么写？其实也不难。。。毕竟TensorFlow都帮我们实现好啦！（有兴趣的话也可以自己尝试着写一下，同样一行代码即可搞定）现在你只需要来这么一句：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nand it returns a tf.Tensor that denotes an average of cross entropies(don&#39;t forget that this is a batch). As for the &#39;softmax&#39; before the &#39;cross_entropy&#39;, it&#39;s a function that project the data in an array to range 0~1, which allows us to do a comparison between our prediction and the ground truth(in one-hot code). The definition is simple too:<br></p>\n<p lang=\"zh-cn\">\n就可以创建一个表示交叉熵平均值（别忘了这可是一个batch）的Tensor了。至于cross_entropy前边的那个<em>softmax</em>呢，它的作用是把输入的数组内数据归一化，投射到0~1的范围内（实际上就是相当于把exp(数组各项的值)的当做频数，求出一个概率），这样子才能跟实际数据做一个比较。定义也比较简单：<br></p>\n$$S_i=\\frac{e^{V_i}}{\\sum_j{e^{V_j}}}$$\n<p></p>\n\n<h3 id=\"2-4-Define-the-train-op\"><a href=\"#2-4-Define-the-train-op\" class=\"headerlink\" title=\"2.4 Define the train op\"></a>2.4 Define the train op</h3><p lang=\"en-us\">\nNow we have the loss function. We&#39;ll have to tell its value to an <em>optimizer</em>, which make our model learn and optimize in order to minimize the loss value. Gradient Descent Optimizer, Adagrad Optimizer, Adam Optimizer and Momentum Optimizer are commonly used optimizers. Here we use an Adam Optimizer for instance. You&#39;re free to try any other one here. When\n</p>\n<p lang=\"zh-cn\">\n现在误差函数已经有了，我们需要把它的值告诉一个优化器（<em>optimizer</em>），并让它去尽可能向着缩小误差函数值得方向努力。这样，模型才能去学习、优化。常用的优化器包括Gradient Descent Optimizer，Adagrad Optimizer，Adam Optimizer以及Momentum Optimizer等等等等。选择优化器时，我们需要给它一个初始的学习速率。这里我用了一个\\(10^-3\\)，如果需要提高准确率，可能后期微调还需要进一步减小。代码如下：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">optimizer = tf.train.AdamOptimizer(learning_rate=<span class=\"number\">0.001</span>)</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nAlso, tell the optimizer that what the loss tensor is. The returned object is a train operation.\n</p>\n<p lang=\"zh-cn\">\n当然还要告诉它要减小的损失函数是哪个Tensor，这个函数返回的是一个训练操作（<em>train op</em>，一种特殊的运算，或者说操作）：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train = optimizer.minimize(loss)</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nThe neural network is finished. It&#39;s time to grab some data and train it.\n</p>\n<p lang=\"zh-cn\">\n其实到这里为止，神经网络已经搭建好了。是时候搞点数据来训练它了。\n</p>\n\n<h3 id=\"2-5-Feed-the-model-with-data-and-train-it\"><a href=\"#2-5-Feed-the-model-with-data-and-train-it\" class=\"headerlink\" title=\"2.5 Feed the model with data, and train it!\"></a>2.5 Feed the model with data, and train it!</h3><p lang=\"en-us\">\nRemember how we defined the placeholders? It&#39;s time to fetch some data that fits the placeholders and train it. See how CIFAR-10 dataset can be fetched on its <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\" target=\"_blank\" rel=\"noopener\">website</a><br></p>\n<p lang=\"zh-cn\">\n还记得我们怎么定义那些placeholder吗？现在我们要把符合它们口径的数据灌进模型。那么来看一下CIFAR-10数据集<a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\" target=\"_blank\" rel=\"noopener\">官网</a>上是怎么描述的吧。它给了这么一段代码：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">unpickle</span><span class=\"params\">(file)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">import</span> pickle</span><br><span class=\"line\">    <span class=\"keyword\">with</span> open(file, <span class=\"string\">'rb'</span>) <span class=\"keyword\">as</span> fo:</span><br><span class=\"line\">        dict = pickle.load(fo, encoding=<span class=\"string\">'bytes'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dict</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nThe returned value <em>dict</em> is a Python dictionary. Every time we unpickle a file, a dictionary would be returned. Its &#39;data&#39; key leads to 10000 RGB images of size 32x32, which is stored in a [10000, 3072] array(3072=32<em>32</em>3, I guess you know how it&#39;s stored now). The &#39;label&#39; key leads to 10000 values in range 0~9. Obviously we have to reshape the data so as to fit it into the network model:\n</p>\n<p lang=\"zh-cn\">\n返回值<em>dict</em>是一个字典（Python的dict类型）。每读一个batch文件（比如data_batch_1），就会返回这样一个字典，它的“data”键值是10000张32x32的RGB图像（数组维数居然是[10000, 3072]，而3072=10000x32x32x3！实际上就是直接把所有像素、所有通道的值罗列在这里了）；“label”键值是10000个0-9之间的整数（代表类别）。显然，为了让数据能够成功放进模型，还需要对它进行一点处理：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch = unpickle(DATA_PATH + <span class=\"string\">'data_batch_&#123;&#125;'</span>.format(i))  <span class=\"comment\"># 'i' is the loop variable</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Read the image data</span></span><br><span class=\"line\">image_data = np.reshape(batch[<span class=\"string\">b'data'</span>], (<span class=\"number\">10000</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>), <span class=\"string\">'F'</span>).astype(np.float32)   </span><br><span class=\"line\">image_data = image_data / <span class=\"number\">255</span>                            <span class=\"comment\"># Cast range(0, 255) to range(0, 1)</span></span><br><span class=\"line\">image_data = np.transpose(image_data, (<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">3</span>))      <span class=\"comment\"># Exchange row and column</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Read the label data and convert into one-hot code</span></span><br><span class=\"line\">label_data = batch[<span class=\"string\">b'labels'</span>]</span><br><span class=\"line\">new_label_data = np.zeros((<span class=\"number\">10000</span>, <span class=\"number\">10</span>))                   </span><br><span class=\"line\"><span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">10000</span>):</span><br><span class=\"line\">    new_label_data[j][label_data[j]] = <span class=\"number\">1</span></span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nThe details for data processing are not covered here. Try doing step-by-step to see the results.\nThe <em>image_data</em> and <em>new_label_data</em> are contain 10000 pieces of data each. Let&#39;s divide them into 100 small batches(100 elements each, including image and label) and feed it into the model. Do this on all the 5 batch files:\n</p>\n<p lang=\"zh-cn\">\n处理的细节不再赘述。你可以尝试一步一步运行来看看每一步的结果。\n这样我们拿到的<em>image_data</em>和<em>new_label_data</em>都是长度为10000的大batch，我们把它们各自分成100份，每次取100个图像+标记数据来塞进模型。对全部5个大batch文件来一遍：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    session.run(tf.global_variables_initializer())</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">100</span>): <span class=\"comment\"># 10000 / BATCH_SIZE</span></span><br><span class=\"line\">        <span class=\"comment\"># Divide them and get one part</span></span><br><span class=\"line\">        image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class=\"line\">        label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># Feed the model</span></span><br><span class=\"line\">        session.run(train, feed_dict=&#123;</span><br><span class=\"line\">            input_tensor: image_batch,</span><br><span class=\"line\">            ground_truth: label_batch</span><br><span class=\"line\">        &#125;)</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nA <em>session</em> - created with <em>tf.Session()</em> - is required every time we run a TensorFlow model, no matter when we&#39;re training it or evaluating it. The first time you run a model, you&#39;ll need to run <em>session.run(tf.global_variables_initializer())</em> to initialize the values of the TensorFlow variables defined previously.\nWhen running <em>session.run()</em>, you must first decide a TensorFlow operation(or a list of operations) that you need. If its result is dependent on some actual data(which means that some data in one or more placeholders flow to this operation), it&#39;s also required that you feed it the actual data by adding a <em>feed_dict</em> parameter. For example, I&#39;m training this ResNet-56 model, in which a loss will be calculated with my <em>ground_truth</em> and the prediction result that comes from the <em>input_tensor</em>. Therefore, I&#39;ll have to give a value for each placeholder given above(format: &quot;placeholder name: corresponding data&quot;), and fold them in one Python dictionary.\n</p>\n<p lang=\"zh-cn\">\n每次运行一个TensorFlow模型（无论是训练还是测试）时，都需要通过tf.Session()创建一个<em>session</em>。第一次运行模型（而不是载入之前保存的模型）时，需要使用<em>session.run(tf.global_variables_initializer())</em>来初始化之前定义的一些可训练的TensorFlow变量。\n运行<em>session.run()</em>时，需要指定一个或一组你要执行的operation，作为这个函数唯一一个必要的参数。如果它的结果依赖于一些实际数据（也就是说在计算图中，一些数据会从placeholder流向这个operation），那么就需要通过填入<em>feed_dict</em>参数的值来填装训练或测试数据。以此模型为例，我在训练它时需要算误差函数值，这需要<em>ground_truth</em>数据和预测结果来计算，而预测结果又需要用输入图像<em>input_tensor</em>来计算得到。因此，我需要给这两个占位符分别给出对应的数据（格式：“占位符名：对应数据”），并把它们封在同一个Python字典中作为feed_dict参数的值。\n</p>\n<p lang=\"en-us\">\nI&#39;m also interested in the loss function value in each iteration(which means feeding a batch of data and executing one forward-propagation and one back-propagation) in the training process. Therefore, what I&#39;ll fill in the parameter is not just the train op, but also the loss tensor. And the session.run() above should be modified to:\n</p>\n<p lang=\"zh-cn\">\n然而呢，我还想看看每次迭代（即把一个batch送进去，执行一次正向传播与反向传播这个过程）中损失函数变成了多大，来监控一下训练的效果。这样，需要session.run()的就不仅是那个train运算，还要加上loss运算。将上边的session.run()部分改为：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[train_, loss_value] = session.run([train, loss],</span><br><span class=\"line\">    feed_dict=&#123;</span><br><span class=\"line\">        input_tensor: image_batch,</span><br><span class=\"line\">        ground_truth: label_batch</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">print(<span class=\"string\">\"Loss = &#123;&#125;\"</span>.format(loss_value)</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nThis is when the return value of session.run() becomes useful. Its value(s) - corresponding to the first parameter of run() - is/are the actual value(s) of the tensor(s) in the first parameter. In our example, <em>loss_value</em> is the actual output of the loss tensor. As for train_, we don&#39;t care what it is. Just add it to match the dimensions.\n</p>\n<p lang=\"zh-cn\">\n这时候，session.run()函数的返回值就有意义了。它与第一个参数的内容一一对应，分别是该参数中各个operation的实际输出值。像这个例子里边，<em>loss_value</em>接收的就是loss运算的输出内容。而train运算的输出我们并不关心，但是为了保证参数维度数与返回值一致，用一个train_变量来接收而已（实际上它的值是None）。\n</p>\n<p lang=\"en-us\">\nActually, one epoch(train the model once with the whole dataset) is not enough for the model to fully optimize. I trained this model for 40 epochs and added some loop variables to display the result. You can see my code and my output below. It&#39;s highly recommended that you train this with a high-performance GPU, or it would be a century before you train your model to a satisfactory degree.\n</p>\n<p lang=\"zh-cn\">\n实际上，一个epoch（把整个数据集都在模型里过一遍的周期）并不足以让模型充分学习。我把这个模型训练了40个epoch并且加了一些循环变量来输出结果。我的代码和结果如下。强烈建议用一个高性能GPU训练（如果手头没有，可以租一个GPU服务器），不然等别人把毕设论文逗写完的时候，你还在训练就很尴尬了。\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensor_chain <span class=\"keyword\">import</span> TensorChain</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">unpickle</span><span class=\"params\">(file)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> open(file, <span class=\"string\">'rb'</span>) <span class=\"keyword\">as</span> fo:</span><br><span class=\"line\">        dict = pickle.load(fo, encoding=<span class=\"string\">'bytes'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dict</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    input_tensor = tf.placeholder(dtype=tf.float32, shape=[<span class=\"keyword\">None</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>])</span><br><span class=\"line\">    ground_truth = tf.placeholder(dtype=tf.float32, shape=[<span class=\"keyword\">None</span>, <span class=\"number\">10</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    chain = TensorChain(input_tensor) \\</span><br><span class=\"line\">            .convolution_layer_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>, stride=<span class=\"number\">2</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>, stride=<span class=\"number\">2</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .flatten() \\</span><br><span class=\"line\">            .fully_connected_layer(<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    prediction = chain.output_tensor</span><br><span class=\"line\">    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))</span><br><span class=\"line\"></span><br><span class=\"line\">    optimizer = tf.train.AdamOptimizer(learning_rate=<span class=\"number\">0.001</span>)</span><br><span class=\"line\">    train = optimizer.minimize(loss)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">        session.run(tf.global_variables_initializer())</span><br><span class=\"line\">        iteration = <span class=\"number\">1</span></span><br><span class=\"line\">        BATCH_SIZE = <span class=\"number\">100</span></span><br><span class=\"line\">        DATA_PATH = <span class=\"string\">'../data/cifar-10-batches-py/'</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, <span class=\"number\">41</span>):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, <span class=\"number\">6</span>):</span><br><span class=\"line\">                data = unpickle(DATA_PATH + <span class=\"string\">'data_batch_&#123;&#125;'</span>.format(i))</span><br><span class=\"line\">                image_data = np.reshape(data[<span class=\"string\">b'data'</span>], (<span class=\"number\">10000</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>), <span class=\"string\">'F'</span>).astype(np.float32)</span><br><span class=\"line\">                image_data = image_data / <span class=\"number\">255</span></span><br><span class=\"line\">                image_data = np.transpose(image_data, (<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">3</span>))</span><br><span class=\"line\">                label_data = data[<span class=\"string\">b'labels'</span>]</span><br><span class=\"line\">                new_label_data = np.zeros((<span class=\"number\">10000</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\">                <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">10000</span>):</span><br><span class=\"line\">                    new_label_data[j][label_data[j]] = <span class=\"number\">1</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(int(<span class=\"number\">10000</span> / BATCH_SIZE)):</span><br><span class=\"line\">                    image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class=\"line\">                    label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class=\"line\">                    [train_, loss_] = session.run(</span><br><span class=\"line\">                        [train, loss],</span><br><span class=\"line\">                        feed_dict=&#123;</span><br><span class=\"line\">                            input_tensor: image_batch,</span><br><span class=\"line\">                            ground_truth: label_batch</span><br><span class=\"line\">                        &#125;)</span><br><span class=\"line\">                    iteration += <span class=\"number\">1</span></span><br><span class=\"line\">                    print(<span class=\"string\">\"Epoch &#123;&#125;, Iteration &#123;&#125;, Loss = &#123;&#125;\"</span>.format(epoch, iteration, loss_))</span><br></pre></td></tr></table></figure><br><div align=\"center\" class=\"figure\">\n<img src=\"/images/tftutorial/train.png\" alt=\"Training result\" width=\"40%\"><br>Fig. 2 Training result: cross entropy has dropped below 0.5\n</div>\n<p></p>\n\n<h3 id=\"2-6-Conclusion\"><a href=\"#2-6-Conclusion\" class=\"headerlink\" title=\"2.6 Conclusion\"></a>2.6 Conclusion</h3><p lang=\"en-us\">\nIn a word, building &amp; training neural network models with TensorFlow involves the following steps:<br>1. Decide the <em>input tensor</em><br>2. Add operations(<em>op</em>s) based on existing tensors<br>3. Define the <em>loss</em> tensor, just like other tensors<br>4. Select an <em>optimizer</em> and define the <em>train</em> op<br>5. Process <em>data</em> and feed the model with them\n</p>\n<p lang=\"zh-cn\">\n总而言之，用TensorFlow建立、训练一个神经网络模型分以下几步：<br>1. 定义<em>输入</em>Tensor<br>2. 在已有的Tensor上添加运算（<em>op</em><br>\n3. 像之前添加的那些运算一样，定义<em>损失</em>Tensor<br>4. 选择一个<em>优化器</em>并定义<em>训练</em>操作<br>5. 把<em>数据</em>处理为合适的shape，并喂进模型训练\n</p>\n\n<h2 id=\"3-A-Closer-Look\"><a href=\"#3-A-Closer-Look\" class=\"headerlink\" title=\"3 A Closer Look\"></a>3 A Closer Look</h2><p lang=\"en-us\">\nWait, it&#39;s too late to leave now<br>TensorChain saved you from having to deal with a mess of TensorFlow classes and functions. Now it&#39;s time that we take a closer look at how TensorChain is implemented, thus understanding the native TensorFlow APIs.\n</p>\n<p lang=\"zh-cn\">\n别走呢喂！\nTensorChain让你不至于面对TensorFlow中乱糟糟的类型和函数而不知所措被水淹没。现在是时候近距离观察一下TensorChain是如何实现的，以便理解TensorFlowAPI了。\n</p>\n\n<h3 id=\"3-1-TensorFlow-variables\"><a href=\"#3-1-TensorFlow-variables\" class=\"headerlink\" title=\"3.1 TensorFlow variables\"></a>3.1 TensorFlow variables</h3><p lang=\"en-us\">\nLet&#39;s begin with TensorFlow variables. Variables in TensorFlow are similar to variables in C, Java or any other strong typed programming languages - they have a type, though not necessarily explicitly decided upon definition. Usually them will change as the training process goes on, getting close to a best value.\nThe most commonly used variables in TensorFlow are weights and biases. I guess that you have seen formulae like:\n</p>\n<p lang=\"zh-cn\">\n先说TensorFlow的变量。TensorFlow的变量和C，Java以及其他强类型语言类似——都有一个类型，尽管不一定在它的定义时就显式地声明。通常它们会随着训练的进行而不断变化，达到一个最佳的值附近。\nTensorFlow中最常用的变量就是weights和biases（权重和偏置）。想必你应该见过这样的式子吧：\n</p>\n$$y=Wx+b$$\n<p lang=\"en-us\">\nThe \\(W\\) here is the weight, and the \\(b\\) here is the bias. When implementing some common network layers, they two are always used as the parameters in the layers. For instance, at the very beginning of our ResNet-56, we had a 3x3 sized convolution layer with 16 channels. Its implementation in TensorChain is:\n</p>\n<p lang=\"zh-cn\">\n这里\\(W\\)就是权重，\\(b\\)就是偏置。在定义一些常用的层时，我们往往也是用这两个变量作为这些层中的参数。比如说，在我们ResNet-56最开始，我们用到了一个3x3大小、16个通道的卷积层，TensorChain中，它的实现如下：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">convolution_layer_2d</span><span class=\"params\">(self, filter_size: int, num_channels: int, stride: int = <span class=\"number\">1</span>, name: str = None,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                         disable_log: bool = False)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Add a 2D convolution layer</span></span><br><span class=\"line\"><span class=\"string\">    :param filter_size: Filter size(width and height) for this operation</span></span><br><span class=\"line\"><span class=\"string\">    :param num_channels: Channel number of this filter</span></span><br><span class=\"line\"><span class=\"string\">    :param stride: Stride for this convolution operation</span></span><br><span class=\"line\"><span class=\"string\">    :param name: The name of the tensor</span></span><br><span class=\"line\"><span class=\"string\">    :param disable_log: Set it True if you don't want this layer to be recorded</span></span><br><span class=\"line\"><span class=\"string\">    :return: This object itself</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    filter = self._weights([filter_size, filter_size, self.num_channels, num_channels], layer_name=name,</span><br><span class=\"line\">                           suffix=<span class=\"string\">'filter'</span>)</span><br><span class=\"line\">    bias = self._bias([num_channels], layer_name=name)</span><br><span class=\"line\">    self.num_channels = num_channels</span><br><span class=\"line\">    self.output_tensor = tf.nn.conv2d(self.output_tensor, filter,</span><br><span class=\"line\">                                      [<span class=\"number\">1</span>, stride, stride, <span class=\"number\">1</span>], <span class=\"string\">'SAME'</span>, name=name)</span><br><span class=\"line\">    self.output_tensor = tf.add(self.output_tensor, bias)</span><br><span class=\"line\">    self._log_layer(</span><br><span class=\"line\">        <span class=\"string\">'2D Convolution layer, filter size = &#123;&#125;x&#123;&#125;, stride = &#123;&#125;, &#123;&#125; channels'</span>.format(filter_size, filter_size,</span><br><span class=\"line\">                                                                                     stride,</span><br><span class=\"line\">                                                                                     num_channels),</span><br><span class=\"line\">        disable=disable_log)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> self</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nSee? On line 16, we used a <em>tf.nn.conv2d()</em> function, the parameters of which are <em>input</em>, <em>filter</em>, <em>strides</em>, <em>padding</em>, etc. As can be guessed from the names, this function does a convolution operation with out input and the weights(the convolution <em>filter</em> here). A <em>bias</em> is added to the result as the final output. There are also many people who argue that the bias here is meaningless and should removed. One line of code is sufficient for defining a variable:\n</p>\n<p lang=\"zh-cn\">\n看见了吧？16行上，我们用了一个<em>tf.nn.conv2d()</em>函数，它的参数是<em>input</em>，<em>filter</em>，<em>strides</em>，<em>padding</em>等等。顾名思义，这个函数就是用我们定义的权重Tensor<em>filter</em>（在这里称之为卷积核）来与这一层的输入input做了一次运算。运算的结果加上了偏置Tensor<em>bias</em>，作为这个卷积层的最终输出。很多人认为这里的偏置bias意义不明，因此他们在卷积之后没有加上这样的一个bias变量。定义一个变量只需要这样一个语句：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tf.Variable(tf.truncated_normal(shape, stddev=sigma), dtype=tf.float32, name=suffix)</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nTo define weight or bias variables, create a <em>tf.Variable</em> object. Usually you&#39;ll need to give the <em>initial_value</em> which also decides the shape of this tensor. <em>tf.truncated_normal()</em> and <em>tf.constant()</em> are usually used as the initial values. Also, other APIs - function <em>tf.get_variable()</em> and package <em>tf.initializers</em> are frequently used when using some more methods for initialization. I strongly recommend that you try using these APIs yourself.\n</p>\n<p lang=\"zh-cn\">\n要定义权重或者偏置变量，请创建一个<em>tf.Variable</em>对象。通常情况下，你会需要给出<em>initial_value</em>（TF变量的初始值），这将顺便定义了这个变量的shape（因为初始值的shape是确定的）。另外，一些新的API——<em>tf.get_variable()</em>函数和<em>tf.initializers</em>包也常用与的参数初始化，以实现更多样的初始化方法。我强烈建议自己动手实践一下，试一试这些API。\n</p>\n\n<h3 id=\"3-2-Tensors-and-operations\"><a href=\"#3-2-Tensors-and-operations\" class=\"headerlink\" title=\"3.2 Tensors and operations\"></a>3.2 Tensors and operations</h3><p lang=\"en-us\">\nGoing on with the parameters of the <em>tf.nn.conv2d()</em> function. The required parameters also include <em>strides</em> and <em>padding</em>. You should have already learned about what strides mean in convolution, and I&#39;ll only talk about their formats. <em>strides</em> require a 1-D vector with a length of 4, like [1, 2, 2, 1]. The 1st and the 4th number is always 1(in order to match dimensions with the input), while the 2nd and the 3rd means the vertical stride and the horizonal stride.<br>The 4th parameter <em>padding</em> is a little bit different from its definition in convolution operation. It requires &#39;SAME&#39; of &#39;VALID&#39;, denoting &#39;with&#39; or &#39;without&#39; zero paddings. When it&#39;s &#39;SAME&#39;, zero padding is introduced to make the shapes match as needed, equally on every side of the input map.\n</p>\n<p lang=\"zh-cn\">\n接着说<em>tf.nn.conv2d()</em>函数的参数。需要的参数还包括<em>strides</em>和<em>padding</em>。你应该在了解卷积运算时学过stride（步幅）含义，我只说一下它的格式吧。<em>strides</em>参数需要是一个1维、长度为4的向量。第一位和第四位永远都是1，第二位和第三位分别是竖直方向和水平方向的步幅长。维持这个形式只是为了与输入的数据维度匹配，因此API看起来非常蹩脚。\n第四个参数<em>padding</em>和卷积运算里的padding不太相同。它的值只能是&#39;SAME&#39;或&#39;VALID&#39;，分别代表“带”和“不带”零补全。如果是&#39;SAME&#39;的话，函数会均匀地在图像的上下左右使用零补全来使得运算结果与之前尽可能保持一致。（stride&gt;1时有可能输出尺寸不是正好等于原来的尺寸/stride，因为补全问题）\n</p>\n<p lang=\"en-us\">\ntf.nn.conv2d() is just an example of TensorFlow <em>operations</em>. Other functions like <em>tf.matmul()</em>, <em>tf.reduce_mean()</em>, <em>tf.global_variables_initializer()</em>, <em>tf.losses.softmax_cross_entropy()</em>, <em>tf.truncated_normal()</em> are all operations. Operation functions return tensors(<em>tf.truncated_normal</em> also return a tensor, a tensor with initializers).\n</p>\n<p lang=\"zh-cn\">\ntf.nn.conv2d()只是TensorFlow运算（<em>operation</em>）的一个例子。其他例如<em>tf.matmul()</em>，<em>tf.reduce_mean()</em>，<em>tf.nn.relu()</em>，<em>tf.batch_normalization()</em>，<em>tf.global_variables_initializer()</em>，<em>tf.losses.softmax_cross_entropy()</em>，<em>tf.truncated_normal()</em>之类的函数也都是TensorFlow的运算。TensorFlow的运算函数会返回一个Tensor对象（包括<em>tf.truncated_normal()</em>也是！它只不过返回的是一个带初始化器的Tensor而已）。\n</p>\n<p lang=\"en-us\">\nAll the functions in the TensorChain class are based on the most basic TensorFlow operations and variables. After learning about these basic TensorFlow concepts, actually you can already abandon TensorChain, go and try implementing your own neural networks yourself<br></p>\n<p lang=\"zh-cn\">\nTensorChain类中的所有成员函数都是基于最基本的TensorFlow运算和变量的。实际上，了解了这些，你现在已经可以抛开TensorChain的束缚，去尝试实现你自己的神经网络了！\n</p>\n\n<h2 id=\"4-Spices\"><a href=\"#4-Spices\" class=\"headerlink\" title=\"4 Spices\"></a>4 Spices</h2><p lang=\"en-us\">\nI&#39;m not joking just now! But I know that there are a lot of things that you still don&#39;t understand about using TensorFlow - like &quot;how do I visualize my computation graph&quot;, &quot;how do I save/load my model to/from files&quot;, &quot;how do I record some tensors&#39; values while training&quot; or &quot;how do I view the loss curves&quot; - after all TensorFlow APIs are far more complicated than just building those nets. Those are also important techniques in your research. If you&#39;d rather ask me than spending some time experimenting, please go on with reading.\n</p>\n<p lang=\"zh-cn\">\n我，我真没开玩笑！但是我知道关于如何使用TensorFlow，你还有许许多多的问题，好比“如何可视化地查看我的计算图结构”、“如何存储/读取模型文件”、“如何记录训练过程中某些Tensor的真实值”、“如何查看损失函数的变化曲线”——毕竟TensorFlow的API太复杂了，远比搭建神经网络那点函数复杂得多。上边说的那些是你使用TensorFlow研究过程中的重要技巧。如果你愿意听我讲而不想花些时间尝试的话，请继续读下去。\n</p>\n\n<h3 id=\"4-1-Saving-and-loading-your-model\"><a href=\"#4-1-Saving-and-loading-your-model\" class=\"headerlink\" title=\"4.1 Saving and loading your model\"></a>4.1 Saving and loading your model</h3><p lang=\"en-us\">\nThe very first thing that you may want to do - after training a network model with nice outcomes - would be saving it. Saving a model is fairly easy - just use a <em>tf.train.Saver</em> object. See my code below:\n</p>\n<p lang=\"zh-cn\">\n训练出一个看起来输出还不错的神经网络模型后你想做的第一件事恐怕就是把它存下来了吧？保存模型其实非常简单：只要用一个<em>tf.train.Saver</em>类的对象。代码示例：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    <span class=\"comment\"># Train it for some iterations</span></span><br><span class=\"line\">    <span class=\"comment\"># Train it for some iterations</span></span><br><span class=\"line\">    <span class=\"comment\"># Train it for some iterations</span></span><br><span class=\"line\">    saver = tf.train.Saver()</span><br><span class=\"line\">    saver.save(session, <span class=\"string\">'models/model.ckpt'</span>)</span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nI saved my model and variable values to &#39;models/model.ckpt&#39;. But actually, you&#39;ll find 3 files in the &#39;models&#39; directory - <em>model.ckpt.data-00000-of-00001</em>, <em>model.ckpt.meta</em> and <em>model.ckpt.index</em> - none of which is &#39;model.ckpt&#39;! That&#39;s because TensorFlow stores the graph structure separately from variables values. The <em>.meta</em> file describes the saved graph structure; the <em>.index</em> file records the mappings between tensor names and tensor metadata; and the <em>.data-00000-of-00001</em> file - which is always the biggest one - saves all the variable values. If you need the graph data together with the variable values to be loaded, use a Saver to load after creating a session:\n</p>\n<p lang=\"zh-cn\">\n我把我的模型和变量值存到了&#39;models/model.ckpt&#39;文件里。但是！实际上在models目录里你会找到三个文件：<em>model.ckpt.data-00000-of-00001</em>，<em>model.ckpt.meta</em>和<em>model.ckpt.index</em>——哪个也不是model.ckpt呀？那是因为TensorFlow把计算图的结构和图中各种变量的值分开存放了。<em>.meta</em>文件描述计算图的结构；<em>.index</em>文件记录各个Tensor名称（是name属性，而不是变量名）与Tensor元信息之间的映射；<em>.data-00000-of-00001</em>文件往往是最大的一个，它存储的是各个TensorFlow变量的实际值。如果读取时需要把图结构和变量值都读进来，在session创建以后，同样用一个Saver来读取即可：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    saver = tf.train.Saver()</span><br><span class=\"line\">    saver.restore(session, <span class=\"string\">'models/model.ckpt'</span>)</span><br><span class=\"line\">    <span class=\"comment\"># Then continue doing everything just like the model is just trained</span></span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nRemember that session.run(tf.global_variables_initializer()) shouldn&#39;t be executed, since variables are already initialized with your saved <em>.data-0000-of-00001</em> file.\nIf you only need the graph to be loaded, only use the <em>.meta</em> file:\n</p>\n<p lang=\"zh-cn\">\n记住，这时候就不要再去执行session.run(tf.global_variables_initializer())了，因为变量已经用存储的checkpoint文件内容初始化过了。\n如果只需要读取计算图结构，只要读取<em>.meta</em>文件：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    tf.train.import_meta_graph(<span class=\"string\">'models/model.ckpt.meta'</span>)</span><br><span class=\"line\">    <span class=\"comment\"># Then continue doing everything just like the model is just built</span></span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nFunction <em>tf.train.import_meta_graph()</em> loads(appends) the graph to your current computation graph. The values of tensors are still uninitialized so you&#39;ll have to execute session.run(tf.global_variables_initializer()) again. The tensors that you defined in the model can be retrieved by their names(property of the Tensor objects, instead of Python variable names). For example:\n</p>\n<p lang=\"zh-cn\">\n<em>tf.train.import_meta_graph()</em>函数将文件里的计算图读到（添加到）你当前的计算图中。其中所有Tensor的值仍未初始化，所以有必要执行一下session.run(tf.global_variables_initializer())了。之前定义的变量可以按照名称取回，示例：\n</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    <span class=\"comment\"># Recover the model here</span></span><br><span class=\"line\"></span><br><span class=\"line\">    graph = tf.get_default_graph()</span><br><span class=\"line\">    image_tensor = graph.get_tensor_by_name(<span class=\"string\">'input_image:0'</span>)</span><br><span class=\"line\">    loss = graph.get_tensor_by_name(<span class=\"string\">'loss:0'</span>)</span><br><span class=\"line\">    train = graph.get_operation_by_name(<span class=\"string\">'train)</span></span><br></pre></td></tr></table></figure><br><p lang=\"en-us\">\nTo retrieve normal tensors, you&#39;ll have to append a <em>&#39;:0&#39;</em> to the name of the op. This means getting the associated tensor of the op. <em>train</em> is a little special - we only need the op, so the function is <em>get_operation_by_name()</em> so the &#39;:0&#39; is not necessary.\n</p>\n<p lang=\"zh-cn\">\n要取回一般的Tensor，需要在Tensor的name属性值后边加一个<em>&#39;:0&#39;</em>，意思是取这个运算对应的Tensor。训练操作<em>train</em>略有不同——我们要的就只是这个op，所以用的函数<em>get_operation_by_name()</em>跟其他Tensor不一样，而且&#39;:0&#39;也不需要加。\n</p>\n\n<p lang=\"en-us\" align=\"center\">\n[THIS SECTION IS UNDER CONSTRUCTION]\n</p>\n<p lang=\"zh-cn\" align=\"center\">\n[本部分内容施工中]\n</p>"},{"title":"Cheers for the 8th International linkinpark213 Day!","_content":"\nCheers for the 8th International linkinpark213 Day!\n\n```bash\nfor i in 'Harper' 'Sweet' 'Kobayashi' 'Kawasaki'\ndo\n    echo \"I'm $i, cheers!\"\ndone\n```\n\n## What is linkinpark213 Day?\n\nInternational linkinpark213 Day is a global anniversary set up by Harper Long in 2011 A.D., celebrated on February 13th every year. This anniversary is officially written as 'linkinpark213 Day', the first letter of which is a lower-case. The establishment of the anniversary dates back to the early 10s in the 21st century.\n\nTill now, the population for this anniversary has already reached 1e-6 million, and the distribution also expanded from a small county to the whole middle-China area, including Hebei, Henan, Shanxi and Shaanxi province. There will also be some Japanese resident who plan to celebrate this day in 2019.\n\n<!-- more -->\n\n## Why 13th Feb?\n\nAccording to the modern Chinese habit of writing, '13th Feb' is usually written as '2.13'. Also, '13' and 'B' look similar and are often regarded to be equal. In conclusion, '13 Feb' can be transformed to '2B', which is a common word in Chinese. Although the word is sometimes classified as \"offensive\", it reflects feelings of optimism, bravery and entertainment.\n\n## How do people celebrate the day?\n\nWhen the anniversary was first set up, there was no officially specified ways of celebration. People gathered, held parties and enjoyed spending time together. \n\nOn the 3th linkinpark213 Day, a proposal by a high school Chinese teacher was taken as the official way of celebration -- on each linkinpark213 Day, a number of people participate in the Pigeon-Flying Competition funded by Harper Long. This way of celebration prevailed until now, and all participants except Harper Long won the competition every year.\n\nPigeon-Flying is a broadly-accepted traditional Chinese custom, the exact origin of which is too ancient to be revealed. Modern scholars tend to believe that this custom became well-known no later than 206 A.D. in China. In modern times, pigeon-flying is an activity involving \"making a promise\" and \"not keeping it\". According to some folk stories, this activity was firstly named when it happened to a pigeon keeper when he forgot to keep a promise that he made with a friend. \n\n```java\npublic static void main(String[] args) {\n    Promise promise = new Promise();\n    promise.make(\"I will come!\");\n    System.exit(0);\n    promise.keep();\n}\n```\n\n(Please notice that pigeon-flying mentioned here is not the same activity as what happens every Olympics since 1896 A.D.)\n\nJoin our celebration today! You can easily participate in the Pigeon-Flying Competition by not participating.","source":"_posts/linkinpark213-day.md","raw":"---\ntitle: Cheers for the 8th International linkinpark213 Day!\ntag: Other\n---\n\nCheers for the 8th International linkinpark213 Day!\n\n```bash\nfor i in 'Harper' 'Sweet' 'Kobayashi' 'Kawasaki'\ndo\n    echo \"I'm $i, cheers!\"\ndone\n```\n\n## What is linkinpark213 Day?\n\nInternational linkinpark213 Day is a global anniversary set up by Harper Long in 2011 A.D., celebrated on February 13th every year. This anniversary is officially written as 'linkinpark213 Day', the first letter of which is a lower-case. The establishment of the anniversary dates back to the early 10s in the 21st century.\n\nTill now, the population for this anniversary has already reached 1e-6 million, and the distribution also expanded from a small county to the whole middle-China area, including Hebei, Henan, Shanxi and Shaanxi province. There will also be some Japanese resident who plan to celebrate this day in 2019.\n\n<!-- more -->\n\n## Why 13th Feb?\n\nAccording to the modern Chinese habit of writing, '13th Feb' is usually written as '2.13'. Also, '13' and 'B' look similar and are often regarded to be equal. In conclusion, '13 Feb' can be transformed to '2B', which is a common word in Chinese. Although the word is sometimes classified as \"offensive\", it reflects feelings of optimism, bravery and entertainment.\n\n## How do people celebrate the day?\n\nWhen the anniversary was first set up, there was no officially specified ways of celebration. People gathered, held parties and enjoyed spending time together. \n\nOn the 3th linkinpark213 Day, a proposal by a high school Chinese teacher was taken as the official way of celebration -- on each linkinpark213 Day, a number of people participate in the Pigeon-Flying Competition funded by Harper Long. This way of celebration prevailed until now, and all participants except Harper Long won the competition every year.\n\nPigeon-Flying is a broadly-accepted traditional Chinese custom, the exact origin of which is too ancient to be revealed. Modern scholars tend to believe that this custom became well-known no later than 206 A.D. in China. In modern times, pigeon-flying is an activity involving \"making a promise\" and \"not keeping it\". According to some folk stories, this activity was firstly named when it happened to a pigeon keeper when he forgot to keep a promise that he made with a friend. \n\n```java\npublic static void main(String[] args) {\n    Promise promise = new Promise();\n    promise.make(\"I will come!\");\n    System.exit(0);\n    promise.keep();\n}\n```\n\n(Please notice that pigeon-flying mentioned here is not the same activity as what happens every Olympics since 1896 A.D.)\n\nJoin our celebration today! You can easily participate in the Pigeon-Flying Competition by not participating.","slug":"linkinpark213-day","published":1,"date":"2018-02-13T07:03:02.047Z","updated":"2018-03-20T12:23:12.255Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomfkrfr0004zcvjzn71shs6","content":"<p>Cheers for the 8th International linkinpark213 Day!</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"string\">'Harper'</span> <span class=\"string\">'Sweet'</span> <span class=\"string\">'Kobayashi'</span> <span class=\"string\">'Kawasaki'</span></span><br><span class=\"line\"><span class=\"keyword\">do</span></span><br><span class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">\"I'm <span class=\"variable\">$i</span>, cheers!\"</span></span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"What-is-linkinpark213-Day\"><a href=\"#What-is-linkinpark213-Day\" class=\"headerlink\" title=\"What is linkinpark213 Day?\"></a>What is linkinpark213 Day?</h2><p>International linkinpark213 Day is a global anniversary set up by Harper Long in 2011 A.D., celebrated on February 13th every year. This anniversary is officially written as &#39;linkinpark213 Day&#39;, the first letter of which is a lower-case. The establishment of the anniversary dates back to the early 10s in the 21st century.</p>\n<p>Till now, the population for this anniversary has already reached 1e-6 million, and the distribution also expanded from a small county to the whole middle-China area, including Hebei, Henan, Shanxi and Shaanxi province. There will also be some Japanese resident who plan to celebrate this day in 2019.</p>\n<a id=\"more\"></a>\n<h2 id=\"Why-13th-Feb\"><a href=\"#Why-13th-Feb\" class=\"headerlink\" title=\"Why 13th Feb?\"></a>Why 13th Feb?</h2><p>According to the modern Chinese habit of writing, &#39;13th Feb&#39; is usually written as &#39;2.13&#39;. Also, &#39;13&#39; and &#39;B&#39; look similar and are often regarded to be equal. In conclusion, &#39;13 Feb&#39; can be transformed to &#39;2B&#39;, which is a common word in Chinese. Although the word is sometimes classified as &quot;offensive&quot;, it reflects feelings of optimism, bravery and entertainment.</p>\n<h2 id=\"How-do-people-celebrate-the-day\"><a href=\"#How-do-people-celebrate-the-day\" class=\"headerlink\" title=\"How do people celebrate the day?\"></a>How do people celebrate the day?</h2><p>When the anniversary was first set up, there was no officially specified ways of celebration. People gathered, held parties and enjoyed spending time together. </p>\n<p>On the 3th linkinpark213 Day, a proposal by a high school Chinese teacher was taken as the official way of celebration -- on each linkinpark213 Day, a number of people participate in the Pigeon-Flying Competition funded by Harper Long. This way of celebration prevailed until now, and all participants except Harper Long won the competition every year.</p>\n<p>Pigeon-Flying is a broadly-accepted traditional Chinese custom, the exact origin of which is too ancient to be revealed. Modern scholars tend to believe that this custom became well-known no later than 206 A.D. in China. In modern times, pigeon-flying is an activity involving &quot;making a promise&quot; and &quot;not keeping it&quot;. According to some folk stories, this activity was firstly named when it happened to a pigeon keeper when he forgot to keep a promise that he made with a friend. </p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">    Promise promise = <span class=\"keyword\">new</span> Promise();</span><br><span class=\"line\">    promise.make(<span class=\"string\">\"I will come!\"</span>);</span><br><span class=\"line\">    System.exit(<span class=\"number\">0</span>);</span><br><span class=\"line\">    promise.keep();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>(Please notice that pigeon-flying mentioned here is not the same activity as what happens every Olympics since 1896 A.D.)</p>\n<p>Join our celebration today! You can easily participate in the Pigeon-Flying Competition by not participating.</p>\n","site":{"data":{}},"excerpt":"<p>Cheers for the 8th International linkinpark213 Day!</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"string\">'Harper'</span> <span class=\"string\">'Sweet'</span> <span class=\"string\">'Kobayashi'</span> <span class=\"string\">'Kawasaki'</span></span><br><span class=\"line\"><span class=\"keyword\">do</span></span><br><span class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">\"I'm <span class=\"variable\">$i</span>, cheers!\"</span></span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"What-is-linkinpark213-Day\"><a href=\"#What-is-linkinpark213-Day\" class=\"headerlink\" title=\"What is linkinpark213 Day?\"></a>What is linkinpark213 Day?</h2><p>International linkinpark213 Day is a global anniversary set up by Harper Long in 2011 A.D., celebrated on February 13th every year. This anniversary is officially written as &#39;linkinpark213 Day&#39;, the first letter of which is a lower-case. The establishment of the anniversary dates back to the early 10s in the 21st century.</p>\n<p>Till now, the population for this anniversary has already reached 1e-6 million, and the distribution also expanded from a small county to the whole middle-China area, including Hebei, Henan, Shanxi and Shaanxi province. There will also be some Japanese resident who plan to celebrate this day in 2019.</p>","more":"<h2 id=\"Why-13th-Feb\"><a href=\"#Why-13th-Feb\" class=\"headerlink\" title=\"Why 13th Feb?\"></a>Why 13th Feb?</h2><p>According to the modern Chinese habit of writing, &#39;13th Feb&#39; is usually written as &#39;2.13&#39;. Also, &#39;13&#39; and &#39;B&#39; look similar and are often regarded to be equal. In conclusion, &#39;13 Feb&#39; can be transformed to &#39;2B&#39;, which is a common word in Chinese. Although the word is sometimes classified as &quot;offensive&quot;, it reflects feelings of optimism, bravery and entertainment.</p>\n<h2 id=\"How-do-people-celebrate-the-day\"><a href=\"#How-do-people-celebrate-the-day\" class=\"headerlink\" title=\"How do people celebrate the day?\"></a>How do people celebrate the day?</h2><p>When the anniversary was first set up, there was no officially specified ways of celebration. People gathered, held parties and enjoyed spending time together. </p>\n<p>On the 3th linkinpark213 Day, a proposal by a high school Chinese teacher was taken as the official way of celebration -- on each linkinpark213 Day, a number of people participate in the Pigeon-Flying Competition funded by Harper Long. This way of celebration prevailed until now, and all participants except Harper Long won the competition every year.</p>\n<p>Pigeon-Flying is a broadly-accepted traditional Chinese custom, the exact origin of which is too ancient to be revealed. Modern scholars tend to believe that this custom became well-known no later than 206 A.D. in China. In modern times, pigeon-flying is an activity involving &quot;making a promise&quot; and &quot;not keeping it&quot;. According to some folk stories, this activity was firstly named when it happened to a pigeon keeper when he forgot to keep a promise that he made with a friend. </p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">    Promise promise = <span class=\"keyword\">new</span> Promise();</span><br><span class=\"line\">    promise.make(<span class=\"string\">\"I will come!\"</span>);</span><br><span class=\"line\">    System.exit(<span class=\"number\">0</span>);</span><br><span class=\"line\">    promise.keep();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>(Please notice that pigeon-flying mentioned here is not the same activity as what happens every Olympics since 1896 A.D.)</p>\n<p>Join our celebration today! You can easily participate in the Pigeon-Flying Competition by not participating.</p>"},{"title":"MathJax - Use Math in Hexo, Just Like Tex! (Including Problem Solutions)","date":"2018-04-24T05:43:41.000Z","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nSometimes you may want to explain some algorithms or principles with beautiful formulae in your blog. How to do this? Edit them in Microsoft Word, take a screenshot, crop it and put it in the blog post? When you finish your article and find out that you missed a symbol in the pictures - oh man, gotta repeat that again? Stop using those images now! A beautiful math display engine - MathJax allows you to code math like a coder.\n\n<div style=\"font-size: 1.2em\">\n$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$\n</div>\n\n<!-- more -->\n\n## 1 Installation\n### 1.1 With npm (For those using Hexo like me)\nFirst, install *hexo-math* in your Hexo blog directory.\n```bash\n$ npm install hexo-math --save\n```\n\nThen, add *math* configurations in your *_config.yml* file.\n```yaml\nmath:\n  engine: 'mathjax'\n```\n\nFinally, also add to your *_config.yml* file in the **theme directory** these configurations below.\n```yaml\nmathjax:\n  enable: true\n  per_page: false\n  cdn: //cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML\n```\n\n### 1.2 Or by inserting a snippet in your HTML code\nMaybe you don't have to use math in every blog post. If so, insert the following snippet in your Markdown file also works.\n```html\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n```\n\n## 2 Usage\nMathJax supports the same grammar that LaTeX does. To learn more about LaTeX, please refer to Chapter 3 of [The Not So Short\nIntroduction to LATEX](https://tobi.oetiker.ch/lshort/lshort.pdf)(CN version also available [here](http://www.mohu.org/info/lshort-cn.pdf)).\n\nUse a \"\\\\\\\\\\(\" and a \"\\\\\\\\\\)\" to insert a formula in the line(they decide the boundary of the formula), or two \"$$\" to insert one that occupy a new line. I'll give a few examples below.\n\n\n```md\n\\\\(\\mathcal{F}(x)=\\mathcal{H}(x)-x\\\\)\n```\n\\\\(\\mathcal{F}(x)=\\mathcal{H}(x)-x\\\\)\n```md\n\\\\(E=mc^2\\\\)\n```\n\\\\(E=mc^2\\\\)\n```md\n$$\\lim_{n\\rightarrow \\infty}(1+2^n+3^n)^\\frac{1}{x+\\sin n}$$\n```\n$$\\lim_{n\\rightarrow \\infty}(1+2^n+3^n)^\\frac{1}{x+\\sin n}$$\n```md\n$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$\n```\n$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$\n\n## 3 Problems when using MathJax with Hexo & Solutions\nThis list will be appended whenever I find any more.\n### 3.1 Subscript symbol \"_\" gets mistaken as Markdown emphasize symbol\nThis is a tough problem. Hexo renderer would first render the .md file into a .html file, and the MathJax script will only work on the .html file. Therefore, when there are multiple subscript symbols, they might be rendered as &lt;em&gt;&lt;/em&gt; tags. \n\nFor example: when you actually need a full-line formula \\\\(x_{i+1}+y_j\\\\), perhaps you'll get a \"$$x<em>{i+1}+y</em>j$$\" instead. Look into the HTML code and you'll understand why.\n\nMy solution for now, is giving up this Markdown emphasize symbol, since both \"\\_\" and \"\\*\" can be used as emphasize tags, and the alternative symbol \"\\*\" will also work if we remove \"\\_\". Using \"\\\\\\_\" also works, but it would be frequently used(while \"\\*\" isn't), thus turning our math code into mess code.\n\nHow do we do this? Bravely look into the *node_modules* directory and find the renderer of the Hexo engine. My renderer is *marked*, which is the default for Hexo. There is a file named *marked.js* inside *node_modules/marked/lib/* directory. You can find two appearances of \"em:\". Like this: \n```js\nvar inline = {\n  ...\n  em: /^\\b_((?:[^_]|__)+?)_\\b|^\\*((?:\\*:\\*|[\\s\\S])+?)\\*(?!\\*)/,\n  ...\n};\n```\nand\n```js\ninline.pedantic = merge({}, inline.normal, {\n  ...\n  em: /^_(?=\\S)([\\s\\S]*?\\S)_(?!_)|^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/\n});\n```\n\nModify the regular expression after them - remove the one about \"\\_\"s and leave the one about \"\\*\"s. The new version would be:\n```js\nvar inline = {\n  ...\n  em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n  ...\n};\n```\nand\n```js\ninline.pedantic = merge({}, inline.normal, {\n  ...\n  em: /^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/\n});\n```\n\nFrom now on, you can use \"\\_\" as the subscript in MathJax freely. You don't have to worry about its becoming &lt;em&gt;&lt;/em&gt; tags anymore.\n\n### 3.2 Using \"&\" for aligning multi-line equations but getting a \"Misplaced &\"\nFor example, in my previous post about ResNet, I tried to use the following code to start a new line in an equation while aligning the lines to the equal sign:\n```md\n$$\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} & = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\\\\\n& = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)$$\n```\nThe \"&\" symbols were used to align the lines to a certain point. However, the result was a \"Misplaced &\" prompt.\n\nBy disabling MathJax, I found out that the rendered equation was correct, which means that **the problem isn't with Hexo renderer**. This was when I realized that although \n```md\n\\begin{equation}\n\\end{equation}\n```\nare not necessary, \n```md\n\\begin{split}\n\\end{split}\n```\nshouldn't be removed. Surround the equation with them will work. My code is here:\n```md\n$$\\begin{split}\n\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} & = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\\\\\n& = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)\n\\end{split}$$\n```\nAnd it runs like:\n$$\\begin{split}\n\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} & = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\\\\\n& = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)\n\\end{split}$$\n\n### 3.3 To be continued\nIf you encounter other issues while using MathJax with Hexo(with or without a solution), feel free to leave a comment below!","source":"_posts/mathjax.md","raw":"---\ntitle: 'MathJax - Use Math in Hexo, Just Like Tex! (Including Problem Solutions)'\ntags: Blogging\ndate: 2018-04-24 14:43:41\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nSometimes you may want to explain some algorithms or principles with beautiful formulae in your blog. How to do this? Edit them in Microsoft Word, take a screenshot, crop it and put it in the blog post? When you finish your article and find out that you missed a symbol in the pictures - oh man, gotta repeat that again? Stop using those images now! A beautiful math display engine - MathJax allows you to code math like a coder.\n\n<div style=\"font-size: 1.2em\">\n$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$\n</div>\n\n<!-- more -->\n\n## 1 Installation\n### 1.1 With npm (For those using Hexo like me)\nFirst, install *hexo-math* in your Hexo blog directory.\n```bash\n$ npm install hexo-math --save\n```\n\nThen, add *math* configurations in your *_config.yml* file.\n```yaml\nmath:\n  engine: 'mathjax'\n```\n\nFinally, also add to your *_config.yml* file in the **theme directory** these configurations below.\n```yaml\nmathjax:\n  enable: true\n  per_page: false\n  cdn: //cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML\n```\n\n### 1.2 Or by inserting a snippet in your HTML code\nMaybe you don't have to use math in every blog post. If so, insert the following snippet in your Markdown file also works.\n```html\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n```\n\n## 2 Usage\nMathJax supports the same grammar that LaTeX does. To learn more about LaTeX, please refer to Chapter 3 of [The Not So Short\nIntroduction to LATEX](https://tobi.oetiker.ch/lshort/lshort.pdf)(CN version also available [here](http://www.mohu.org/info/lshort-cn.pdf)).\n\nUse a \"\\\\\\\\\\(\" and a \"\\\\\\\\\\)\" to insert a formula in the line(they decide the boundary of the formula), or two \"$$\" to insert one that occupy a new line. I'll give a few examples below.\n\n\n```md\n\\\\(\\mathcal{F}(x)=\\mathcal{H}(x)-x\\\\)\n```\n\\\\(\\mathcal{F}(x)=\\mathcal{H}(x)-x\\\\)\n```md\n\\\\(E=mc^2\\\\)\n```\n\\\\(E=mc^2\\\\)\n```md\n$$\\lim_{n\\rightarrow \\infty}(1+2^n+3^n)^\\frac{1}{x+\\sin n}$$\n```\n$$\\lim_{n\\rightarrow \\infty}(1+2^n+3^n)^\\frac{1}{x+\\sin n}$$\n```md\n$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$\n```\n$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$\n\n## 3 Problems when using MathJax with Hexo & Solutions\nThis list will be appended whenever I find any more.\n### 3.1 Subscript symbol \"_\" gets mistaken as Markdown emphasize symbol\nThis is a tough problem. Hexo renderer would first render the .md file into a .html file, and the MathJax script will only work on the .html file. Therefore, when there are multiple subscript symbols, they might be rendered as &lt;em&gt;&lt;/em&gt; tags. \n\nFor example: when you actually need a full-line formula \\\\(x_{i+1}+y_j\\\\), perhaps you'll get a \"$$x<em>{i+1}+y</em>j$$\" instead. Look into the HTML code and you'll understand why.\n\nMy solution for now, is giving up this Markdown emphasize symbol, since both \"\\_\" and \"\\*\" can be used as emphasize tags, and the alternative symbol \"\\*\" will also work if we remove \"\\_\". Using \"\\\\\\_\" also works, but it would be frequently used(while \"\\*\" isn't), thus turning our math code into mess code.\n\nHow do we do this? Bravely look into the *node_modules* directory and find the renderer of the Hexo engine. My renderer is *marked*, which is the default for Hexo. There is a file named *marked.js* inside *node_modules/marked/lib/* directory. You can find two appearances of \"em:\". Like this: \n```js\nvar inline = {\n  ...\n  em: /^\\b_((?:[^_]|__)+?)_\\b|^\\*((?:\\*:\\*|[\\s\\S])+?)\\*(?!\\*)/,\n  ...\n};\n```\nand\n```js\ninline.pedantic = merge({}, inline.normal, {\n  ...\n  em: /^_(?=\\S)([\\s\\S]*?\\S)_(?!_)|^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/\n});\n```\n\nModify the regular expression after them - remove the one about \"\\_\"s and leave the one about \"\\*\"s. The new version would be:\n```js\nvar inline = {\n  ...\n  em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n  ...\n};\n```\nand\n```js\ninline.pedantic = merge({}, inline.normal, {\n  ...\n  em: /^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/\n});\n```\n\nFrom now on, you can use \"\\_\" as the subscript in MathJax freely. You don't have to worry about its becoming &lt;em&gt;&lt;/em&gt; tags anymore.\n\n### 3.2 Using \"&\" for aligning multi-line equations but getting a \"Misplaced &\"\nFor example, in my previous post about ResNet, I tried to use the following code to start a new line in an equation while aligning the lines to the equal sign:\n```md\n$$\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} & = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\\\\\n& = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)$$\n```\nThe \"&\" symbols were used to align the lines to a certain point. However, the result was a \"Misplaced &\" prompt.\n\nBy disabling MathJax, I found out that the rendered equation was correct, which means that **the problem isn't with Hexo renderer**. This was when I realized that although \n```md\n\\begin{equation}\n\\end{equation}\n```\nare not necessary, \n```md\n\\begin{split}\n\\end{split}\n```\nshouldn't be removed. Surround the equation with them will work. My code is here:\n```md\n$$\\begin{split}\n\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} & = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\\\\\n& = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)\n\\end{split}$$\n```\nAnd it runs like:\n$$\\begin{split}\n\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} & = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\\\\\n& = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)\n\\end{split}$$\n\n### 3.3 To be continued\nIf you encounter other issues while using MathJax with Hexo(with or without a solution), feel free to leave a comment below!","slug":"mathjax","published":1,"updated":"2018-04-24T08:59:11.947Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomfkrg90005zcvjntk3wg2e","content":"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML\" async></script>\n\n<p>Sometimes you may want to explain some algorithms or principles with beautiful formulae in your blog. How to do this? Edit them in Microsoft Word, take a screenshot, crop it and put it in the blog post? When you finish your article and find out that you missed a symbol in the pictures - oh man, gotta repeat that again? Stop using those images now! A beautiful math display engine - MathJax allows you to code math like a coder.</p>\n<div style=\"font-size: 1.2em\">\n$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$\n</div>\n\n<a id=\"more\"></a>\n<h2 id=\"1-Installation\"><a href=\"#1-Installation\" class=\"headerlink\" title=\"1 Installation\"></a>1 Installation</h2><h3 id=\"1-1-With-npm-For-those-using-Hexo-like-me\"><a href=\"#1-1-With-npm-For-those-using-Hexo-like-me\" class=\"headerlink\" title=\"1.1 With npm (For those using Hexo like me)\"></a>1.1 With npm (For those using Hexo like me)</h3><p>First, install <em>hexo-math</em> in your Hexo blog directory.\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install hexo-math --save</span><br></pre></td></tr></table></figure></p>\n<p>Then, add <em>math</em> configurations in your <em>_config.yml</em> file.\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">math:</span></span><br><span class=\"line\"><span class=\"attr\">  engine:</span> <span class=\"string\">'mathjax'</span></span><br></pre></td></tr></table></figure></p>\n<p>Finally, also add to your <em>_config.yml</em> file in the <strong>theme directory</strong> these configurations below.\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">mathjax:</span></span><br><span class=\"line\"><span class=\"attr\">  enable:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">  per_page:</span> <span class=\"literal\">false</span></span><br><span class=\"line\"><span class=\"attr\">  cdn:</span> <span class=\"string\">//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"1-2-Or-by-inserting-a-snippet-in-your-HTML-code\"><a href=\"#1-2-Or-by-inserting-a-snippet-in-your-HTML-code\" class=\"headerlink\" title=\"1.2 Or by inserting a snippet in your HTML code\"></a>1.2 Or by inserting a snippet in your HTML code</h3><p>Maybe you don&#39;t have to use math in every blog post. If so, insert the following snippet in your Markdown file also works.\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">script</span> <span class=\"attr\">src</span>=<span class=\"string\">'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML'</span> <span class=\"attr\">async</span>&gt;</span><span class=\"undefined\"></span><span class=\"tag\">&lt;/<span class=\"name\">script</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<h2 id=\"2-Usage\"><a href=\"#2-Usage\" class=\"headerlink\" title=\"2 Usage\"></a>2 Usage</h2><p>MathJax supports the same grammar that LaTeX does. To learn more about LaTeX, please refer to Chapter 3 of <a href=\"https://tobi.oetiker.ch/lshort/lshort.pdf\" target=\"_blank\" rel=\"noopener\">The Not So Short\nIntroduction to LATEX</a>(CN version also available <a href=\"http://www.mohu.org/info/lshort-cn.pdf\" target=\"_blank\" rel=\"noopener\">here</a>).</p>\n<p>Use a &quot;\\\\(&quot; and a &quot;\\\\)&quot; to insert a formula in the line(they decide the boundary of the formula), or two &quot;$$&quot; to insert one that occupy a new line. I&#39;ll give a few examples below.</p>\n<figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\\\(\\mathcal&#123;F&#125;(x)=\\mathcal&#123;H&#125;(x)-x\\\\)</span><br></pre></td></tr></table></figure>\n<p>\\(\\mathcal{F}(x)=\\mathcal{H}(x)-x\\<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\\\(E=mc^2\\\\)</span><br></pre></td></tr></table></figure></p>\n<p>\\(E=mc^2\\<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\lim_&#123;n\\rightarrow \\infty&#125;(1+2^n+3^n)^\\frac&#123;1&#125;&#123;x+\\sin n&#125;$$</span><br></pre></td></tr></table></figure></p>\n<p>$$\\lim_{n\\rightarrow \\infty}(1+2^n+3^n)^\\frac{1}{x+\\sin n}$$\n<figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\mathcal&#123;C&#125;\\phi \\delta e \\mathfrak&#123;M&#125;\\alpha th \\mathit&#123;I&#125;n \\mathcal&#123;H&#125;ex\\sigma \\mathbb&#123;N&#125;o\\omega!$$</span><br></pre></td></tr></table></figure></p>\n<p>$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$</p>\n<h2 id=\"3-Problems-when-using-MathJax-with-Hexo-amp-Solutions\"><a href=\"#3-Problems-when-using-MathJax-with-Hexo-amp-Solutions\" class=\"headerlink\" title=\"3 Problems when using MathJax with Hexo &amp; Solutions\"></a>3 Problems when using MathJax with Hexo &amp; Solutions</h2><p>This list will be appended whenever I find any more.</p>\n<h3 id=\"3-1-Subscript-symbol-quot-quot-gets-mistaken-as-Markdown-emphasize-symbol\"><a href=\"#3-1-Subscript-symbol-quot-quot-gets-mistaken-as-Markdown-emphasize-symbol\" class=\"headerlink\" title=\"3.1 Subscript symbol &quot;_&quot; gets mistaken as Markdown emphasize symbol\"></a>3.1 Subscript symbol &quot;_&quot; gets mistaken as Markdown emphasize symbol</h3><p>This is a tough problem. Hexo renderer would first render the .md file into a .html file, and the MathJax script will only work on the .html file. Therefore, when there are multiple subscript symbols, they might be rendered as &lt;em&gt;&lt;/em&gt; tags. </p>\n<p>For example: when you actually need a full-line formula \\(x_{i+1}+y_j\\), perhaps you&#39;ll get a &quot;$$x<em>{i+1}+y</em>j$$&quot; instead. Look into the HTML code and you&#39;ll understand why.</p>\n<p>My solution for now, is giving up this Markdown emphasize symbol, since both &quot;_&quot; and &quot;*&quot; can be used as emphasize tags, and the alternative symbol &quot;*&quot; will also work if we remove &quot;_&quot;. Using &quot;\\_&quot; also works, but it would be frequently used(while &quot;*&quot; isn&#39;t), thus turning our math code into mess code.</p>\n<p>How do we do this? Bravely look into the <em>node_modules</em> directory and find the renderer of the Hexo engine. My renderer is <em>marked</em>, which is the default for Hexo. There is a file named <em>marked.js</em> inside <em>node_modules/marked/lib/</em> directory. You can find two appearances of &quot;em:&quot;. Like this:<br><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> inline = &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  em: <span class=\"regexp\">/^\\b_((?:[^_]|__)+?)_\\b|^\\*((?:\\*:\\*|[\\s\\S])+?)\\*(?!\\*)/</span>,</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure></p>\n<p>and\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline.pedantic = merge(&#123;&#125;, inline.normal, &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  em: <span class=\"regexp\">/^_(?=\\S)([\\s\\S]*?\\S)_(?!_)|^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/</span></span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>Modify the regular expression after them - remove the one about &quot;_&quot;s and leave the one about &quot;*&quot;s. The new version would be:\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> inline = &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  em: <span class=\"regexp\">/^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/</span>,</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure></p>\n<p>and\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline.pedantic = merge(&#123;&#125;, inline.normal, &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  em: <span class=\"regexp\">/^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/</span></span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>From now on, you can use &quot;_&quot; as the subscript in MathJax freely. You don&#39;t have to worry about its becoming &lt;em&gt;&lt;/em&gt; tags anymore.</p>\n<h3 id=\"3-2-Using-quot-amp-quot-for-aligning-multi-line-equations-but-getting-a-quot-Misplaced-amp-quot\"><a href=\"#3-2-Using-quot-amp-quot-for-aligning-multi-line-equations-but-getting-a-quot-Misplaced-amp-quot\" class=\"headerlink\" title=\"3.2 Using &quot;&amp;&quot; for aligning multi-line equations but getting a &quot;Misplaced &amp;&quot;\"></a>3.2 Using &quot;&amp;&quot; for aligning multi-line equations but getting a &quot;Misplaced &amp;&quot;</h3><p>For example, in my previous post about ResNet, I tried to use the following code to start a new line in an equation while aligning the lines to the equal sign:\n<figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x<span class=\"emphasis\">_l&#125;&#125; &amp; = \\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x_</span>L&#125;&#125;\\frac&#123;\\partial&#123;x<span class=\"emphasis\">_L&#125;&#125;&#123;\\partial&#123;x_</span>l&#125;&#125;\\\\\\\\</span><br><span class=\"line\">&amp; = \\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x<span class=\"emphasis\">_L&#125;&#125;\\Big(1+\\frac&#123;\\partial&#123;&#125;&#125;&#123;\\partial&#123;x_</span>l&#125;&#125;\\sum<span class=\"emphasis\">_&#123;i=l&#125;^&#123;L-1&#125;\\mathcal&#123;F&#125;(x_</span>i,\\mathcal&#123;W&#125;_i)\\Big)$$</span><br></pre></td></tr></table></figure></p>\n<p>The &quot;&amp;&quot; symbols were used to align the lines to a certain point. However, the result was a &quot;Misplaced &amp;&quot; prompt.</p>\n<p>By disabling MathJax, I found out that the rendered equation was correct, which means that <strong>the problem isn&#39;t with Hexo renderer</strong>. This was when I realized that although<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\begin&#123;equation&#125;</span><br><span class=\"line\">\\end&#123;equation&#125;</span><br></pre></td></tr></table></figure></p>\n<p>are not necessary,<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\begin&#123;split&#125;</span><br><span class=\"line\">\\end&#123;split&#125;</span><br></pre></td></tr></table></figure></p>\n<p>shouldn&#39;t be removed. Surround the equation with them will work. My code is here:\n<figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\begin&#123;split&#125;</span><br><span class=\"line\">\\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x<span class=\"emphasis\">_l&#125;&#125; &amp; = \\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x_</span>L&#125;&#125;\\frac&#123;\\partial&#123;x<span class=\"emphasis\">_L&#125;&#125;&#123;\\partial&#123;x_</span>l&#125;&#125;\\\\\\\\</span><br><span class=\"line\">&amp; = \\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x<span class=\"emphasis\">_L&#125;&#125;\\Big(1+\\frac&#123;\\partial&#123;&#125;&#125;&#123;\\partial&#123;x_</span>l&#125;&#125;\\sum<span class=\"emphasis\">_&#123;i=l&#125;^&#123;L-1&#125;\\mathcal&#123;F&#125;(x_</span>i,\\mathcal&#123;W&#125;_i)\\Big)</span><br><span class=\"line\">\\end&#123;split&#125;$$</span><br></pre></td></tr></table></figure></p>\n<p>And it runs like:\n$$\\begin{split}\n\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} &amp; = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\n&amp; = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)\n\\end{split}$$</p>\n<h3 id=\"3-3-To-be-continued\"><a href=\"#3-3-To-be-continued\" class=\"headerlink\" title=\"3.3 To be continued\"></a>3.3 To be continued</h3><p>If you encounter other issues while using MathJax with Hexo(with or without a solution), feel free to leave a comment below!</p>\n","site":{"data":{}},"excerpt":"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML\" async></script>\n\n<p>Sometimes you may want to explain some algorithms or principles with beautiful formulae in your blog. How to do this? Edit them in Microsoft Word, take a screenshot, crop it and put it in the blog post? When you finish your article and find out that you missed a symbol in the pictures - oh man, gotta repeat that again? Stop using those images now! A beautiful math display engine - MathJax allows you to code math like a coder.</p>\n<div style=\"font-size: 1.2em\">\n$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$\n</div>","more":"<h2 id=\"1-Installation\"><a href=\"#1-Installation\" class=\"headerlink\" title=\"1 Installation\"></a>1 Installation</h2><h3 id=\"1-1-With-npm-For-those-using-Hexo-like-me\"><a href=\"#1-1-With-npm-For-those-using-Hexo-like-me\" class=\"headerlink\" title=\"1.1 With npm (For those using Hexo like me)\"></a>1.1 With npm (For those using Hexo like me)</h3><p>First, install <em>hexo-math</em> in your Hexo blog directory.\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install hexo-math --save</span><br></pre></td></tr></table></figure></p>\n<p>Then, add <em>math</em> configurations in your <em>_config.yml</em> file.\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">math:</span></span><br><span class=\"line\"><span class=\"attr\">  engine:</span> <span class=\"string\">'mathjax'</span></span><br></pre></td></tr></table></figure></p>\n<p>Finally, also add to your <em>_config.yml</em> file in the <strong>theme directory</strong> these configurations below.\n<figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">mathjax:</span></span><br><span class=\"line\"><span class=\"attr\">  enable:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">  per_page:</span> <span class=\"literal\">false</span></span><br><span class=\"line\"><span class=\"attr\">  cdn:</span> <span class=\"string\">//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"1-2-Or-by-inserting-a-snippet-in-your-HTML-code\"><a href=\"#1-2-Or-by-inserting-a-snippet-in-your-HTML-code\" class=\"headerlink\" title=\"1.2 Or by inserting a snippet in your HTML code\"></a>1.2 Or by inserting a snippet in your HTML code</h3><p>Maybe you don&#39;t have to use math in every blog post. If so, insert the following snippet in your Markdown file also works.\n<figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">script</span> <span class=\"attr\">src</span>=<span class=\"string\">'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML'</span> <span class=\"attr\">async</span>&gt;</span><span class=\"undefined\"></span><span class=\"tag\">&lt;/<span class=\"name\">script</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<h2 id=\"2-Usage\"><a href=\"#2-Usage\" class=\"headerlink\" title=\"2 Usage\"></a>2 Usage</h2><p>MathJax supports the same grammar that LaTeX does. To learn more about LaTeX, please refer to Chapter 3 of <a href=\"https://tobi.oetiker.ch/lshort/lshort.pdf\" target=\"_blank\" rel=\"noopener\">The Not So Short\nIntroduction to LATEX</a>(CN version also available <a href=\"http://www.mohu.org/info/lshort-cn.pdf\" target=\"_blank\" rel=\"noopener\">here</a>).</p>\n<p>Use a &quot;\\\\(&quot; and a &quot;\\\\)&quot; to insert a formula in the line(they decide the boundary of the formula), or two &quot;$$&quot; to insert one that occupy a new line. I&#39;ll give a few examples below.</p>\n<figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\\\(\\mathcal&#123;F&#125;(x)=\\mathcal&#123;H&#125;(x)-x\\\\)</span><br></pre></td></tr></table></figure>\n<p>\\(\\mathcal{F}(x)=\\mathcal{H}(x)-x\\<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\\\(E=mc^2\\\\)</span><br></pre></td></tr></table></figure></p>\n<p>\\(E=mc^2\\<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\lim_&#123;n\\rightarrow \\infty&#125;(1+2^n+3^n)^\\frac&#123;1&#125;&#123;x+\\sin n&#125;$$</span><br></pre></td></tr></table></figure></p>\n<p>$$\\lim_{n\\rightarrow \\infty}(1+2^n+3^n)^\\frac{1}{x+\\sin n}$$\n<figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\mathcal&#123;C&#125;\\phi \\delta e \\mathfrak&#123;M&#125;\\alpha th \\mathit&#123;I&#125;n \\mathcal&#123;H&#125;ex\\sigma \\mathbb&#123;N&#125;o\\omega!$$</span><br></pre></td></tr></table></figure></p>\n<p>$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$</p>\n<h2 id=\"3-Problems-when-using-MathJax-with-Hexo-amp-Solutions\"><a href=\"#3-Problems-when-using-MathJax-with-Hexo-amp-Solutions\" class=\"headerlink\" title=\"3 Problems when using MathJax with Hexo &amp; Solutions\"></a>3 Problems when using MathJax with Hexo &amp; Solutions</h2><p>This list will be appended whenever I find any more.</p>\n<h3 id=\"3-1-Subscript-symbol-quot-quot-gets-mistaken-as-Markdown-emphasize-symbol\"><a href=\"#3-1-Subscript-symbol-quot-quot-gets-mistaken-as-Markdown-emphasize-symbol\" class=\"headerlink\" title=\"3.1 Subscript symbol &quot;_&quot; gets mistaken as Markdown emphasize symbol\"></a>3.1 Subscript symbol &quot;_&quot; gets mistaken as Markdown emphasize symbol</h3><p>This is a tough problem. Hexo renderer would first render the .md file into a .html file, and the MathJax script will only work on the .html file. Therefore, when there are multiple subscript symbols, they might be rendered as &lt;em&gt;&lt;/em&gt; tags. </p>\n<p>For example: when you actually need a full-line formula \\(x_{i+1}+y_j\\), perhaps you&#39;ll get a &quot;$$x<em>{i+1}+y</em>j$$&quot; instead. Look into the HTML code and you&#39;ll understand why.</p>\n<p>My solution for now, is giving up this Markdown emphasize symbol, since both &quot;_&quot; and &quot;*&quot; can be used as emphasize tags, and the alternative symbol &quot;*&quot; will also work if we remove &quot;_&quot;. Using &quot;\\_&quot; also works, but it would be frequently used(while &quot;*&quot; isn&#39;t), thus turning our math code into mess code.</p>\n<p>How do we do this? Bravely look into the <em>node_modules</em> directory and find the renderer of the Hexo engine. My renderer is <em>marked</em>, which is the default for Hexo. There is a file named <em>marked.js</em> inside <em>node_modules/marked/lib/</em> directory. You can find two appearances of &quot;em:&quot;. Like this:<br><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> inline = &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  em: <span class=\"regexp\">/^\\b_((?:[^_]|__)+?)_\\b|^\\*((?:\\*:\\*|[\\s\\S])+?)\\*(?!\\*)/</span>,</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure></p>\n<p>and\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline.pedantic = merge(&#123;&#125;, inline.normal, &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  em: <span class=\"regexp\">/^_(?=\\S)([\\s\\S]*?\\S)_(?!_)|^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/</span></span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>Modify the regular expression after them - remove the one about &quot;_&quot;s and leave the one about &quot;*&quot;s. The new version would be:\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> inline = &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  em: <span class=\"regexp\">/^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/</span>,</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure></p>\n<p>and\n<figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline.pedantic = merge(&#123;&#125;, inline.normal, &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  em: <span class=\"regexp\">/^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/</span></span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>From now on, you can use &quot;_&quot; as the subscript in MathJax freely. You don&#39;t have to worry about its becoming &lt;em&gt;&lt;/em&gt; tags anymore.</p>\n<h3 id=\"3-2-Using-quot-amp-quot-for-aligning-multi-line-equations-but-getting-a-quot-Misplaced-amp-quot\"><a href=\"#3-2-Using-quot-amp-quot-for-aligning-multi-line-equations-but-getting-a-quot-Misplaced-amp-quot\" class=\"headerlink\" title=\"3.2 Using &quot;&amp;&quot; for aligning multi-line equations but getting a &quot;Misplaced &amp;&quot;\"></a>3.2 Using &quot;&amp;&quot; for aligning multi-line equations but getting a &quot;Misplaced &amp;&quot;</h3><p>For example, in my previous post about ResNet, I tried to use the following code to start a new line in an equation while aligning the lines to the equal sign:\n<figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x<span class=\"emphasis\">_l&#125;&#125; &amp; = \\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x_</span>L&#125;&#125;\\frac&#123;\\partial&#123;x<span class=\"emphasis\">_L&#125;&#125;&#123;\\partial&#123;x_</span>l&#125;&#125;\\\\\\\\</span><br><span class=\"line\">&amp; = \\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x<span class=\"emphasis\">_L&#125;&#125;\\Big(1+\\frac&#123;\\partial&#123;&#125;&#125;&#123;\\partial&#123;x_</span>l&#125;&#125;\\sum<span class=\"emphasis\">_&#123;i=l&#125;^&#123;L-1&#125;\\mathcal&#123;F&#125;(x_</span>i,\\mathcal&#123;W&#125;_i)\\Big)$$</span><br></pre></td></tr></table></figure></p>\n<p>The &quot;&amp;&quot; symbols were used to align the lines to a certain point. However, the result was a &quot;Misplaced &amp;&quot; prompt.</p>\n<p>By disabling MathJax, I found out that the rendered equation was correct, which means that <strong>the problem isn&#39;t with Hexo renderer</strong>. This was when I realized that although<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\begin&#123;equation&#125;</span><br><span class=\"line\">\\end&#123;equation&#125;</span><br></pre></td></tr></table></figure></p>\n<p>are not necessary,<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\begin&#123;split&#125;</span><br><span class=\"line\">\\end&#123;split&#125;</span><br></pre></td></tr></table></figure></p>\n<p>shouldn&#39;t be removed. Surround the equation with them will work. My code is here:\n<figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\begin&#123;split&#125;</span><br><span class=\"line\">\\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x<span class=\"emphasis\">_l&#125;&#125; &amp; = \\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x_</span>L&#125;&#125;\\frac&#123;\\partial&#123;x<span class=\"emphasis\">_L&#125;&#125;&#123;\\partial&#123;x_</span>l&#125;&#125;\\\\\\\\</span><br><span class=\"line\">&amp; = \\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x<span class=\"emphasis\">_L&#125;&#125;\\Big(1+\\frac&#123;\\partial&#123;&#125;&#125;&#123;\\partial&#123;x_</span>l&#125;&#125;\\sum<span class=\"emphasis\">_&#123;i=l&#125;^&#123;L-1&#125;\\mathcal&#123;F&#125;(x_</span>i,\\mathcal&#123;W&#125;_i)\\Big)</span><br><span class=\"line\">\\end&#123;split&#125;$$</span><br></pre></td></tr></table></figure></p>\n<p>And it runs like:\n$$\\begin{split}\n\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} &amp; = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\n&amp; = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)\n\\end{split}$$</p>\n<h3 id=\"3-3-To-be-continued\"><a href=\"#3-3-To-be-continued\" class=\"headerlink\" title=\"3.3 To be continued\"></a>3.3 To be continued</h3><p>If you encounter other issues while using MathJax with Hexo(with or without a solution), feel free to leave a comment below!</p>"},{"title":"Smartypants is NOT SO SMART","date":"2018-03-20T02:00:22.000Z","_content":"When blogging with Hexo, every time I type a single quotation mark(also called apostrophe) like this: \n```\n'\n```\n, Hexo would convert it to a symbol like this\n```\n’\n```\nYou would say that this is also an apotrophe, but it really looks UNBEARABLE in the articles. It's been a problem bothering me for more than a month.(I'm not saying that this is the reason for not updating my blog, but I don't mind if you think so!)\n\n<!-- more -->\n\n\n![apostrophe](/images/smartypants/apostrophe.png)\n\nTherefore, I Googled about this problem and tried to find other victims. According to their sharings, this problem is caused by *marked* -- the default markdown renderer of Hexo.The *\"smatrypants\"* function of marked was turned on by default.\n\nNow take a look at the introduction of *smartypants* on the *hexo-renderer-marked* page:\n> *smartypants* - Use \"smart\" typograhic punctuation for things like quotes and dashes.\n\nC'mon, seriously? \n\nThere are a few bloggers who solved this by adding the code below to the _config.yml file in the blog directory. \n\n```yml\nmark:\n  smartypants: false\n```\n\nThis worked for most victims (perhaps all of them), but not for me. I have no idea why those config wasn't working, so if anyone finds out the reason, please contact me by e-mail.\n\nIf you're sure *smartypants* is causing the problem, and the solution above didn't work for you either, maybe you can try my solution.\n\nSince *hexo-renderer-marked* is installed in the blog's *node_modules* directory(may also be in your Node.js directory if installed globally), isn't it possible that we change its own configurations? I looked at the *index.js* file in the *node_modules/hexo-renderer-marked/* directory. There you are, smartypants!\n\n```javascript\nhexo.config.marked = assign({\n  gfm: true,\n  pedantic: false,\n  sanitize: false,\n  tables: true,\n  breaks: true,\n  smartLists: true,\n  smartypants: true,\n  modifyAnchors: '',\n  autolink: true\n}, hexo.config.marked);\n```\n\nNow you know what to do.\n\nAaaaaaaaaaaaaaaand many thanks to Xizi Wu, the artist of my new avatar! I love it!\n![Harper Long by Xizi Wu](/images/long_nobg.png)","source":"_posts/smartypants.md","raw":"---\ntitle: Smartypants is NOT SO SMART\ndate: 2018-03-20 11:00:22\ntags: Blogging\n---\nWhen blogging with Hexo, every time I type a single quotation mark(also called apostrophe) like this: \n```\n'\n```\n, Hexo would convert it to a symbol like this\n```\n’\n```\nYou would say that this is also an apotrophe, but it really looks UNBEARABLE in the articles. It's been a problem bothering me for more than a month.(I'm not saying that this is the reason for not updating my blog, but I don't mind if you think so!)\n\n<!-- more -->\n\n\n![apostrophe](/images/smartypants/apostrophe.png)\n\nTherefore, I Googled about this problem and tried to find other victims. According to their sharings, this problem is caused by *marked* -- the default markdown renderer of Hexo.The *\"smatrypants\"* function of marked was turned on by default.\n\nNow take a look at the introduction of *smartypants* on the *hexo-renderer-marked* page:\n> *smartypants* - Use \"smart\" typograhic punctuation for things like quotes and dashes.\n\nC'mon, seriously? \n\nThere are a few bloggers who solved this by adding the code below to the _config.yml file in the blog directory. \n\n```yml\nmark:\n  smartypants: false\n```\n\nThis worked for most victims (perhaps all of them), but not for me. I have no idea why those config wasn't working, so if anyone finds out the reason, please contact me by e-mail.\n\nIf you're sure *smartypants* is causing the problem, and the solution above didn't work for you either, maybe you can try my solution.\n\nSince *hexo-renderer-marked* is installed in the blog's *node_modules* directory(may also be in your Node.js directory if installed globally), isn't it possible that we change its own configurations? I looked at the *index.js* file in the *node_modules/hexo-renderer-marked/* directory. There you are, smartypants!\n\n```javascript\nhexo.config.marked = assign({\n  gfm: true,\n  pedantic: false,\n  sanitize: false,\n  tables: true,\n  breaks: true,\n  smartLists: true,\n  smartypants: true,\n  modifyAnchors: '',\n  autolink: true\n}, hexo.config.marked);\n```\n\nNow you know what to do.\n\nAaaaaaaaaaaaaaaand many thanks to Xizi Wu, the artist of my new avatar! I love it!\n![Harper Long by Xizi Wu](/images/long_nobg.png)","slug":"smartypants","published":1,"updated":"2018-04-22T15:15:12.339Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomfkrgb0006zcvjwa88wl1q","content":"<p>When blogging with Hexo, every time I type a single quotation mark(also called apostrophe) like this:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure></p>\n<p>, Hexo would convert it to a symbol like this\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">’</span><br></pre></td></tr></table></figure></p>\n<p>You would say that this is also an apotrophe, but it really looks UNBEARABLE in the articles. It&#39;s been a problem bothering me for more than a month.(I&#39;m not saying that this is the reason for not updating my blog, but I don&#39;t mind if you think so!)</p>\n<a id=\"more\"></a>\n<p><img src=\"/images/smartypants/apostrophe.png\" alt=\"apostrophe\"></p>\n<p>Therefore, I Googled about this problem and tried to find other victims. According to their sharings, this problem is caused by <em>marked</em> -- the default markdown renderer of Hexo.The <em>&quot;smatrypants&quot;</em> function of marked was turned on by default.</p>\n<p>Now take a look at the introduction of <em>smartypants</em> on the <em>hexo-renderer-marked</em> page:</p>\n<blockquote>\n<p><em>smartypants</em> - Use &quot;smart&quot; typograhic punctuation for things like quotes and dashes.</p>\n</blockquote>\n<p>C&#39;mon, seriously? </p>\n<p>There are a few bloggers who solved this by adding the code below to the _config.yml file in the blog directory. </p>\n<figure class=\"highlight yml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">mark:</span></span><br><span class=\"line\"><span class=\"attr\">  smartypants:</span> <span class=\"literal\">false</span></span><br></pre></td></tr></table></figure>\n<p>This worked for most victims (perhaps all of them), but not for me. I have no idea why those config wasn&#39;t working, so if anyone finds out the reason, please contact me by e-mail.</p>\n<p>If you&#39;re sure <em>smartypants</em> is causing the problem, and the solution above didn&#39;t work for you either, maybe you can try my solution.</p>\n<p>Since <em>hexo-renderer-marked</em> is installed in the blog&#39;s <em>node_modules</em> directory(may also be in your Node.js directory if installed globally), isn&#39;t it possible that we change its own configurations? I looked at the <em>index.js</em> file in the <em>node_modules/hexo-renderer-marked/</em> directory. There you are, smartypants!</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo.config.marked = assign(&#123;</span><br><span class=\"line\">  gfm: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  pedantic: <span class=\"literal\">false</span>,</span><br><span class=\"line\">  sanitize: <span class=\"literal\">false</span>,</span><br><span class=\"line\">  tables: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  breaks: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  smartLists: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  smartypants: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  modifyAnchors: <span class=\"string\">''</span>,</span><br><span class=\"line\">  autolink: <span class=\"literal\">true</span></span><br><span class=\"line\">&#125;, hexo.config.marked);</span><br></pre></td></tr></table></figure>\n<p>Now you know what to do.</p>\n<p>Aaaaaaaaaaaaaaaand many thanks to Xizi Wu, the artist of my new avatar! I love it<br><img src=\"/images/long_nobg.png\" alt=\"Harper Long by Xizi Wu\"></p>\n","site":{"data":{}},"excerpt":"<p>When blogging with Hexo, every time I type a single quotation mark(also called apostrophe) like this:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure></p>\n<p>, Hexo would convert it to a symbol like this\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">’</span><br></pre></td></tr></table></figure></p>\n<p>You would say that this is also an apotrophe, but it really looks UNBEARABLE in the articles. It&#39;s been a problem bothering me for more than a month.(I&#39;m not saying that this is the reason for not updating my blog, but I don&#39;t mind if you think so!)</p>","more":"<p><img src=\"/images/smartypants/apostrophe.png\" alt=\"apostrophe\"></p>\n<p>Therefore, I Googled about this problem and tried to find other victims. According to their sharings, this problem is caused by <em>marked</em> -- the default markdown renderer of Hexo.The <em>&quot;smatrypants&quot;</em> function of marked was turned on by default.</p>\n<p>Now take a look at the introduction of <em>smartypants</em> on the <em>hexo-renderer-marked</em> page:</p>\n<blockquote>\n<p><em>smartypants</em> - Use &quot;smart&quot; typograhic punctuation for things like quotes and dashes.</p>\n</blockquote>\n<p>C&#39;mon, seriously? </p>\n<p>There are a few bloggers who solved this by adding the code below to the _config.yml file in the blog directory. </p>\n<figure class=\"highlight yml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">mark:</span></span><br><span class=\"line\"><span class=\"attr\">  smartypants:</span> <span class=\"literal\">false</span></span><br></pre></td></tr></table></figure>\n<p>This worked for most victims (perhaps all of them), but not for me. I have no idea why those config wasn&#39;t working, so if anyone finds out the reason, please contact me by e-mail.</p>\n<p>If you&#39;re sure <em>smartypants</em> is causing the problem, and the solution above didn&#39;t work for you either, maybe you can try my solution.</p>\n<p>Since <em>hexo-renderer-marked</em> is installed in the blog&#39;s <em>node_modules</em> directory(may also be in your Node.js directory if installed globally), isn&#39;t it possible that we change its own configurations? I looked at the <em>index.js</em> file in the <em>node_modules/hexo-renderer-marked/</em> directory. There you are, smartypants!</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo.config.marked = assign(&#123;</span><br><span class=\"line\">  gfm: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  pedantic: <span class=\"literal\">false</span>,</span><br><span class=\"line\">  sanitize: <span class=\"literal\">false</span>,</span><br><span class=\"line\">  tables: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  breaks: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  smartLists: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  smartypants: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  modifyAnchors: <span class=\"string\">''</span>,</span><br><span class=\"line\">  autolink: <span class=\"literal\">true</span></span><br><span class=\"line\">&#125;, hexo.config.marked);</span><br></pre></td></tr></table></figure>\n<p>Now you know what to do.</p>\n<p>Aaaaaaaaaaaaaaaand many thanks to Xizi Wu, the artist of my new avatar! I love it<br><img src=\"/images/long_nobg.png\" alt=\"Harper Long by Xizi Wu\"></p>"},{"title":"A Review of ResNet - Residual Networks","date":"2018-04-22T05:55:35.000Z","_content":"\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n## 0 Introduction\nDeep learning researchers have been constructing skyscrapers in recent years. Especially, VGG nets and GoogLeNet have pushed the depths of convolutional networks to the extreme. But questions remain: if time and money aren't problems, are deeper networks always performing better? Not exactly.\n\nWhen residual networks were proposed, researchers around the world was stunned by its depth. \"Jesus Christ! Is this a neural network or the Dubai Tower?\" But **don't be afraid!** These networks are deep but the structures are simple. Interestingly, these networks not only defeated all opponents in the classification, detection, localization challenges in ImageNet 2015, but were also the main innovation in the best paper of CVPR2016.\n\n<div align=\"center\">\n    <img src=\"/images/resnet/network_growth.jpg\" width=\"40%\" height=\"40%\" alt=\"Network Growth\">\n</div>\n\n<!-- more -->\n\n## 1 The Crisis: Degradation of Deep Networks\n\nVGG nets proved the beneficial of representation depth of convolutional neural networks, at least within a certain range, to be exact. However, when Kaiming He et al. tried to deepen some plain networks, the training error and test error stopped decreasing after the network reached a certain depth(which is not surprising) and soon degraded. This is not an overfitting problem, because training errors also increased; nor is it a gradient vanishing problem, because there are some techniques(e.g. batch normalization[4]) that ease the pain.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/downgrade.png\" width=\"60%\" height=\"60%\" alt=\"The Downgrade Problem\">\n    Fig.1 The downgrade problem\n\n</div>\n\nWhat seems to be the cause of this degradation? Obviously, deeper neural networks are more difficult to train, but that doesn't mean deeper neural networks would yield worse results. To explain this problem, Balduzzi et al.[3] identified shattered gradient problem - as depth increases, gradients in standard feedforward networks increasingly resemble white noise. I will write about that later.\n\n## 2 A Closer Look at ResNet: The Residual Blocks\n\nAs the old saying goes, \"千里之行，始于足下\". Although ResNets are as deep as a thousand layers, they are built with these basic residual blocks(the right part of the figure). \n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/residual_blocks.png\" width=\"50%\" height=\"50%\" alt=\"Comparison between normal weight layers and residual blocks\">\n    Fig.2 Parts of plain networks and a residual block(or residual unit)\n\n</div>\n\n### 2.1 Skip Connections\n\nIn comparison, basic units of plain network models would look like the one on the left: one ReLU function after a weight layer(usually also with biases), repeated several times. Let's denote the desired underlying mapping(the ideal mapping) of the two layers as \\\\(\\mathcal{H}(x)\\\\), and the real mapping as \\\\(\\mathcal{F}(x)\\\\). Clearly, the closer \\\\(\\mathcal{F}(x)\\\\) is to \\\\(\\mathcal{H}(x)\\\\), the better it fits.\n\nHowever, He et al. explicitly let these layers fit a residual mapping instead of the desired underlying mapping. This is implemented with \"shortcut connections\", which skip one or more layers, simply performing identity mappings and getting added to the outputs of the stacked weight layers. This way, \\\\(\\mathcal{F}(x)\\\\) would not try to fit \\\\(\\mathcal{H}(x)\\\\), but \\\\(\\mathcal{H}(x)-x\\\\). The whole structure(from the identity mapping branch, to merging the branches by the addition operation) are named \"residual blocks\"(or \"residual units\").\n\nWhat's the point in this? Let's do a simple analysis. The computation done by the original residual block is: $$y_l=h(x_l)+\\mathcal{F}(x_l,\\mathcal{W}_l),$$ $$x_{l+1}=f(y_l).$$\n\nHere are the definitions of symbols: \n\\\\(x_l\\\\): input features to the \\\\(l\\\\)-th residual block; \n\\\\(\\mathcal{W}_{l}={W_{l,k}|_{1\\leq k\\leq K}}\\\\): a set of weights(and biases) associated with the \\\\(l\\\\)-th residual unit. \\\\(K\\\\) is the number of layers in this block;\n\\\\(\\mathcal{F}(x,\\mathcal{W})\\\\): the residual function, which we talked about earlier. It's a stack of 2 conv. layers here;\n\\\\(f(x)\\\\): the activation function. We are using ReLU here;\n\\\\(h(x)\\\\): identity mapping.\n\nIf \\\\(f(x)\\\\) is also an identity mapping(as if we're not using any activation function), the first equation would become:\n$$x_{l+1}=x_l+\\mathcal{F}(x_l,\\mathcal{W}_l)$$\n\nTherefore, we can define \\\\(x_L\\\\) recursively of any layer:\n$$x_L=x_l+\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)$$\n\nThat's not the end yet! When it comes to the gradients, according to the chain rules of backpropagation, we have a beautiful definition:\n$$\\begin{split}\n\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} & = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\\\\\n& = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)\n\\end{split}$$\n\nWhat does it mean? It means that the information is directly backpropagated to ANY shallower block. This way, the gradients of a layer never vanish or explode even if the weights are too small or too big.\n\n### 2.2 Identity Mappings\nIt's important that we use identity mapping here! Just consider doing a simple modification here, for example, \\\\(h(x)=\\lambda_lx_l\\\\)(\\\\(\\lambda_l\\\\) is a modulating scalar). The definition of \\\\(x_L\\\\) and \\\\(\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}}\\\\) would become:\n$$x_L=(\\prod_{i=l}^{L-1}\\lambda_i)x_l+\\sum_{i=l}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i,\\mathcal{W}_i)$$\n$$\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}}=\\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big((\\prod_{i=l}^{L-1}\\lambda_i)+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)$$\n\nFor extremely deep neural networks where \\\\(L\\\\) is too large, \\\\(\\prod_{i=l}^{L-1}\\lambda_i\\\\) could be either too small or too large, causing gradient vanishing or gradient explosion. For \\\\(h(x)\\\\) with complex definitions, the gradient could be extremely complicated, thus losing the advantage of the skip connection. Skip connection works best under the condition where the grey channel in Fig. 3 cover no operations (except the addition) and is clean.\n\nInterestingly, this comfirmed the philosophy of \"大道至简\" once again.\n\n### 2.3 Post-activation or Pre-activation?\n\nWait a second... \"\\\\(f(x)\\\\) is also an identity mapping\" is just our assumption. The activation function is still there!\n\nRight. There IS an activation function, but it's moved to somewhere else.  In fact, the original residual block is still a little bit problematic - the output of one residual block is not always the input of the next, since there is a ReLU activation function after the addition(It did NOT REALLY keep the identity mapping to the next block!). Therefore, in[2], He et al. fixed the residual blocks by changing the order of operations.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/identity_mapping.png\" width=\"30%\" height=\"30%\" alt=\"New identity mapping\">\n    Fig.3 New identity mapping proposed by He et al.\n\n</div>\n\nBesides using a simple identity mapping, He et al. also discussed about the position of the activation function and the batch normalization operation. Assuming that we got a special(asymmetric) activation function \\\\(\\hat f(x)\\\\), which only affects the path to the next residual unit. Now our definition of \\\\(x_{x+1}\\\\) would become:\n$$x_{l+1}=x_l+\\mathcal{F}(\\hat f(x_l),\\mathcal{W}_l)$$\n\nWith \\\\(x_l\\\\) still multiplied by 1, information is still fully backpropagated to shallower residual blocks. And the good thing is that using this asymmetric activation function after the addition(partial post-activation) is equivalent to using it beforehand(pre-activation)! This is why He et al. chose to use pre-activation - otherwise it would be necessary to implement that magical activation function \\\\(\\hat f(x)\\\\).\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/pre-activation.png\" width=\"80%\" height=\"80%\" alt=\"Asymmetric after-addition activation\">\n    Fig.4 Using asymmetric after-addition activation is equivalent to constructing a pre-activation residual unit\n\n</div>\n\n## 3 ResNet Architectures\n\nHere are the ResNet architectures for ImageNet. Building blocks are shown in brackets, with the numbers of blocks stacked. With the first block of every stack(starting from conv3_x), a downsampling is performed. Each column represents one of the residual networks, and the deepest one has 152 weight layers! Since ResNets were proposed, VGG nets - which were officially called \"Very Deep Convolutional Networks\" - are not relatively deep anymore. Maybe call them \"A Little Bit Deep Convolutional Networks\".\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/architectures.png\" width=\"80%\" height=\"80%\" alt=\"ResNet architectures for ImageNet\">\n    Table. 1 ResNet architectures for ImageNet.\n\n</div>\n\n## 4 Experiments\n\n### 4.1 Performance on ImageNet\nHe et al. trained ResNet-18 and ResNet-34 on the ImageNet dataset, and also compared them to plain convolutional networks. In Fig. 5, the thin curves denote training error, and the bold ones denote validation error. The figure on the left shows the results of plain convolution networks(in which the 34-layered ones has higher error rates than the 18-layered one), and the figure on the right shows that residual networks perform better than plain ones, while deeper ones perform better than shallow ones.\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/training.png\" width=\"80%\" height=\"80%\" alt=\"Training ResNet on ImageNet\"/>\n    Fig. 5 Training ResNet on ImageNet\n\n</div>\n\n### 4.2 Effects of Different Shortcut Connections\n\nHe et al. also tried various types of shortcut connections to replace the identity mapping, and various positions of activation functions / batch normalization. Experiments show that the original identity mapping and full pre-activation yield the best results.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/shortcut-connections.png\" width=\"50%\" height=\"50%\" alt=\"Different shortcuts of residual units\"/>\n    Fig. 6 Various shortcuts in residual units\n\n    <img src=\"/images/resnet/shortcut-connections-experiment.png\" width=\"70%\" height=\"70%\" alt=\"Classification errors with different shortcuts\"/>\n    Table. 2 Classification error on CIFAR-10 test set with various shortcut connections in residual units\n\n    <img src=\"/images/resnet/activations.png\" width=\"70%\" height=\"50%\" alt=\"Different usages of activation in residual units\"/>\n    Fig. 7 Various usages of activation in residual units\n\n    <img src=\"/images/resnet/activations-experiment.png\" width=\"50%\" height=\"50%\" alt=\"Classification errors with different activations\"/>\n    Table. 3 Classification error on CIFAR-10 test set with various usages of activation in residual units\n\n</div>\n\n\n## 5 Conclusion\nResidual learning can be crowned as \"ONE OF THE GREATEST HITS IN DEEP LEARNING FIELDS\". With a simple identity mapping, it solved the degradation problem of deep neural networks. Now that you have learned about the concept of ResNet, why not give it a try and implement your first residual learning model today?\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/resnet-yooo.jpg\" width=\"40%\" height=\"40%\" alt=\"\">\n</div>\n\n## References\n\n[1] [He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.](http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n\n[2] [He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks[C]//European Conference on Computer Vision. Springer, Cham, 2016: 630-645.](https://arxiv.org/pdf/1603.05027.pdf)\n\n[3] [Balduzzi D, Frean M, Leary L, et al. The Shattered Gradients Problem: If resnets are the answer, then what is the question?[J]. arXiv preprint arXiv:1702.08591, 2017.](https://arxiv.org/pdf/1702.08591.pdf)\n\n[4] [Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.](https://arxiv.org/pdf/1502.03167.pdf)","source":"_posts/resnet.md","raw":"---\ntitle: A Review of ResNet - Residual Networks\ndate: 2018-04-22 14:55:35\ntags: [Deep Learning, Computer Vision]\n---\n\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n## 0 Introduction\nDeep learning researchers have been constructing skyscrapers in recent years. Especially, VGG nets and GoogLeNet have pushed the depths of convolutional networks to the extreme. But questions remain: if time and money aren't problems, are deeper networks always performing better? Not exactly.\n\nWhen residual networks were proposed, researchers around the world was stunned by its depth. \"Jesus Christ! Is this a neural network or the Dubai Tower?\" But **don't be afraid!** These networks are deep but the structures are simple. Interestingly, these networks not only defeated all opponents in the classification, detection, localization challenges in ImageNet 2015, but were also the main innovation in the best paper of CVPR2016.\n\n<div align=\"center\">\n    <img src=\"/images/resnet/network_growth.jpg\" width=\"40%\" height=\"40%\" alt=\"Network Growth\">\n</div>\n\n<!-- more -->\n\n## 1 The Crisis: Degradation of Deep Networks\n\nVGG nets proved the beneficial of representation depth of convolutional neural networks, at least within a certain range, to be exact. However, when Kaiming He et al. tried to deepen some plain networks, the training error and test error stopped decreasing after the network reached a certain depth(which is not surprising) and soon degraded. This is not an overfitting problem, because training errors also increased; nor is it a gradient vanishing problem, because there are some techniques(e.g. batch normalization[4]) that ease the pain.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/downgrade.png\" width=\"60%\" height=\"60%\" alt=\"The Downgrade Problem\">\n    Fig.1 The downgrade problem\n\n</div>\n\nWhat seems to be the cause of this degradation? Obviously, deeper neural networks are more difficult to train, but that doesn't mean deeper neural networks would yield worse results. To explain this problem, Balduzzi et al.[3] identified shattered gradient problem - as depth increases, gradients in standard feedforward networks increasingly resemble white noise. I will write about that later.\n\n## 2 A Closer Look at ResNet: The Residual Blocks\n\nAs the old saying goes, \"千里之行，始于足下\". Although ResNets are as deep as a thousand layers, they are built with these basic residual blocks(the right part of the figure). \n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/residual_blocks.png\" width=\"50%\" height=\"50%\" alt=\"Comparison between normal weight layers and residual blocks\">\n    Fig.2 Parts of plain networks and a residual block(or residual unit)\n\n</div>\n\n### 2.1 Skip Connections\n\nIn comparison, basic units of plain network models would look like the one on the left: one ReLU function after a weight layer(usually also with biases), repeated several times. Let's denote the desired underlying mapping(the ideal mapping) of the two layers as \\\\(\\mathcal{H}(x)\\\\), and the real mapping as \\\\(\\mathcal{F}(x)\\\\). Clearly, the closer \\\\(\\mathcal{F}(x)\\\\) is to \\\\(\\mathcal{H}(x)\\\\), the better it fits.\n\nHowever, He et al. explicitly let these layers fit a residual mapping instead of the desired underlying mapping. This is implemented with \"shortcut connections\", which skip one or more layers, simply performing identity mappings and getting added to the outputs of the stacked weight layers. This way, \\\\(\\mathcal{F}(x)\\\\) would not try to fit \\\\(\\mathcal{H}(x)\\\\), but \\\\(\\mathcal{H}(x)-x\\\\). The whole structure(from the identity mapping branch, to merging the branches by the addition operation) are named \"residual blocks\"(or \"residual units\").\n\nWhat's the point in this? Let's do a simple analysis. The computation done by the original residual block is: $$y_l=h(x_l)+\\mathcal{F}(x_l,\\mathcal{W}_l),$$ $$x_{l+1}=f(y_l).$$\n\nHere are the definitions of symbols: \n\\\\(x_l\\\\): input features to the \\\\(l\\\\)-th residual block; \n\\\\(\\mathcal{W}_{l}={W_{l,k}|_{1\\leq k\\leq K}}\\\\): a set of weights(and biases) associated with the \\\\(l\\\\)-th residual unit. \\\\(K\\\\) is the number of layers in this block;\n\\\\(\\mathcal{F}(x,\\mathcal{W})\\\\): the residual function, which we talked about earlier. It's a stack of 2 conv. layers here;\n\\\\(f(x)\\\\): the activation function. We are using ReLU here;\n\\\\(h(x)\\\\): identity mapping.\n\nIf \\\\(f(x)\\\\) is also an identity mapping(as if we're not using any activation function), the first equation would become:\n$$x_{l+1}=x_l+\\mathcal{F}(x_l,\\mathcal{W}_l)$$\n\nTherefore, we can define \\\\(x_L\\\\) recursively of any layer:\n$$x_L=x_l+\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)$$\n\nThat's not the end yet! When it comes to the gradients, according to the chain rules of backpropagation, we have a beautiful definition:\n$$\\begin{split}\n\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} & = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\\\\\n& = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)\n\\end{split}$$\n\nWhat does it mean? It means that the information is directly backpropagated to ANY shallower block. This way, the gradients of a layer never vanish or explode even if the weights are too small or too big.\n\n### 2.2 Identity Mappings\nIt's important that we use identity mapping here! Just consider doing a simple modification here, for example, \\\\(h(x)=\\lambda_lx_l\\\\)(\\\\(\\lambda_l\\\\) is a modulating scalar). The definition of \\\\(x_L\\\\) and \\\\(\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}}\\\\) would become:\n$$x_L=(\\prod_{i=l}^{L-1}\\lambda_i)x_l+\\sum_{i=l}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i,\\mathcal{W}_i)$$\n$$\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}}=\\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big((\\prod_{i=l}^{L-1}\\lambda_i)+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)$$\n\nFor extremely deep neural networks where \\\\(L\\\\) is too large, \\\\(\\prod_{i=l}^{L-1}\\lambda_i\\\\) could be either too small or too large, causing gradient vanishing or gradient explosion. For \\\\(h(x)\\\\) with complex definitions, the gradient could be extremely complicated, thus losing the advantage of the skip connection. Skip connection works best under the condition where the grey channel in Fig. 3 cover no operations (except the addition) and is clean.\n\nInterestingly, this comfirmed the philosophy of \"大道至简\" once again.\n\n### 2.3 Post-activation or Pre-activation?\n\nWait a second... \"\\\\(f(x)\\\\) is also an identity mapping\" is just our assumption. The activation function is still there!\n\nRight. There IS an activation function, but it's moved to somewhere else.  In fact, the original residual block is still a little bit problematic - the output of one residual block is not always the input of the next, since there is a ReLU activation function after the addition(It did NOT REALLY keep the identity mapping to the next block!). Therefore, in[2], He et al. fixed the residual blocks by changing the order of operations.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/identity_mapping.png\" width=\"30%\" height=\"30%\" alt=\"New identity mapping\">\n    Fig.3 New identity mapping proposed by He et al.\n\n</div>\n\nBesides using a simple identity mapping, He et al. also discussed about the position of the activation function and the batch normalization operation. Assuming that we got a special(asymmetric) activation function \\\\(\\hat f(x)\\\\), which only affects the path to the next residual unit. Now our definition of \\\\(x_{x+1}\\\\) would become:\n$$x_{l+1}=x_l+\\mathcal{F}(\\hat f(x_l),\\mathcal{W}_l)$$\n\nWith \\\\(x_l\\\\) still multiplied by 1, information is still fully backpropagated to shallower residual blocks. And the good thing is that using this asymmetric activation function after the addition(partial post-activation) is equivalent to using it beforehand(pre-activation)! This is why He et al. chose to use pre-activation - otherwise it would be necessary to implement that magical activation function \\\\(\\hat f(x)\\\\).\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/pre-activation.png\" width=\"80%\" height=\"80%\" alt=\"Asymmetric after-addition activation\">\n    Fig.4 Using asymmetric after-addition activation is equivalent to constructing a pre-activation residual unit\n\n</div>\n\n## 3 ResNet Architectures\n\nHere are the ResNet architectures for ImageNet. Building blocks are shown in brackets, with the numbers of blocks stacked. With the first block of every stack(starting from conv3_x), a downsampling is performed. Each column represents one of the residual networks, and the deepest one has 152 weight layers! Since ResNets were proposed, VGG nets - which were officially called \"Very Deep Convolutional Networks\" - are not relatively deep anymore. Maybe call them \"A Little Bit Deep Convolutional Networks\".\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/architectures.png\" width=\"80%\" height=\"80%\" alt=\"ResNet architectures for ImageNet\">\n    Table. 1 ResNet architectures for ImageNet.\n\n</div>\n\n## 4 Experiments\n\n### 4.1 Performance on ImageNet\nHe et al. trained ResNet-18 and ResNet-34 on the ImageNet dataset, and also compared them to plain convolutional networks. In Fig. 5, the thin curves denote training error, and the bold ones denote validation error. The figure on the left shows the results of plain convolution networks(in which the 34-layered ones has higher error rates than the 18-layered one), and the figure on the right shows that residual networks perform better than plain ones, while deeper ones perform better than shallow ones.\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/training.png\" width=\"80%\" height=\"80%\" alt=\"Training ResNet on ImageNet\"/>\n    Fig. 5 Training ResNet on ImageNet\n\n</div>\n\n### 4.2 Effects of Different Shortcut Connections\n\nHe et al. also tried various types of shortcut connections to replace the identity mapping, and various positions of activation functions / batch normalization. Experiments show that the original identity mapping and full pre-activation yield the best results.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/shortcut-connections.png\" width=\"50%\" height=\"50%\" alt=\"Different shortcuts of residual units\"/>\n    Fig. 6 Various shortcuts in residual units\n\n    <img src=\"/images/resnet/shortcut-connections-experiment.png\" width=\"70%\" height=\"70%\" alt=\"Classification errors with different shortcuts\"/>\n    Table. 2 Classification error on CIFAR-10 test set with various shortcut connections in residual units\n\n    <img src=\"/images/resnet/activations.png\" width=\"70%\" height=\"50%\" alt=\"Different usages of activation in residual units\"/>\n    Fig. 7 Various usages of activation in residual units\n\n    <img src=\"/images/resnet/activations-experiment.png\" width=\"50%\" height=\"50%\" alt=\"Classification errors with different activations\"/>\n    Table. 3 Classification error on CIFAR-10 test set with various usages of activation in residual units\n\n</div>\n\n\n## 5 Conclusion\nResidual learning can be crowned as \"ONE OF THE GREATEST HITS IN DEEP LEARNING FIELDS\". With a simple identity mapping, it solved the degradation problem of deep neural networks. Now that you have learned about the concept of ResNet, why not give it a try and implement your first residual learning model today?\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/resnet-yooo.jpg\" width=\"40%\" height=\"40%\" alt=\"\">\n</div>\n\n## References\n\n[1] [He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.](http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n\n[2] [He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks[C]//European Conference on Computer Vision. Springer, Cham, 2016: 630-645.](https://arxiv.org/pdf/1603.05027.pdf)\n\n[3] [Balduzzi D, Frean M, Leary L, et al. The Shattered Gradients Problem: If resnets are the answer, then what is the question?[J]. arXiv preprint arXiv:1702.08591, 2017.](https://arxiv.org/pdf/1702.08591.pdf)\n\n[4] [Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.](https://arxiv.org/pdf/1502.03167.pdf)","slug":"resnet","published":1,"updated":"2018-04-24T08:23:16.165Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomfkrgj0008zcvj09y2wnzy","content":"<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n<h2 id=\"0-Introduction\"><a href=\"#0-Introduction\" class=\"headerlink\" title=\"0 Introduction\"></a>0 Introduction</h2><p>Deep learning researchers have been constructing skyscrapers in recent years. Especially, VGG nets and GoogLeNet have pushed the depths of convolutional networks to the extreme. But questions remain: if time and money aren&#39;t problems, are deeper networks always performing better? Not exactly.</p>\n<p>When residual networks were proposed, researchers around the world was stunned by its depth. &quot;Jesus Christ! Is this a neural network or the Dubai Tower?&quot; But <strong>don&#39;t be afraid!</strong> These networks are deep but the structures are simple. Interestingly, these networks not only defeated all opponents in the classification, detection, localization challenges in ImageNet 2015, but were also the main innovation in the best paper of CVPR2016.</p>\n<div align=\"center\">\n    <img src=\"/images/resnet/network_growth.jpg\" width=\"40%\" height=\"40%\" alt=\"Network Growth\">\n</div>\n\n<a id=\"more\"></a>\n<h2 id=\"1-The-Crisis-Degradation-of-Deep-Networks\"><a href=\"#1-The-Crisis-Degradation-of-Deep-Networks\" class=\"headerlink\" title=\"1 The Crisis: Degradation of Deep Networks\"></a>1 The Crisis: Degradation of Deep Networks</h2><p>VGG nets proved the beneficial of representation depth of convolutional neural networks, at least within a certain range, to be exact. However, when Kaiming He et al. tried to deepen some plain networks, the training error and test error stopped decreasing after the network reached a certain depth(which is not surprising) and soon degraded. This is not an overfitting problem, because training errors also increased; nor is it a gradient vanishing problem, because there are some techniques(e.g. batch normalization[4]) that ease the pain.</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/downgrade.png\" width=\"60%\" height=\"60%\" alt=\"The Downgrade Problem\">\n    Fig.1 The downgrade problem<br></div>\n\n<p>What seems to be the cause of this degradation? Obviously, deeper neural networks are more difficult to train, but that doesn&#39;t mean deeper neural networks would yield worse results. To explain this problem, Balduzzi et al.[3] identified shattered gradient problem - as depth increases, gradients in standard feedforward networks increasingly resemble white noise. I will write about that later.</p>\n<h2 id=\"2-A-Closer-Look-at-ResNet-The-Residual-Blocks\"><a href=\"#2-A-Closer-Look-at-ResNet-The-Residual-Blocks\" class=\"headerlink\" title=\"2 A Closer Look at ResNet: The Residual Blocks\"></a>2 A Closer Look at ResNet: The Residual Blocks</h2><p>As the old saying goes, &quot;千里之行，始于足下&quot;. Although ResNets are as deep as a thousand layers, they are built with these basic residual blocks(the right part of the figure). </p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/residual_blocks.png\" width=\"50%\" height=\"50%\" alt=\"Comparison between normal weight layers and residual blocks\">\n    Fig.2 Parts of plain networks and a residual block(or residual unit)<br></div>\n\n<h3 id=\"2-1-Skip-Connections\"><a href=\"#2-1-Skip-Connections\" class=\"headerlink\" title=\"2.1 Skip Connections\"></a>2.1 Skip Connections</h3><p>In comparison, basic units of plain network models would look like the one on the left: one ReLU function after a weight layer(usually also with biases), repeated several times. Let&#39;s denote the desired underlying mapping(the ideal mapping) of the two layers as \\(\\mathcal{H}(x)\\), and the real mapping as \\(\\mathcal{F}(x)\\). Clearly, the closer \\(\\mathcal{F}(x)\\) is to \\(\\mathcal{H}(x)\\), the better it fits.</p>\n<p>However, He et al. explicitly let these layers fit a residual mapping instead of the desired underlying mapping. This is implemented with &quot;shortcut connections&quot;, which skip one or more layers, simply performing identity mappings and getting added to the outputs of the stacked weight layers. This way, \\(\\mathcal{F}(x)\\) would not try to fit \\(\\mathcal{H}(x)\\), but \\(\\mathcal{H}(x)-x\\). The whole structure(from the identity mapping branch, to merging the branches by the addition operation) are named &quot;residual blocks&quot;(or &quot;residual units&quot;).</p>\n<p>What&#39;s the point in this? Let&#39;s do a simple analysis. The computation done by the original residual block is: $$y_l=h(x_l)+\\mathcal{F}(x_l,\\mathcal{W}_l),$$ $$x_{l+1}=f(y_l).$$</p>\n<p>Here are the definitions of symbols:<br>\\(x_l\\): input features to the \\(l\\)-th residual block;<br>\\(\\mathcal{W}_{l}={W_{l,k}|_{1\\leq k\\leq K}}\\): a set of weights(and biases) associated with the \\(l\\)-th residual unit. \\(K\\) is the number of layers in this block;\n\\(\\mathcal{F}(x,\\mathcal{W})\\): the residual function, which we talked about earlier. It&#39;s a stack of 2 conv. layers here;\n\\(f(x)\\): the activation function. We are using ReLU here;\n\\(h(x)\\): identity mapping.</p>\n<p>If \\(f(x)\\) is also an identity mapping(as if we&#39;re not using any activation function), the first equation would become:\n$$x_{l+1}=x_l+\\mathcal{F}(x_l,\\mathcal{W}_l)$$</p>\n<p>Therefore, we can define \\(x_L\\) recursively of any layer:\n$$x_L=x_l+\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)$$</p>\n<p>That&#39;s not the end yet! When it comes to the gradients, according to the chain rules of backpropagation, we have a beautiful definition:\n$$\\begin{split}\n\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} &amp; = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\n&amp; = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)\n\\end{split}$$</p>\n<p>What does it mean? It means that the information is directly backpropagated to ANY shallower block. This way, the gradients of a layer never vanish or explode even if the weights are too small or too big.</p>\n<h3 id=\"2-2-Identity-Mappings\"><a href=\"#2-2-Identity-Mappings\" class=\"headerlink\" title=\"2.2 Identity Mappings\"></a>2.2 Identity Mappings</h3><p>It&#39;s important that we use identity mapping here! Just consider doing a simple modification here, for example, \\(h(x)=\\lambda_lx_l\\)(\\(\\lambda_l\\) is a modulating scalar). The definition of \\(x_L\\) and \\(\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}}\\) would become:\n$$x_L=(\\prod_{i=l}^{L-1}\\lambda_i)x_l+\\sum_{i=l}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i,\\mathcal{W}_i)$$\n$$\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}}=\\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big((\\prod_{i=l}^{L-1}\\lambda_i)+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)$$</p>\n<p>For extremely deep neural networks where \\(L\\) is too large, \\(\\prod_{i=l}^{L-1}\\lambda_i\\) could be either too small or too large, causing gradient vanishing or gradient explosion. For \\(h(x)\\) with complex definitions, the gradient could be extremely complicated, thus losing the advantage of the skip connection. Skip connection works best under the condition where the grey channel in Fig. 3 cover no operations (except the addition) and is clean.</p>\n<p>Interestingly, this comfirmed the philosophy of &quot;大道至简&quot; once again.</p>\n<h3 id=\"2-3-Post-activation-or-Pre-activation\"><a href=\"#2-3-Post-activation-or-Pre-activation\" class=\"headerlink\" title=\"2.3 Post-activation or Pre-activation?\"></a>2.3 Post-activation or Pre-activation?</h3><p>Wait a second... &quot;\\(f(x)\\) is also an identity mapping&quot; is just our assumption. The activation function is still there!</p>\n<p>Right. There IS an activation function, but it&#39;s moved to somewhere else.  In fact, the original residual block is still a little bit problematic - the output of one residual block is not always the input of the next, since there is a ReLU activation function after the addition(It did NOT REALLY keep the identity mapping to the next block!). Therefore, in[2], He et al. fixed the residual blocks by changing the order of operations.</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/identity_mapping.png\" width=\"30%\" height=\"30%\" alt=\"New identity mapping\">\n    Fig.3 New identity mapping proposed by He et al.<br></div>\n\n<p>Besides using a simple identity mapping, He et al. also discussed about the position of the activation function and the batch normalization operation. Assuming that we got a special(asymmetric) activation function \\(\\hat f(x)\\), which only affects the path to the next residual unit. Now our definition of \\(x_{x+1}\\) would become:\n$$x_{l+1}=x_l+\\mathcal{F}(\\hat f(x_l),\\mathcal{W}_l)$$</p>\n<p>With \\(x_l\\) still multiplied by 1, information is still fully backpropagated to shallower residual blocks. And the good thing is that using this asymmetric activation function after the addition(partial post-activation) is equivalent to using it beforehand(pre-activation)! This is why He et al. chose to use pre-activation - otherwise it would be necessary to implement that magical activation function \\(\\hat f(x)\\).</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/pre-activation.png\" width=\"80%\" height=\"80%\" alt=\"Asymmetric after-addition activation\">\n    Fig.4 Using asymmetric after-addition activation is equivalent to constructing a pre-activation residual unit<br></div>\n\n<h2 id=\"3-ResNet-Architectures\"><a href=\"#3-ResNet-Architectures\" class=\"headerlink\" title=\"3 ResNet Architectures\"></a>3 ResNet Architectures</h2><p>Here are the ResNet architectures for ImageNet. Building blocks are shown in brackets, with the numbers of blocks stacked. With the first block of every stack(starting from conv3_x), a downsampling is performed. Each column represents one of the residual networks, and the deepest one has 152 weight layers! Since ResNets were proposed, VGG nets - which were officially called &quot;Very Deep Convolutional Networks&quot; - are not relatively deep anymore. Maybe call them &quot;A Little Bit Deep Convolutional Networks&quot;.</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/architectures.png\" width=\"80%\" height=\"80%\" alt=\"ResNet architectures for ImageNet\">\n    Table. 1 ResNet architectures for ImageNet.<br></div>\n\n<h2 id=\"4-Experiments\"><a href=\"#4-Experiments\" class=\"headerlink\" title=\"4 Experiments\"></a>4 Experiments</h2><h3 id=\"4-1-Performance-on-ImageNet\"><a href=\"#4-1-Performance-on-ImageNet\" class=\"headerlink\" title=\"4.1 Performance on ImageNet\"></a>4.1 Performance on ImageNet</h3><p>He et al. trained ResNet-18 and ResNet-34 on the ImageNet dataset, and also compared them to plain convolutional networks. In Fig. 5, the thin curves denote training error, and the bold ones denote validation error. The figure on the left shows the results of plain convolution networks(in which the 34-layered ones has higher error rates than the 18-layered one), and the figure on the right shows that residual networks perform better than plain ones, while deeper ones perform better than shallow ones.</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/training.png\" width=\"80%\" height=\"80%\" alt=\"Training ResNet on ImageNet\">\n    Fig. 5 Training ResNet on ImageNet<br></div>\n\n<h3 id=\"4-2-Effects-of-Different-Shortcut-Connections\"><a href=\"#4-2-Effects-of-Different-Shortcut-Connections\" class=\"headerlink\" title=\"4.2 Effects of Different Shortcut Connections\"></a>4.2 Effects of Different Shortcut Connections</h3><p>He et al. also tried various types of shortcut connections to replace the identity mapping, and various positions of activation functions / batch normalization. Experiments show that the original identity mapping and full pre-activation yield the best results.</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/shortcut-connections.png\" width=\"50%\" height=\"50%\" alt=\"Different shortcuts of residual units\">\n    Fig. 6 Various shortcuts in residual units<br>    <img src=\"/images/resnet/shortcut-connections-experiment.png\" width=\"70%\" height=\"70%\" alt=\"Classification errors with different shortcuts\">\n    Table. 2 Classification error on CIFAR-10 test set with various shortcut connections in residual units<br>    <img src=\"/images/resnet/activations.png\" width=\"70%\" height=\"50%\" alt=\"Different usages of activation in residual units\">\n    Fig. 7 Various usages of activation in residual units<br>    <img src=\"/images/resnet/activations-experiment.png\" width=\"50%\" height=\"50%\" alt=\"Classification errors with different activations\">\n    Table. 3 Classification error on CIFAR-10 test set with various usages of activation in residual units<br></div>\n\n\n<h2 id=\"5-Conclusion\"><a href=\"#5-Conclusion\" class=\"headerlink\" title=\"5 Conclusion\"></a>5 Conclusion</h2><p>Residual learning can be crowned as &quot;ONE OF THE GREATEST HITS IN DEEP LEARNING FIELDS&quot;. With a simple identity mapping, it solved the degradation problem of deep neural networks. Now that you have learned about the concept of ResNet, why not give it a try and implement your first residual learning model today?</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/resnet-yooo.jpg\" width=\"40%\" height=\"40%\" alt=\"\">\n</div>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><p>[1] <a href=\"http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</a></p>\n<p>[2] <a href=\"https://arxiv.org/pdf/1603.05027.pdf\" target=\"_blank\" rel=\"noopener\">He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks[C]//European Conference on Computer Vision. Springer, Cham, 2016: 630-645.</a></p>\n<p>[3] <a href=\"https://arxiv.org/pdf/1702.08591.pdf\" target=\"_blank\" rel=\"noopener\">Balduzzi D, Frean M, Leary L, et al. The Shattered Gradients Problem: If resnets are the answer, then what is the question?[J]. arXiv preprint arXiv:1702.08591, 2017.</a></p>\n<p>[4] <a href=\"https://arxiv.org/pdf/1502.03167.pdf\" target=\"_blank\" rel=\"noopener\">Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.</a></p>\n","site":{"data":{}},"excerpt":"<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n<h2 id=\"0-Introduction\"><a href=\"#0-Introduction\" class=\"headerlink\" title=\"0 Introduction\"></a>0 Introduction</h2><p>Deep learning researchers have been constructing skyscrapers in recent years. Especially, VGG nets and GoogLeNet have pushed the depths of convolutional networks to the extreme. But questions remain: if time and money aren&#39;t problems, are deeper networks always performing better? Not exactly.</p>\n<p>When residual networks were proposed, researchers around the world was stunned by its depth. &quot;Jesus Christ! Is this a neural network or the Dubai Tower?&quot; But <strong>don&#39;t be afraid!</strong> These networks are deep but the structures are simple. Interestingly, these networks not only defeated all opponents in the classification, detection, localization challenges in ImageNet 2015, but were also the main innovation in the best paper of CVPR2016.</p>\n<div align=\"center\">\n    <img src=\"/images/resnet/network_growth.jpg\" width=\"40%\" height=\"40%\" alt=\"Network Growth\">\n</div>","more":"<h2 id=\"1-The-Crisis-Degradation-of-Deep-Networks\"><a href=\"#1-The-Crisis-Degradation-of-Deep-Networks\" class=\"headerlink\" title=\"1 The Crisis: Degradation of Deep Networks\"></a>1 The Crisis: Degradation of Deep Networks</h2><p>VGG nets proved the beneficial of representation depth of convolutional neural networks, at least within a certain range, to be exact. However, when Kaiming He et al. tried to deepen some plain networks, the training error and test error stopped decreasing after the network reached a certain depth(which is not surprising) and soon degraded. This is not an overfitting problem, because training errors also increased; nor is it a gradient vanishing problem, because there are some techniques(e.g. batch normalization[4]) that ease the pain.</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/downgrade.png\" width=\"60%\" height=\"60%\" alt=\"The Downgrade Problem\">\n    Fig.1 The downgrade problem<br></div>\n\n<p>What seems to be the cause of this degradation? Obviously, deeper neural networks are more difficult to train, but that doesn&#39;t mean deeper neural networks would yield worse results. To explain this problem, Balduzzi et al.[3] identified shattered gradient problem - as depth increases, gradients in standard feedforward networks increasingly resemble white noise. I will write about that later.</p>\n<h2 id=\"2-A-Closer-Look-at-ResNet-The-Residual-Blocks\"><a href=\"#2-A-Closer-Look-at-ResNet-The-Residual-Blocks\" class=\"headerlink\" title=\"2 A Closer Look at ResNet: The Residual Blocks\"></a>2 A Closer Look at ResNet: The Residual Blocks</h2><p>As the old saying goes, &quot;千里之行，始于足下&quot;. Although ResNets are as deep as a thousand layers, they are built with these basic residual blocks(the right part of the figure). </p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/residual_blocks.png\" width=\"50%\" height=\"50%\" alt=\"Comparison between normal weight layers and residual blocks\">\n    Fig.2 Parts of plain networks and a residual block(or residual unit)<br></div>\n\n<h3 id=\"2-1-Skip-Connections\"><a href=\"#2-1-Skip-Connections\" class=\"headerlink\" title=\"2.1 Skip Connections\"></a>2.1 Skip Connections</h3><p>In comparison, basic units of plain network models would look like the one on the left: one ReLU function after a weight layer(usually also with biases), repeated several times. Let&#39;s denote the desired underlying mapping(the ideal mapping) of the two layers as \\(\\mathcal{H}(x)\\), and the real mapping as \\(\\mathcal{F}(x)\\). Clearly, the closer \\(\\mathcal{F}(x)\\) is to \\(\\mathcal{H}(x)\\), the better it fits.</p>\n<p>However, He et al. explicitly let these layers fit a residual mapping instead of the desired underlying mapping. This is implemented with &quot;shortcut connections&quot;, which skip one or more layers, simply performing identity mappings and getting added to the outputs of the stacked weight layers. This way, \\(\\mathcal{F}(x)\\) would not try to fit \\(\\mathcal{H}(x)\\), but \\(\\mathcal{H}(x)-x\\). The whole structure(from the identity mapping branch, to merging the branches by the addition operation) are named &quot;residual blocks&quot;(or &quot;residual units&quot;).</p>\n<p>What&#39;s the point in this? Let&#39;s do a simple analysis. The computation done by the original residual block is: $$y_l=h(x_l)+\\mathcal{F}(x_l,\\mathcal{W}_l),$$ $$x_{l+1}=f(y_l).$$</p>\n<p>Here are the definitions of symbols:<br>\\(x_l\\): input features to the \\(l\\)-th residual block;<br>\\(\\mathcal{W}_{l}={W_{l,k}|_{1\\leq k\\leq K}}\\): a set of weights(and biases) associated with the \\(l\\)-th residual unit. \\(K\\) is the number of layers in this block;\n\\(\\mathcal{F}(x,\\mathcal{W})\\): the residual function, which we talked about earlier. It&#39;s a stack of 2 conv. layers here;\n\\(f(x)\\): the activation function. We are using ReLU here;\n\\(h(x)\\): identity mapping.</p>\n<p>If \\(f(x)\\) is also an identity mapping(as if we&#39;re not using any activation function), the first equation would become:\n$$x_{l+1}=x_l+\\mathcal{F}(x_l,\\mathcal{W}_l)$$</p>\n<p>Therefore, we can define \\(x_L\\) recursively of any layer:\n$$x_L=x_l+\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)$$</p>\n<p>That&#39;s not the end yet! When it comes to the gradients, according to the chain rules of backpropagation, we have a beautiful definition:\n$$\\begin{split}\n\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} &amp; = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\n&amp; = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)\n\\end{split}$$</p>\n<p>What does it mean? It means that the information is directly backpropagated to ANY shallower block. This way, the gradients of a layer never vanish or explode even if the weights are too small or too big.</p>\n<h3 id=\"2-2-Identity-Mappings\"><a href=\"#2-2-Identity-Mappings\" class=\"headerlink\" title=\"2.2 Identity Mappings\"></a>2.2 Identity Mappings</h3><p>It&#39;s important that we use identity mapping here! Just consider doing a simple modification here, for example, \\(h(x)=\\lambda_lx_l\\)(\\(\\lambda_l\\) is a modulating scalar). The definition of \\(x_L\\) and \\(\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}}\\) would become:\n$$x_L=(\\prod_{i=l}^{L-1}\\lambda_i)x_l+\\sum_{i=l}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i,\\mathcal{W}_i)$$\n$$\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}}=\\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big((\\prod_{i=l}^{L-1}\\lambda_i)+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)$$</p>\n<p>For extremely deep neural networks where \\(L\\) is too large, \\(\\prod_{i=l}^{L-1}\\lambda_i\\) could be either too small or too large, causing gradient vanishing or gradient explosion. For \\(h(x)\\) with complex definitions, the gradient could be extremely complicated, thus losing the advantage of the skip connection. Skip connection works best under the condition where the grey channel in Fig. 3 cover no operations (except the addition) and is clean.</p>\n<p>Interestingly, this comfirmed the philosophy of &quot;大道至简&quot; once again.</p>\n<h3 id=\"2-3-Post-activation-or-Pre-activation\"><a href=\"#2-3-Post-activation-or-Pre-activation\" class=\"headerlink\" title=\"2.3 Post-activation or Pre-activation?\"></a>2.3 Post-activation or Pre-activation?</h3><p>Wait a second... &quot;\\(f(x)\\) is also an identity mapping&quot; is just our assumption. The activation function is still there!</p>\n<p>Right. There IS an activation function, but it&#39;s moved to somewhere else.  In fact, the original residual block is still a little bit problematic - the output of one residual block is not always the input of the next, since there is a ReLU activation function after the addition(It did NOT REALLY keep the identity mapping to the next block!). Therefore, in[2], He et al. fixed the residual blocks by changing the order of operations.</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/identity_mapping.png\" width=\"30%\" height=\"30%\" alt=\"New identity mapping\">\n    Fig.3 New identity mapping proposed by He et al.<br></div>\n\n<p>Besides using a simple identity mapping, He et al. also discussed about the position of the activation function and the batch normalization operation. Assuming that we got a special(asymmetric) activation function \\(\\hat f(x)\\), which only affects the path to the next residual unit. Now our definition of \\(x_{x+1}\\) would become:\n$$x_{l+1}=x_l+\\mathcal{F}(\\hat f(x_l),\\mathcal{W}_l)$$</p>\n<p>With \\(x_l\\) still multiplied by 1, information is still fully backpropagated to shallower residual blocks. And the good thing is that using this asymmetric activation function after the addition(partial post-activation) is equivalent to using it beforehand(pre-activation)! This is why He et al. chose to use pre-activation - otherwise it would be necessary to implement that magical activation function \\(\\hat f(x)\\).</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/pre-activation.png\" width=\"80%\" height=\"80%\" alt=\"Asymmetric after-addition activation\">\n    Fig.4 Using asymmetric after-addition activation is equivalent to constructing a pre-activation residual unit<br></div>\n\n<h2 id=\"3-ResNet-Architectures\"><a href=\"#3-ResNet-Architectures\" class=\"headerlink\" title=\"3 ResNet Architectures\"></a>3 ResNet Architectures</h2><p>Here are the ResNet architectures for ImageNet. Building blocks are shown in brackets, with the numbers of blocks stacked. With the first block of every stack(starting from conv3_x), a downsampling is performed. Each column represents one of the residual networks, and the deepest one has 152 weight layers! Since ResNets were proposed, VGG nets - which were officially called &quot;Very Deep Convolutional Networks&quot; - are not relatively deep anymore. Maybe call them &quot;A Little Bit Deep Convolutional Networks&quot;.</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/architectures.png\" width=\"80%\" height=\"80%\" alt=\"ResNet architectures for ImageNet\">\n    Table. 1 ResNet architectures for ImageNet.<br></div>\n\n<h2 id=\"4-Experiments\"><a href=\"#4-Experiments\" class=\"headerlink\" title=\"4 Experiments\"></a>4 Experiments</h2><h3 id=\"4-1-Performance-on-ImageNet\"><a href=\"#4-1-Performance-on-ImageNet\" class=\"headerlink\" title=\"4.1 Performance on ImageNet\"></a>4.1 Performance on ImageNet</h3><p>He et al. trained ResNet-18 and ResNet-34 on the ImageNet dataset, and also compared them to plain convolutional networks. In Fig. 5, the thin curves denote training error, and the bold ones denote validation error. The figure on the left shows the results of plain convolution networks(in which the 34-layered ones has higher error rates than the 18-layered one), and the figure on the right shows that residual networks perform better than plain ones, while deeper ones perform better than shallow ones.</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/training.png\" width=\"80%\" height=\"80%\" alt=\"Training ResNet on ImageNet\">\n    Fig. 5 Training ResNet on ImageNet<br></div>\n\n<h3 id=\"4-2-Effects-of-Different-Shortcut-Connections\"><a href=\"#4-2-Effects-of-Different-Shortcut-Connections\" class=\"headerlink\" title=\"4.2 Effects of Different Shortcut Connections\"></a>4.2 Effects of Different Shortcut Connections</h3><p>He et al. also tried various types of shortcut connections to replace the identity mapping, and various positions of activation functions / batch normalization. Experiments show that the original identity mapping and full pre-activation yield the best results.</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/shortcut-connections.png\" width=\"50%\" height=\"50%\" alt=\"Different shortcuts of residual units\">\n    Fig. 6 Various shortcuts in residual units<br>    <img src=\"/images/resnet/shortcut-connections-experiment.png\" width=\"70%\" height=\"70%\" alt=\"Classification errors with different shortcuts\">\n    Table. 2 Classification error on CIFAR-10 test set with various shortcut connections in residual units<br>    <img src=\"/images/resnet/activations.png\" width=\"70%\" height=\"50%\" alt=\"Different usages of activation in residual units\">\n    Fig. 7 Various usages of activation in residual units<br>    <img src=\"/images/resnet/activations-experiment.png\" width=\"50%\" height=\"50%\" alt=\"Classification errors with different activations\">\n    Table. 3 Classification error on CIFAR-10 test set with various usages of activation in residual units<br></div>\n\n\n<h2 id=\"5-Conclusion\"><a href=\"#5-Conclusion\" class=\"headerlink\" title=\"5 Conclusion\"></a>5 Conclusion</h2><p>Residual learning can be crowned as &quot;ONE OF THE GREATEST HITS IN DEEP LEARNING FIELDS&quot;. With a simple identity mapping, it solved the degradation problem of deep neural networks. Now that you have learned about the concept of ResNet, why not give it a try and implement your first residual learning model today?</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/resnet-yooo.jpg\" width=\"40%\" height=\"40%\" alt=\"\">\n</div>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><p>[1] <a href=\"http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</a></p>\n<p>[2] <a href=\"https://arxiv.org/pdf/1603.05027.pdf\" target=\"_blank\" rel=\"noopener\">He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks[C]//European Conference on Computer Vision. Springer, Cham, 2016: 630-645.</a></p>\n<p>[3] <a href=\"https://arxiv.org/pdf/1702.08591.pdf\" target=\"_blank\" rel=\"noopener\">Balduzzi D, Frean M, Leary L, et al. The Shattered Gradients Problem: If resnets are the answer, then what is the question?[J]. arXiv preprint arXiv:1702.08591, 2017.</a></p>\n<p>[4] <a href=\"https://arxiv.org/pdf/1502.03167.pdf\" target=\"_blank\" rel=\"noopener\">Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.</a></p>"},{"title":"A Review of VGG net - Very Deep Convolutional Neural Networks","date":"2018-04-21T07:15:55.000Z","_content":"\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n## 0 Introduction \nConvolutional neural networks(CNN) have enjoyed great success in computer vision research fields in the past few years. A number of attempts are made based on the original CNN architecture to improve its accuracy and performance. In 2014, Karen Simonyan et al. did an investigation on the effect of depth on CNNs' accuracy in large-scale image recognition (thus also proposing a series of very deep CNNs which are usually called VGG nets). The result confirmed the importance of CNN depth in visual representations.\n\n## 1 Background: VGG net's ancestors\nBefore introducing VGG net, let's take a glance at prior convolutional neural networks. \n\n### 1.1 LeNet: The Origin\nBasic neural network structures(for example, multi-layer perceptron) learn patterns on 1D vectors, which cannot cope with 2D features in images well. In 1986, Lecun et al. proposed a convolution network model called LeNet-5. Its structure is fairly simple: two convolution layers, two subsampling layers and a few fully connected layers. This network was used to solve a number recognition problem. (If you need to learn more about the convolution operation, please refer to Google or *Digital Image Processing* by Rafael C. Gonzalez)\n\n<!-- more -->\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/lenet.png\" width=\"80%\" height=\"60%\" alt=\"LeNet\">\n\n    Fig. 1 Architecture of LeNet\n\n</div>\n\n### 1.2 AlexNet: The Powerful Convolution\nIn 2012, Alex Krizhevsky et al. won the first place in ILSVRC-2012(ImageNet Large-Scale Visual Recognition Challenge 2012) and achieved the highest top-5 error rate of 15.3% with a convolutional network model, while the second-best entry only achieved 26.2%. The network, namely AlexNet, was trained on two GTX580 3GB GPUs in parallel. Since a single GTX580 GPU has only 3GB memory, the maximum size of networks is limited. This model proved the effectiveness of CNNs under complicated circumstances and the power of GPUs. So what if the network can go deeper? Will the top-5 error rate get even lower?\n\n<div align=\"center\" class=\"figure\">\n![AlexNet](/images/vgg/alexnet.png)\n\nFig. 2 Architecture of AlexNet\n</div>\n\n## 2 Main Contributions of VGG Nets\nHere comes our hero - VGG nets. By the way, VGG is not the name of the network, but the name of the authors' group - *Visual Geometry Group*, from Department of Engineering Science, University of Oxford. The networks they proposed were therefore named after the group. The main contributions of VGG nets are: 1. more but smaller convolution filters; 2. great depth of networks.\n\n### 2.1 Stacks of Smaller Convolution Filters\nRather than using relatively large receptive fields in the first convolution layers, Simonyan et al. selected very small 3x3 receptive fields throughout the whole net, which are convolved with the input at every pixel with a stride of 1. As is shown in the figures below, a stack of two 3x3 convolution layers has an effective receptive field of 5x5. We can also conclude that a stack of three 3x3 convolution filters has an effective receptive field of 7x7.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/conv1.png\" width=\"50%\" height=\"50%\" alt=\"Conv5x5\">\n\n    Fig. 3 A convolution layer with one 5x5 conv. filter has a receptive field of 5x5\n\n    <img src=\"/images/vgg/conv2.png\" width=\"60%\" height=\"60%\" alt=\"Conv3x3x2\">\n\n    Fig. 4 A convolution layer stack with two 3x3 conv. filter also has a effective receptive field of 5x5\n\n</div>\n\nNow that we're clear that stacks of small-kernel convolution layers have equal sized receptive fields, why are they the better choice? Well, the first advantage is incorporating more rectification layers instead of a single one, since every convolution layer includes an activation function(usually ReLU). More rectification brings more non-linearity, and more non-linearity makes the decision function more discriminative and fit better. Also, when the receptive field isn't too large, a stack of 3x3 convolution filters have fewer parameters to train. Assuming the number of input and output channels of a convolution layer stack are equal(let's call it C) and the receptive field is 5x5, we have \\\\(2\\*3\\*3\\*C\\*C=18C^2\\\\) instead of \\\\(5\\*5\\*C\\*C=25C^2\\\\) parameters here. Similarly, when the receptive field is 7x7, we have \\\\(3\\*3\\*3\\*C\\*C=27C^2\\\\) instead of \\\\(7\\*7\\*C\\*C=49C^2\\\\). When the field gets even larger? A function with \\\\(O(n)\\\\) complexity only has greater advantage against an \\\\(O(n^2)\\\\) when \\\\(n\\\\) grows.\n\n### 2.2 Deep Dark Fantasy\nCliches time. Just like any blogger mentioning VGG nets would do, here are the network structures proposed by Simonyan et al.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/vggnets.png\" width=\"60%\" height=\"60%\" alt=\"VGG Nets\">\n\n    Table. 1 VGG nets of various depths\n\n</div>\n\nLook at the table column-by-column. Each column(A, A-LRN, B, C, D, E) corresponds to one network structure. As you can see, their networks grew from 11 layers(in net A) to 19 layers(in net E). Each time something is added to the previous net, it would appear bold. Clearly, LRN(Local Response Normalization) didn't work well in this case(actually, A-LRN net performed worse than A, while consuming much more memory and computation time), and was thus removed. \n\nWhat's worth mentioning are the 1x1 convolution layers appearing in network C. This is a way to increase non-linearity(also by introducing activation functions) of the decision function while also keeping the size of the receptive fields unchanged.\n\n## 3 Training & Evaluation\n\nBad initialization could stall learning due to the instability of gradient deep networks. Therefore, the authors first trained the network A, which is shallow enough to be trained with random initialization. Then, the next networks (B to E) are initialized with the pre-trained models, and only weights of the new layers are randomly initialized.\n\nIn spite of the larger number of parameters and the greater depth of our nets compared to AlexNet, the nets required less epochs to converge due to the implicit regularization imposed by greater depth, smaller convolution filter sizes and the pre-initialization of certain layers. They also generalize well to other datasets, achieving state-of-the-art performances. Results of VGG nets in comparison against other models in ILSVRC are shown in the table below.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/VGG-performance-comparison.png\" width=\"70%\" height=\"70%\" alt=\"VGG net results\">\n    \n    Table. 2 VGG net performance, in comparison with the state of the art in ILSVRC classification\n\n</div>\n\nIn conclusion, the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. \n\nYou might ask: Why not even deeper, with more powerful GPUs(the authors used Titan Black), we can absolutely train deeper networks that perform better! Not exactly. Problems arose as the networks get too deep, and this is where ResNet comes in.\n\n## References\n\n[1] [Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.](https://arxiv.org/pdf/1409.1556/)","source":"_posts/vgg.md","raw":"---\ntitle: A Review of VGG net - Very Deep Convolutional Neural Networks\ndate: 2018-04-21 16:15:55\ntags: [Deep Learning, Computer Vision]\n---\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n## 0 Introduction \nConvolutional neural networks(CNN) have enjoyed great success in computer vision research fields in the past few years. A number of attempts are made based on the original CNN architecture to improve its accuracy and performance. In 2014, Karen Simonyan et al. did an investigation on the effect of depth on CNNs' accuracy in large-scale image recognition (thus also proposing a series of very deep CNNs which are usually called VGG nets). The result confirmed the importance of CNN depth in visual representations.\n\n## 1 Background: VGG net's ancestors\nBefore introducing VGG net, let's take a glance at prior convolutional neural networks. \n\n### 1.1 LeNet: The Origin\nBasic neural network structures(for example, multi-layer perceptron) learn patterns on 1D vectors, which cannot cope with 2D features in images well. In 1986, Lecun et al. proposed a convolution network model called LeNet-5. Its structure is fairly simple: two convolution layers, two subsampling layers and a few fully connected layers. This network was used to solve a number recognition problem. (If you need to learn more about the convolution operation, please refer to Google or *Digital Image Processing* by Rafael C. Gonzalez)\n\n<!-- more -->\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/lenet.png\" width=\"80%\" height=\"60%\" alt=\"LeNet\">\n\n    Fig. 1 Architecture of LeNet\n\n</div>\n\n### 1.2 AlexNet: The Powerful Convolution\nIn 2012, Alex Krizhevsky et al. won the first place in ILSVRC-2012(ImageNet Large-Scale Visual Recognition Challenge 2012) and achieved the highest top-5 error rate of 15.3% with a convolutional network model, while the second-best entry only achieved 26.2%. The network, namely AlexNet, was trained on two GTX580 3GB GPUs in parallel. Since a single GTX580 GPU has only 3GB memory, the maximum size of networks is limited. This model proved the effectiveness of CNNs under complicated circumstances and the power of GPUs. So what if the network can go deeper? Will the top-5 error rate get even lower?\n\n<div align=\"center\" class=\"figure\">\n![AlexNet](/images/vgg/alexnet.png)\n\nFig. 2 Architecture of AlexNet\n</div>\n\n## 2 Main Contributions of VGG Nets\nHere comes our hero - VGG nets. By the way, VGG is not the name of the network, but the name of the authors' group - *Visual Geometry Group*, from Department of Engineering Science, University of Oxford. The networks they proposed were therefore named after the group. The main contributions of VGG nets are: 1. more but smaller convolution filters; 2. great depth of networks.\n\n### 2.1 Stacks of Smaller Convolution Filters\nRather than using relatively large receptive fields in the first convolution layers, Simonyan et al. selected very small 3x3 receptive fields throughout the whole net, which are convolved with the input at every pixel with a stride of 1. As is shown in the figures below, a stack of two 3x3 convolution layers has an effective receptive field of 5x5. We can also conclude that a stack of three 3x3 convolution filters has an effective receptive field of 7x7.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/conv1.png\" width=\"50%\" height=\"50%\" alt=\"Conv5x5\">\n\n    Fig. 3 A convolution layer with one 5x5 conv. filter has a receptive field of 5x5\n\n    <img src=\"/images/vgg/conv2.png\" width=\"60%\" height=\"60%\" alt=\"Conv3x3x2\">\n\n    Fig. 4 A convolution layer stack with two 3x3 conv. filter also has a effective receptive field of 5x5\n\n</div>\n\nNow that we're clear that stacks of small-kernel convolution layers have equal sized receptive fields, why are they the better choice? Well, the first advantage is incorporating more rectification layers instead of a single one, since every convolution layer includes an activation function(usually ReLU). More rectification brings more non-linearity, and more non-linearity makes the decision function more discriminative and fit better. Also, when the receptive field isn't too large, a stack of 3x3 convolution filters have fewer parameters to train. Assuming the number of input and output channels of a convolution layer stack are equal(let's call it C) and the receptive field is 5x5, we have \\\\(2\\*3\\*3\\*C\\*C=18C^2\\\\) instead of \\\\(5\\*5\\*C\\*C=25C^2\\\\) parameters here. Similarly, when the receptive field is 7x7, we have \\\\(3\\*3\\*3\\*C\\*C=27C^2\\\\) instead of \\\\(7\\*7\\*C\\*C=49C^2\\\\). When the field gets even larger? A function with \\\\(O(n)\\\\) complexity only has greater advantage against an \\\\(O(n^2)\\\\) when \\\\(n\\\\) grows.\n\n### 2.2 Deep Dark Fantasy\nCliches time. Just like any blogger mentioning VGG nets would do, here are the network structures proposed by Simonyan et al.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/vggnets.png\" width=\"60%\" height=\"60%\" alt=\"VGG Nets\">\n\n    Table. 1 VGG nets of various depths\n\n</div>\n\nLook at the table column-by-column. Each column(A, A-LRN, B, C, D, E) corresponds to one network structure. As you can see, their networks grew from 11 layers(in net A) to 19 layers(in net E). Each time something is added to the previous net, it would appear bold. Clearly, LRN(Local Response Normalization) didn't work well in this case(actually, A-LRN net performed worse than A, while consuming much more memory and computation time), and was thus removed. \n\nWhat's worth mentioning are the 1x1 convolution layers appearing in network C. This is a way to increase non-linearity(also by introducing activation functions) of the decision function while also keeping the size of the receptive fields unchanged.\n\n## 3 Training & Evaluation\n\nBad initialization could stall learning due to the instability of gradient deep networks. Therefore, the authors first trained the network A, which is shallow enough to be trained with random initialization. Then, the next networks (B to E) are initialized with the pre-trained models, and only weights of the new layers are randomly initialized.\n\nIn spite of the larger number of parameters and the greater depth of our nets compared to AlexNet, the nets required less epochs to converge due to the implicit regularization imposed by greater depth, smaller convolution filter sizes and the pre-initialization of certain layers. They also generalize well to other datasets, achieving state-of-the-art performances. Results of VGG nets in comparison against other models in ILSVRC are shown in the table below.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/VGG-performance-comparison.png\" width=\"70%\" height=\"70%\" alt=\"VGG net results\">\n    \n    Table. 2 VGG net performance, in comparison with the state of the art in ILSVRC classification\n\n</div>\n\nIn conclusion, the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. \n\nYou might ask: Why not even deeper, with more powerful GPUs(the authors used Titan Black), we can absolutely train deeper networks that perform better! Not exactly. Problems arose as the networks get too deep, and this is where ResNet comes in.\n\n## References\n\n[1] [Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.](https://arxiv.org/pdf/1409.1556/)","slug":"vgg","published":1,"updated":"2018-06-05T13:38:03.436Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomfkrgl0009zcvjsyi6e65j","content":"<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n<h2 id=\"0-Introduction\"><a href=\"#0-Introduction\" class=\"headerlink\" title=\"0 Introduction\"></a>0 Introduction</h2><p>Convolutional neural networks(CNN) have enjoyed great success in computer vision research fields in the past few years. A number of attempts are made based on the original CNN architecture to improve its accuracy and performance. In 2014, Karen Simonyan et al. did an investigation on the effect of depth on CNNs&#39; accuracy in large-scale image recognition (thus also proposing a series of very deep CNNs which are usually called VGG nets). The result confirmed the importance of CNN depth in visual representations.</p>\n<h2 id=\"1-Background-VGG-net-39-s-ancestors\"><a href=\"#1-Background-VGG-net-39-s-ancestors\" class=\"headerlink\" title=\"1 Background: VGG net&#39;s ancestors\"></a>1 Background: VGG net&#39;s ancestors</h2><p>Before introducing VGG net, let&#39;s take a glance at prior convolutional neural networks. </p>\n<h3 id=\"1-1-LeNet-The-Origin\"><a href=\"#1-1-LeNet-The-Origin\" class=\"headerlink\" title=\"1.1 LeNet: The Origin\"></a>1.1 LeNet: The Origin</h3><p>Basic neural network structures(for example, multi-layer perceptron) learn patterns on 1D vectors, which cannot cope with 2D features in images well. In 1986, Lecun et al. proposed a convolution network model called LeNet-5. Its structure is fairly simple: two convolution layers, two subsampling layers and a few fully connected layers. This network was used to solve a number recognition problem. (If you need to learn more about the convolution operation, please refer to Google or <em>Digital Image Processing</em> by Rafael C. Gonzalez)</p>\n<a id=\"more\"></a>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/lenet.png\" width=\"80%\" height=\"60%\" alt=\"LeNet\"><br>    Fig. 1 Architecture of LeNet<br></div>\n\n<h3 id=\"1-2-AlexNet-The-Powerful-Convolution\"><a href=\"#1-2-AlexNet-The-Powerful-Convolution\" class=\"headerlink\" title=\"1.2 AlexNet: The Powerful Convolution\"></a>1.2 AlexNet: The Powerful Convolution</h3><p>In 2012, Alex Krizhevsky et al. won the first place in ILSVRC-2012(ImageNet Large-Scale Visual Recognition Challenge 2012) and achieved the highest top-5 error rate of 15.3% with a convolutional network model, while the second-best entry only achieved 26.2%. The network, namely AlexNet, was trained on two GTX580 3GB GPUs in parallel. Since a single GTX580 GPU has only 3GB memory, the maximum size of networks is limited. This model proved the effectiveness of CNNs under complicated circumstances and the power of GPUs. So what if the network can go deeper? Will the top-5 error rate get even lower?</p>\n<div align=\"center\" class=\"figure\">\n<img src=\"/images/vgg/alexnet.png\" alt=\"AlexNet\"><br>Fig. 2 Architecture of AlexNet\n</div>\n\n<h2 id=\"2-Main-Contributions-of-VGG-Nets\"><a href=\"#2-Main-Contributions-of-VGG-Nets\" class=\"headerlink\" title=\"2 Main Contributions of VGG Nets\"></a>2 Main Contributions of VGG Nets</h2><p>Here comes our hero - VGG nets. By the way, VGG is not the name of the network, but the name of the authors&#39; group - <em>Visual Geometry Group</em>, from Department of Engineering Science, University of Oxford. The networks they proposed were therefore named after the group. The main contributions of VGG nets are: 1. more but smaller convolution filters; 2. great depth of networks.</p>\n<h3 id=\"2-1-Stacks-of-Smaller-Convolution-Filters\"><a href=\"#2-1-Stacks-of-Smaller-Convolution-Filters\" class=\"headerlink\" title=\"2.1 Stacks of Smaller Convolution Filters\"></a>2.1 Stacks of Smaller Convolution Filters</h3><p>Rather than using relatively large receptive fields in the first convolution layers, Simonyan et al. selected very small 3x3 receptive fields throughout the whole net, which are convolved with the input at every pixel with a stride of 1. As is shown in the figures below, a stack of two 3x3 convolution layers has an effective receptive field of 5x5. We can also conclude that a stack of three 3x3 convolution filters has an effective receptive field of 7x7.</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/conv1.png\" width=\"50%\" height=\"50%\" alt=\"Conv5x5\"><br>    Fig. 3 A convolution layer with one 5x5 conv. filter has a receptive field of 5x5<br>    <img src=\"/images/vgg/conv2.png\" width=\"60%\" height=\"60%\" alt=\"Conv3x3x2\"><br>    Fig. 4 A convolution layer stack with two 3x3 conv. filter also has a effective receptive field of 5x5<br></div>\n\n<p>Now that we&#39;re clear that stacks of small-kernel convolution layers have equal sized receptive fields, why are they the better choice? Well, the first advantage is incorporating more rectification layers instead of a single one, since every convolution layer includes an activation function(usually ReLU). More rectification brings more non-linearity, and more non-linearity makes the decision function more discriminative and fit better. Also, when the receptive field isn&#39;t too large, a stack of 3x3 convolution filters have fewer parameters to train. Assuming the number of input and output channels of a convolution layer stack are equal(let&#39;s call it C) and the receptive field is 5x5, we have \\(2*3*3*C*C=18C^2\\) instead of \\(5*5*C*C=25C^2\\) parameters here. Similarly, when the receptive field is 7x7, we have \\(3*3*3*C*C=27C^2\\) instead of \\(7*7*C*C=49C^2\\). When the field gets even larger? A function with \\(O(n)\\) complexity only has greater advantage against an \\(O(n^2)\\) when \\(n\\) grows.</p>\n<h3 id=\"2-2-Deep-Dark-Fantasy\"><a href=\"#2-2-Deep-Dark-Fantasy\" class=\"headerlink\" title=\"2.2 Deep Dark Fantasy\"></a>2.2 Deep Dark Fantasy</h3><p>Cliches time. Just like any blogger mentioning VGG nets would do, here are the network structures proposed by Simonyan et al.</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/vggnets.png\" width=\"60%\" height=\"60%\" alt=\"VGG Nets\"><br>    Table. 1 VGG nets of various depths<br></div>\n\n<p>Look at the table column-by-column. Each column(A, A-LRN, B, C, D, E) corresponds to one network structure. As you can see, their networks grew from 11 layers(in net A) to 19 layers(in net E). Each time something is added to the previous net, it would appear bold. Clearly, LRN(Local Response Normalization) didn&#39;t work well in this case(actually, A-LRN net performed worse than A, while consuming much more memory and computation time), and was thus removed. </p>\n<p>What&#39;s worth mentioning are the 1x1 convolution layers appearing in network C. This is a way to increase non-linearity(also by introducing activation functions) of the decision function while also keeping the size of the receptive fields unchanged.</p>\n<h2 id=\"3-Training-amp-Evaluation\"><a href=\"#3-Training-amp-Evaluation\" class=\"headerlink\" title=\"3 Training &amp; Evaluation\"></a>3 Training &amp; Evaluation</h2><p>Bad initialization could stall learning due to the instability of gradient deep networks. Therefore, the authors first trained the network A, which is shallow enough to be trained with random initialization. Then, the next networks (B to E) are initialized with the pre-trained models, and only weights of the new layers are randomly initialized.</p>\n<p>In spite of the larger number of parameters and the greater depth of our nets compared to AlexNet, the nets required less epochs to converge due to the implicit regularization imposed by greater depth, smaller convolution filter sizes and the pre-initialization of certain layers. They also generalize well to other datasets, achieving state-of-the-art performances. Results of VGG nets in comparison against other models in ILSVRC are shown in the table below.</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/VGG-performance-comparison.png\" width=\"70%\" height=\"70%\" alt=\"VGG net results\"><br>    Table. 2 VGG net performance, in comparison with the state of the art in ILSVRC classification<br></div>\n\n<p>In conclusion, the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. </p>\n<p>You might ask: Why not even deeper, with more powerful GPUs(the authors used Titan Black), we can absolutely train deeper networks that perform better! Not exactly. Problems arose as the networks get too deep, and this is where ResNet comes in.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><p>[1] <a href=\"https://arxiv.org/pdf/1409.1556/\" target=\"_blank\" rel=\"noopener\">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.</a></p>\n","site":{"data":{}},"excerpt":"<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n<h2 id=\"0-Introduction\"><a href=\"#0-Introduction\" class=\"headerlink\" title=\"0 Introduction\"></a>0 Introduction</h2><p>Convolutional neural networks(CNN) have enjoyed great success in computer vision research fields in the past few years. A number of attempts are made based on the original CNN architecture to improve its accuracy and performance. In 2014, Karen Simonyan et al. did an investigation on the effect of depth on CNNs&#39; accuracy in large-scale image recognition (thus also proposing a series of very deep CNNs which are usually called VGG nets). The result confirmed the importance of CNN depth in visual representations.</p>\n<h2 id=\"1-Background-VGG-net-39-s-ancestors\"><a href=\"#1-Background-VGG-net-39-s-ancestors\" class=\"headerlink\" title=\"1 Background: VGG net&#39;s ancestors\"></a>1 Background: VGG net&#39;s ancestors</h2><p>Before introducing VGG net, let&#39;s take a glance at prior convolutional neural networks. </p>\n<h3 id=\"1-1-LeNet-The-Origin\"><a href=\"#1-1-LeNet-The-Origin\" class=\"headerlink\" title=\"1.1 LeNet: The Origin\"></a>1.1 LeNet: The Origin</h3><p>Basic neural network structures(for example, multi-layer perceptron) learn patterns on 1D vectors, which cannot cope with 2D features in images well. In 1986, Lecun et al. proposed a convolution network model called LeNet-5. Its structure is fairly simple: two convolution layers, two subsampling layers and a few fully connected layers. This network was used to solve a number recognition problem. (If you need to learn more about the convolution operation, please refer to Google or <em>Digital Image Processing</em> by Rafael C. Gonzalez)</p>","more":"<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/lenet.png\" width=\"80%\" height=\"60%\" alt=\"LeNet\"><br>    Fig. 1 Architecture of LeNet<br></div>\n\n<h3 id=\"1-2-AlexNet-The-Powerful-Convolution\"><a href=\"#1-2-AlexNet-The-Powerful-Convolution\" class=\"headerlink\" title=\"1.2 AlexNet: The Powerful Convolution\"></a>1.2 AlexNet: The Powerful Convolution</h3><p>In 2012, Alex Krizhevsky et al. won the first place in ILSVRC-2012(ImageNet Large-Scale Visual Recognition Challenge 2012) and achieved the highest top-5 error rate of 15.3% with a convolutional network model, while the second-best entry only achieved 26.2%. The network, namely AlexNet, was trained on two GTX580 3GB GPUs in parallel. Since a single GTX580 GPU has only 3GB memory, the maximum size of networks is limited. This model proved the effectiveness of CNNs under complicated circumstances and the power of GPUs. So what if the network can go deeper? Will the top-5 error rate get even lower?</p>\n<div align=\"center\" class=\"figure\">\n<img src=\"/images/vgg/alexnet.png\" alt=\"AlexNet\"><br>Fig. 2 Architecture of AlexNet\n</div>\n\n<h2 id=\"2-Main-Contributions-of-VGG-Nets\"><a href=\"#2-Main-Contributions-of-VGG-Nets\" class=\"headerlink\" title=\"2 Main Contributions of VGG Nets\"></a>2 Main Contributions of VGG Nets</h2><p>Here comes our hero - VGG nets. By the way, VGG is not the name of the network, but the name of the authors&#39; group - <em>Visual Geometry Group</em>, from Department of Engineering Science, University of Oxford. The networks they proposed were therefore named after the group. The main contributions of VGG nets are: 1. more but smaller convolution filters; 2. great depth of networks.</p>\n<h3 id=\"2-1-Stacks-of-Smaller-Convolution-Filters\"><a href=\"#2-1-Stacks-of-Smaller-Convolution-Filters\" class=\"headerlink\" title=\"2.1 Stacks of Smaller Convolution Filters\"></a>2.1 Stacks of Smaller Convolution Filters</h3><p>Rather than using relatively large receptive fields in the first convolution layers, Simonyan et al. selected very small 3x3 receptive fields throughout the whole net, which are convolved with the input at every pixel with a stride of 1. As is shown in the figures below, a stack of two 3x3 convolution layers has an effective receptive field of 5x5. We can also conclude that a stack of three 3x3 convolution filters has an effective receptive field of 7x7.</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/conv1.png\" width=\"50%\" height=\"50%\" alt=\"Conv5x5\"><br>    Fig. 3 A convolution layer with one 5x5 conv. filter has a receptive field of 5x5<br>    <img src=\"/images/vgg/conv2.png\" width=\"60%\" height=\"60%\" alt=\"Conv3x3x2\"><br>    Fig. 4 A convolution layer stack with two 3x3 conv. filter also has a effective receptive field of 5x5<br></div>\n\n<p>Now that we&#39;re clear that stacks of small-kernel convolution layers have equal sized receptive fields, why are they the better choice? Well, the first advantage is incorporating more rectification layers instead of a single one, since every convolution layer includes an activation function(usually ReLU). More rectification brings more non-linearity, and more non-linearity makes the decision function more discriminative and fit better. Also, when the receptive field isn&#39;t too large, a stack of 3x3 convolution filters have fewer parameters to train. Assuming the number of input and output channels of a convolution layer stack are equal(let&#39;s call it C) and the receptive field is 5x5, we have \\(2*3*3*C*C=18C^2\\) instead of \\(5*5*C*C=25C^2\\) parameters here. Similarly, when the receptive field is 7x7, we have \\(3*3*3*C*C=27C^2\\) instead of \\(7*7*C*C=49C^2\\). When the field gets even larger? A function with \\(O(n)\\) complexity only has greater advantage against an \\(O(n^2)\\) when \\(n\\) grows.</p>\n<h3 id=\"2-2-Deep-Dark-Fantasy\"><a href=\"#2-2-Deep-Dark-Fantasy\" class=\"headerlink\" title=\"2.2 Deep Dark Fantasy\"></a>2.2 Deep Dark Fantasy</h3><p>Cliches time. Just like any blogger mentioning VGG nets would do, here are the network structures proposed by Simonyan et al.</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/vggnets.png\" width=\"60%\" height=\"60%\" alt=\"VGG Nets\"><br>    Table. 1 VGG nets of various depths<br></div>\n\n<p>Look at the table column-by-column. Each column(A, A-LRN, B, C, D, E) corresponds to one network structure. As you can see, their networks grew from 11 layers(in net A) to 19 layers(in net E). Each time something is added to the previous net, it would appear bold. Clearly, LRN(Local Response Normalization) didn&#39;t work well in this case(actually, A-LRN net performed worse than A, while consuming much more memory and computation time), and was thus removed. </p>\n<p>What&#39;s worth mentioning are the 1x1 convolution layers appearing in network C. This is a way to increase non-linearity(also by introducing activation functions) of the decision function while also keeping the size of the receptive fields unchanged.</p>\n<h2 id=\"3-Training-amp-Evaluation\"><a href=\"#3-Training-amp-Evaluation\" class=\"headerlink\" title=\"3 Training &amp; Evaluation\"></a>3 Training &amp; Evaluation</h2><p>Bad initialization could stall learning due to the instability of gradient deep networks. Therefore, the authors first trained the network A, which is shallow enough to be trained with random initialization. Then, the next networks (B to E) are initialized with the pre-trained models, and only weights of the new layers are randomly initialized.</p>\n<p>In spite of the larger number of parameters and the greater depth of our nets compared to AlexNet, the nets required less epochs to converge due to the implicit regularization imposed by greater depth, smaller convolution filter sizes and the pre-initialization of certain layers. They also generalize well to other datasets, achieving state-of-the-art performances. Results of VGG nets in comparison against other models in ILSVRC are shown in the table below.</p>\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/VGG-performance-comparison.png\" width=\"70%\" height=\"70%\" alt=\"VGG net results\"><br>    Table. 2 VGG net performance, in comparison with the state of the art in ILSVRC classification<br></div>\n\n<p>In conclusion, the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. </p>\n<p>You might ask: Why not even deeper, with more powerful GPUs(the authors used Titan Black), we can absolutely train deeper networks that perform better! Not exactly. Problems arose as the networks get too deep, and this is where ResNet comes in.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><p>[1] <a href=\"https://arxiv.org/pdf/1409.1556/\" target=\"_blank\" rel=\"noopener\">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.</a></p>"},{"title":"Hello New World!","date":"2018-07-26T01:12:52.000Z","_content":"\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/jquery.fancybox.min.css\"/>\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/images//photo-waterfall.css\"/>\n<script src=\"https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js\"></script>\n<script src=\"https://unpkg.com/minigrid@3.1.1/dist/minigrid.min.js\"></script>\n<script src=\"/js/jquery.fancybox.min.js\"></script>\n<script src=\"/js/photo-waterfall.js\"></script>\n\n\nHere are some photos that I took during my trip to Higashi-Osaka, Nara and Kyoto.\n\n<!-- more -->\n\n<div class=\"ImageGrid\"></div>\n\n<script>\nphoto_waterfall.init('/images/hello-osaka/');\n</script>\n<script src=\"/js/photo-waterfall-carousel.js\"></script>","source":"_posts/hello-osaka.md","raw":"---\ntitle: Hello New World!\ntags: Travel Gallery\ndate: 2018-07-26 10:12:52\n---\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/jquery.fancybox.min.css\"/>\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/images//photo-waterfall.css\"/>\n<script src=\"https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js\"></script>\n<script src=\"https://unpkg.com/minigrid@3.1.1/dist/minigrid.min.js\"></script>\n<script src=\"/js/jquery.fancybox.min.js\"></script>\n<script src=\"/js/photo-waterfall.js\"></script>\n\n\nHere are some photos that I took during my trip to Higashi-Osaka, Nara and Kyoto.\n\n<!-- more -->\n\n<div class=\"ImageGrid\"></div>\n\n<script>\nphoto_waterfall.init('/images/hello-osaka/');\n</script>\n<script src=\"/js/photo-waterfall-carousel.js\"></script>","slug":"hello-osaka","published":1,"updated":"2018-07-26T13:22:21.650Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomfkrgv000bzcvjmiqupecb","content":"<p><link rel=\"stylesheet\" type=\"text/css\" href=\"/css/jquery.fancybox.min.css\"></p>\n<p><link rel=\"stylesheet\" type=\"text/css\" href=\"/css/images//photo-waterfall.css\"></p>\n<script src=\"https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js\"></script>\n<script src=\"https://unpkg.com/minigrid@3.1.1/dist/minigrid.min.js\"></script>\n<script src=\"/js/jquery.fancybox.min.js\"></script>\n<script src=\"/js/photo-waterfall.js\"></script>\n\n\n<p>Here are some photos that I took during my trip to Higashi-Osaka, Nara and Kyoto.</p>\n<a id=\"more\"></a>\n<div class=\"ImageGrid\"></div>\n\n<script>\nphoto_waterfall.init('/images/hello-osaka/');\n</script>\n<script src=\"/js/photo-waterfall-carousel.js\"></script>","site":{"data":{}},"excerpt":"<p><link rel=\"stylesheet\" type=\"text/css\" href=\"/css/jquery.fancybox.min.css\"></p>\n<p><link rel=\"stylesheet\" type=\"text/css\" href=\"/css/images//photo-waterfall.css\"></p>\n<script src=\"https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js\"></script>\n<script src=\"https://unpkg.com/minigrid@3.1.1/dist/minigrid.min.js\"></script>\n<script src=\"/js/jquery.fancybox.min.js\"></script>\n<script src=\"/js/photo-waterfall.js\"></script>\n\n\n<p>Here are some photos that I took during my trip to Higashi-Osaka, Nara and Kyoto.</p>","more":"<div class=\"ImageGrid\"></div>\n\n<script>\nphoto_waterfall.init('/images/hello-osaka/');\n</script>\n<script src=\"/js/photo-waterfall-carousel.js\"></script>"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cjomfkrf70000zcvjyam020ez","tag_id":"cjomfkrfr0003zcvjhnyaj3sm","_id":"cjomfkrgv000czcvjbx3v6jhu"},{"post_id":"cjomfkrf70000zcvjyam020ez","tag_id":"cjomfkrgb0007zcvjay4meqfv","_id":"cjomfkrh3000dzcvj830k4dk7"},{"post_id":"cjomfkrfh0002zcvj3iyyw0rm","tag_id":"cjomfkrfr0003zcvjhnyaj3sm","_id":"cjomfkrh5000gzcvjo2l11f11"},{"post_id":"cjomfkrfh0002zcvj3iyyw0rm","tag_id":"cjomfkrgb0007zcvjay4meqfv","_id":"cjomfkrhd000hzcvj9vrt8p7v"},{"post_id":"cjomfkrfr0004zcvjzn71shs6","tag_id":"cjomfkrh5000fzcvjrf458vfv","_id":"cjomfkrhf000jzcvjog2dp37i"},{"post_id":"cjomfkrg90005zcvjntk3wg2e","tag_id":"cjomfkrhd000izcvjivakwn1h","_id":"cjomfkrhf000lzcvjft03qbmm"},{"post_id":"cjomfkrgb0006zcvjwa88wl1q","tag_id":"cjomfkrhd000izcvjivakwn1h","_id":"cjomfkrhf000nzcvjtsljh6g1"},{"post_id":"cjomfkrgj0008zcvj09y2wnzy","tag_id":"cjomfkrfr0003zcvjhnyaj3sm","_id":"cjomfkrhp000pzcvjy436bua5"},{"post_id":"cjomfkrgj0008zcvj09y2wnzy","tag_id":"cjomfkrhf000mzcvjxo6bd9kb","_id":"cjomfkrhp000qzcvj5eo15c84"},{"post_id":"cjomfkrgl0009zcvjsyi6e65j","tag_id":"cjomfkrfr0003zcvjhnyaj3sm","_id":"cjomfkrhp000szcvj1mkmyi0a"},{"post_id":"cjomfkrgl0009zcvjsyi6e65j","tag_id":"cjomfkrhf000mzcvjxo6bd9kb","_id":"cjomfkrhp000tzcvjcd9i1t02"},{"post_id":"cjomfkrgv000bzcvjmiqupecb","tag_id":"cjomfkrhp000rzcvjxan5k9gp","_id":"cjomfkrhp000uzcvjcs215cvl"}],"Tag":[{"name":"Deep Learning","_id":"cjomfkrfr0003zcvjhnyaj3sm"},{"name":"DIY","_id":"cjomfkrgb0007zcvjay4meqfv"},{"name":"Other","_id":"cjomfkrh5000fzcvjrf458vfv"},{"name":"Blogging","_id":"cjomfkrhd000izcvjivakwn1h"},{"name":"Computer Vision","_id":"cjomfkrhf000mzcvjxo6bd9kb"},{"name":"Travel Gallery","_id":"cjomfkrhp000rzcvjxan5k9gp"}]}}