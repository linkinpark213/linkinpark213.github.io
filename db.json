{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/files/tensor_chain.py","path":"files/tensor_chain.py","modified":1,"renderable":0},{"_id":"themes/myconcise/source/CNAME","path":"CNAME","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/mixin.css","path":"css/mixin.css","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/vars.css","path":"css/vars.css","modified":1,"renderable":1},{"_id":"source/images/densepose-ms/facebook.jpg","path":"images/densepose-ms/facebook.jpg","modified":1,"renderable":0},{"_id":"source/images/resnet/identity_mapping.png","path":"images/resnet/identity_mapping.png","modified":1,"renderable":0},{"_id":"source/images/resnet/downgrade.png","path":"images/resnet/downgrade.png","modified":1,"renderable":0},{"_id":"source/images/smartypants/apostrophe.png","path":"images/smartypants/apostrophe.png","modified":1,"renderable":0},{"_id":"source/images/tftutorial/oyo.gif","path":"images/tftutorial/oyo.gif","modified":1,"renderable":0},{"_id":"source/images/tftutorial/op.png","path":"images/tftutorial/op.png","modified":1,"renderable":0},{"_id":"source/images/tftutorial/variable.png","path":"images/tftutorial/variable.png","modified":1,"renderable":0},{"_id":"themes/myconcise/source/css/app.less","path":"css/app.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/archive.less","path":"css/archive.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/article.less","path":"css/article.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/category.less","path":"css/category.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/footer.less","path":"css/footer.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/aside.less","path":"css/aside.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/icons.less","path":"css/icons.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/jquery.fancybox.min.css","path":"css/jquery.fancybox.min.css","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/markdown.less","path":"css/markdown.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/mixin.less","path":"css/mixin.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/monokai-sublime.less","path":"css/monokai-sublime.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/gitalk.less","path":"css/gitalk.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/nav.less","path":"css/nav.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/page.less","path":"css/page.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/style.less","path":"css/style.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/tags.less","path":"css/tags.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/reset.less","path":"css/reset.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/vars.less","path":"css/vars.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/widget.less","path":"css/widget.less","modified":1,"renderable":1},{"_id":"themes/myconcise/source/fonts/icomoon.eot","path":"fonts/icomoon.eot","modified":1,"renderable":1},{"_id":"themes/myconcise/source/fonts/icomoon.ttf","path":"fonts/icomoon.ttf","modified":1,"renderable":1},{"_id":"themes/myconcise/source/fonts/icomoon.svg","path":"fonts/icomoon.svg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/fonts/icomoon.woff","path":"fonts/icomoon.woff","modified":1,"renderable":1},{"_id":"themes/myconcise/source/js/photo-waterfall-carousel.js","path":"js/photo-waterfall-carousel.js","modified":1,"renderable":1},{"_id":"themes/myconcise/source/js/jquery.lazyload.js","path":"js/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/myconcise/source/js/photo-waterfall.js","path":"js/photo-waterfall.js","modified":1,"renderable":1},{"_id":"themes/myconcise/source/js/script.js","path":"js/script.js","modified":1,"renderable":1},{"_id":"themes/myconcise/source/js/scrollspy.min.js","path":"js/scrollspy.min.js","modified":1,"renderable":1},{"_id":"themes/myconcise/source/js/jquery.fancybox.min.js","path":"js/jquery.fancybox.min.js","modified":1,"renderable":1},{"_id":"source/images/resnet/activations-experiment.png","path":"images/resnet/activations-experiment.png","modified":1,"renderable":0},{"_id":"source/images/resnet/residual_blocks.png","path":"images/resnet/residual_blocks.png","modified":1,"renderable":0},{"_id":"source/images/tftutorial/loss.png","path":"images/tftutorial/loss.png","modified":1,"renderable":0},{"_id":"source/images/tftutorial/resnet56.png","path":"images/tftutorial/resnet56.png","modified":1,"renderable":0},{"_id":"source/images/vgg/conv2.png","path":"images/vgg/conv2.png","modified":1,"renderable":0},{"_id":"source/images/vgg/conv1.png","path":"images/vgg/conv1.png","modified":1,"renderable":0},{"_id":"themes/myconcise/source/images/favicon.png","path":"images/favicon.png","modified":1,"renderable":1},{"_id":"source/images/resnet/activations.png","path":"images/resnet/activations.png","modified":1,"renderable":0},{"_id":"source/images/resnet/pre-activation.png","path":"images/resnet/pre-activation.png","modified":1,"renderable":0},{"_id":"source/images/resnet/resnet-yooo.jpg","path":"images/resnet/resnet-yooo.jpg","modified":1,"renderable":0},{"_id":"source/images/resnet/shortcut-connections.png","path":"images/resnet/shortcut-connections.png","modified":1,"renderable":0},{"_id":"source/images/resnet/training.png","path":"images/resnet/training.png","modified":1,"renderable":0},{"_id":"source/images/vgg/alexnet.png","path":"images/vgg/alexnet.png","modified":1,"renderable":0},{"_id":"source/images/vgg/lenet.png","path":"images/vgg/lenet.png","modified":1,"renderable":0},{"_id":"source/images/resnet/shortcut-connections-experiment.png","path":"images/resnet/shortcut-connections-experiment.png","modified":1,"renderable":0},{"_id":"themes/myconcise/source/css/fonts/FontAwesome.otf","path":"css/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/fonts/fontawesome-webfont.eot","path":"css/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/fonts/fontawesome-webfont.woff","path":"css/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/images/photo-waterfall.css","path":"css/images/photo-waterfall.css","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/output.json","path":"images/hello-osaka/output.json","modified":1,"renderable":1},{"_id":"source/images/avatar.png","path":"images/avatar.png","modified":1,"renderable":0},{"_id":"themes/myconcise/source/css/fonts/fontawesome-webfont.ttf","path":"css/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"source/images/resnet/network_growth.jpg","path":"images/resnet/network_growth.jpg","modified":1,"renderable":0},{"_id":"source/images/vgg/vggnets.png","path":"images/vgg/vggnets.png","modified":1,"renderable":0},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-1.jpg","path":"images/hello-osaka/thumbnail/osaka-1.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-13.jpg","path":"images/hello-osaka/thumbnail/osaka-13.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-14.jpg","path":"images/hello-osaka/thumbnail/osaka-14.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-15.jpg","path":"images/hello-osaka/thumbnail/osaka-15.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-17.jpg","path":"images/hello-osaka/thumbnail/osaka-17.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-2.jpg","path":"images/hello-osaka/thumbnail/osaka-2.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-5.jpg","path":"images/hello-osaka/thumbnail/osaka-5.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-7.jpg","path":"images/hello-osaka/thumbnail/osaka-7.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-8.jpg","path":"images/hello-osaka/thumbnail/osaka-8.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-9.jpg","path":"images/hello-osaka/thumbnail/osaka-9.jpg","modified":1,"renderable":1},{"_id":"source/images/hello-osaka/osaka-20.jpg","path":"images/hello-osaka/osaka-20.jpg","modified":1,"renderable":0},{"_id":"source/images/resnet/architectures.png","path":"images/resnet/architectures.png","modified":1,"renderable":0},{"_id":"source/images/vgg/VGG-performance-comparison.png","path":"images/vgg/VGG-performance-comparison.png","modified":1,"renderable":0},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-10.jpg","path":"images/hello-osaka/thumbnail/osaka-10.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-12.jpg","path":"images/hello-osaka/thumbnail/osaka-12.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-16.jpg","path":"images/hello-osaka/thumbnail/osaka-16.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-4.jpg","path":"images/hello-osaka/thumbnail/osaka-4.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/css/fonts/fontawesome-webfont.svg","path":"css/fonts/fontawesome-webfont.svg","modified":1,"renderable":1},{"_id":"source/images/tftutorial/train.png","path":"images/tftutorial/train.png","modified":1,"renderable":0},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-20.jpg","path":"images/hello-osaka/thumbnail/osaka-20.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-11.jpg","path":"images/hello-osaka/thumbnail/osaka-11.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-18.jpg","path":"images/hello-osaka/thumbnail/osaka-18.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-6.jpg","path":"images/hello-osaka/thumbnail/osaka-6.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-3.jpg","path":"images/hello-osaka/thumbnail/osaka-3.jpg","modified":1,"renderable":1},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-19.jpg","path":"images/hello-osaka/thumbnail/osaka-19.jpg","modified":1,"renderable":1},{"_id":"source/images/hello-osaka/osaka-19.jpg","path":"images/hello-osaka/osaka-19.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-6.jpg","path":"images/hello-osaka/osaka-6.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-15.jpg","path":"images/hello-osaka/osaka-15.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-9.jpg","path":"images/hello-osaka/osaka-9.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-13.jpg","path":"images/hello-osaka/osaka-13.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-5.jpg","path":"images/hello-osaka/osaka-5.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-1.jpg","path":"images/hello-osaka/osaka-1.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-12.jpg","path":"images/hello-osaka/osaka-12.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-8.jpg","path":"images/hello-osaka/osaka-8.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-2.jpg","path":"images/hello-osaka/osaka-2.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-18.jpg","path":"images/hello-osaka/osaka-18.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-11.jpg","path":"images/hello-osaka/osaka-11.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-14.jpg","path":"images/hello-osaka/osaka-14.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-3.jpg","path":"images/hello-osaka/osaka-3.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-17.jpg","path":"images/hello-osaka/osaka-17.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-7.jpg","path":"images/hello-osaka/osaka-7.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-10.jpg","path":"images/hello-osaka/osaka-10.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-4.jpg","path":"images/hello-osaka/osaka-4.jpg","modified":1,"renderable":0},{"_id":"source/images/hello-osaka/osaka-16.jpg","path":"images/hello-osaka/osaka-16.jpg","modified":1,"renderable":0}],"Cache":[{"_id":"themes/myconcise/_config.yml","hash":"71d27bf48e2d01f9787a05612ee59b9f282546ef","modified":1532572106000},{"_id":"source/_drafts/densepose-minesweeping.md","hash":"ea3a31da34ee5193b34a6949ca420df163fb103c","modified":1542527947053},{"_id":"source/_posts/densepose-minesweeping.md","hash":"6c652c9a7fa65ee63c6fb3606aacaf54fda24932","modified":1542529111951},{"_id":"source/_posts/diy-resnet.md","hash":"ffe1b86a91514fea272f010d5818636aa4ee7bbb","modified":1542519885857},{"_id":"source/_posts/hello-osaka.md","hash":"a956d7431b85051db62aaba2df3ebebdab102cd7","modified":1542519885857},{"_id":"source/_posts/linkinpark213-day.md","hash":"61fa6910ab0739e45583b1e60885a22861d7b236","modified":1542521081608},{"_id":"source/_posts/mathjax.md","hash":"981d4724a1bb680351efa3c26431bca2820524f1","modified":1542519885857},{"_id":"source/_posts/resnet.md","hash":"0a36e1fbf925d7ca2897acd80b1fd3fb162a6071","modified":1542519885857},{"_id":"source/_posts/smartypants.md","hash":"200e0166832cf8d7279ff8be42ca2f7fe3cf82ea","modified":1542519885857},{"_id":"source/_posts/vgg.md","hash":"3f98cd1289918c8658aaa7382e29a03b27bc1426","modified":1542519885857},{"_id":"source/about/index.md","hash":"3684f408ee64e1d0d8fce8d4438193153a2f54c6","modified":1542519885857},{"_id":"source/files/tensor_chain.py","hash":"33a8320704e1a0b275b28f23a8f08a22f83c2422","modified":1542519885857},{"_id":"themes/myconcise/languages/default.yml","hash":"aa22dc162fc76a36bb62860096e057a52d79196e","modified":1524726349000},{"_id":"themes/myconcise/languages/zh-CN.yml","hash":"50a8c9b15be3e5512dadf2583a1a38cf94578bb1","modified":1524726625000},{"_id":"themes/myconcise/languages/ja-jp.yml","hash":"ad8d3d27b4dba679b46990195af5886fcf1fb734","modified":1524726332000},{"_id":"themes/myconcise/layout/archive.ejs","hash":"3e4d0c3fa27aacb7ecf7c7d93b3c603087caf3a8","modified":1518505949000},{"_id":"themes/myconcise/layout/categories.ejs","hash":"453ec20e5985487dd2eaa0a32003e2ec4495c391","modified":1518505949000},{"_id":"themes/myconcise/layout/category.ejs","hash":"e29542322fe798281a40eed476468aa8afba7d50","modified":1518505949000},{"_id":"themes/myconcise/layout/index.ejs","hash":"e29542322fe798281a40eed476468aa8afba7d50","modified":1518505949000},{"_id":"themes/myconcise/layout/page.ejs","hash":"f87550d14cc73fadfb95e8e8040d97640cfa4923","modified":1518612137000},{"_id":"themes/myconcise/layout/post.ejs","hash":"df2a2fe6a17bec6a99c6494148517e5610e76e93","modified":1518612156000},{"_id":"themes/myconcise/layout/tag.ejs","hash":"e29542322fe798281a40eed476468aa8afba7d50","modified":1518505949000},{"_id":"themes/myconcise/source/CNAME","hash":"87ad9ee278279eb9ce26622039fc9a96b4b65c81","modified":1521558121000},{"_id":"themes/myconcise/layout/layout.ejs","hash":"20f3e6e680b21af31b734af40ec6eb2b71d47dfd","modified":1518519237000},{"_id":"themes/myconcise/source/css/mixin.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1542521239778},{"_id":"themes/myconcise/source/css/vars.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1542521239778},{"_id":"source/images/densepose-ms/20180701130844_76576.jpg~","hash":"0874294738793d0424bff09b6d9315a28bd37210","modified":1542527028719},{"_id":"source/images/densepose-ms/facebook.jpg","hash":"d14c826388f534453b2f81cb6a714cdd22cd206e","modified":1542528180828},{"_id":"source/images/resnet/identity_mapping.png","hash":"ff7e65b6867a3975b0fb85a61261282bf0da233f","modified":1542519886105},{"_id":"source/images/resnet/downgrade.png","hash":"75143520d0b8abc3d071c6bd43341081d5d82072","modified":1542519886105},{"_id":"source/images/smartypants/apostrophe.png","hash":"97219c36fec83eac54499d2211c47f130732d2a2","modified":1542519886109},{"_id":"source/images/tftutorial/oyo.gif","hash":"5d1a50a925b688315425c7f45f0b568aaed9fea2","modified":1542519886109},{"_id":"source/images/tftutorial/op.png","hash":"09bbaeb37b975d1e0411051f80fb3ac61af01dbe","modified":1542519886109},{"_id":"source/images/tftutorial/variable.png","hash":"09bbaeb37b975d1e0411051f80fb3ac61af01dbe","modified":1542519886113},{"_id":"themes/myconcise/layout/_partial/archive.ejs","hash":"4f3cdb19da78ac7d42d37fb4d152988f99ea74f4","modified":1518505949000},{"_id":"themes/myconcise/layout/_partial/article.ejs","hash":"a46443b099972a06f4a44566f2b04ea4ca44d0b7","modified":1524727653000},{"_id":"themes/myconcise/layout/_partial/articles.ejs","hash":"3ca4869df35f85c39997a99ea32b3366e02de34a","modified":1518612145000},{"_id":"themes/myconcise/layout/_partial/comments.ejs","hash":"30fbd53f1caa373c09f79be1c8194716cc4ecb38","modified":1525015642000},{"_id":"themes/myconcise/layout/_partial/aside.ejs","hash":"69ecfed5f8a877bf7b3eec834d4244b8a38c6e29","modified":1521526722000},{"_id":"themes/myconcise/layout/_partial/footer.ejs","hash":"13d506ac2d63bd649bd5a0c732e36c9f4d2d96f6","modified":1518505949000},{"_id":"themes/myconcise/layout/_partial/header.ejs","hash":"4b3a9262654ef3bc91fd9067e0db20866bb18da8","modified":1518505949000},{"_id":"themes/myconcise/layout/_partial/nav.ejs","hash":"bd79439750262f5ee910e2b7b99060570364d7b1","modified":1524475708000},{"_id":"themes/myconcise/layout/_widget/category.ejs","hash":"866790acc13fed44b7ef74c3e19c300a3d6180d8","modified":1518505949000},{"_id":"themes/myconcise/layout/_widget/archive.ejs","hash":"e1bdddeaada31c00aa67278fcc50ad845ecf1722","modified":1518505949000},{"_id":"themes/myconcise/layout/_widget/recent_posts.ejs","hash":"16800f85ffb036d2644a26e02facd61acb3706e9","modified":1518505949000},{"_id":"themes/myconcise/layout/_widget/tag.ejs","hash":"c000ec9f1479f74ec8d0e9378ba61f23f3778431","modified":1518505949000},{"_id":"themes/myconcise/layout/_widget/tagcloud.ejs","hash":"7259c179aa0c41c02e467ad892292e90430aaabc","modified":1518505949000},{"_id":"themes/myconcise/source/css/app.less","hash":"048c0c1bfc8034d6f4a824d6c06fe0bfdb5c09d9","modified":1518505949000},{"_id":"themes/myconcise/source/css/archive.less","hash":"acf97e6d82c7bce8591cde5730fcecdc70c9c6c6","modified":1518505949000},{"_id":"themes/myconcise/source/css/article.less","hash":"4426fcf2b4143413298b18309cd6be9e683e59d9","modified":1524730845000},{"_id":"themes/myconcise/source/css/category.less","hash":"9288664fef128c5527078d6c742611c370de08bd","modified":1518505949000},{"_id":"themes/myconcise/source/css/footer.less","hash":"9798524e6235b96cc0235c77c436c38c41e9b6f7","modified":1518505949000},{"_id":"themes/myconcise/source/css/aside.less","hash":"7c1a25e887ba25174b3854384cce97d0e70f089e","modified":1518505949000},{"_id":"themes/myconcise/source/css/icons.less","hash":"2be8a47cf73ea97e3ca5918f20d97756d340e46d","modified":1518509528000},{"_id":"themes/myconcise/source/css/jquery.fancybox.min.css","hash":"fe69cca0e7d419533924228c6bd434bc57ab542f","modified":1542521239778},{"_id":"themes/myconcise/source/css/markdown.less","hash":"56adc0573ebe6708fae370ade0411124df317c20","modified":1524387564000},{"_id":"themes/myconcise/source/css/mixin.less","hash":"d3d7673330e60d0294ac8a806ef697fa89e906f0","modified":1518505949000},{"_id":"themes/myconcise/source/css/monokai-sublime.less","hash":"9b36b183a224d784dcb4655be909368cd68ba1f0","modified":1518505949000},{"_id":"themes/myconcise/source/css/gitalk.less","hash":"e4e0d12e3f08ad15ee68de222045725d00f1036e","modified":1525012797000},{"_id":"themes/myconcise/source/css/nav.less","hash":"4fa74995bbaf8bc314f4c1f05de87e1a0a34620f","modified":1518505949000},{"_id":"themes/myconcise/source/css/page.less","hash":"7610d789fe6f3a43e321682b38159dd405ec299f","modified":1524387599000},{"_id":"themes/myconcise/source/css/style.less","hash":"43146606459315a9d27d04c88a1e2d00194d080a","modified":1525012143000},{"_id":"themes/myconcise/source/css/tags.less","hash":"92edb713370078cf4757e8546555f91ab7632614","modified":1518505949000},{"_id":"themes/myconcise/source/css/reset.less","hash":"d1535fa425fc1b0aab25397002fb6c1c51538326","modified":1524730830000},{"_id":"themes/myconcise/source/css/vars.less","hash":"336ac69d2666581716602f3b353c6ec34f6df623","modified":1518505949000},{"_id":"themes/myconcise/source/css/widget.less","hash":"8373c2246dfb0261278003f1985a53707494e0b9","modified":1518505949000},{"_id":"themes/myconcise/source/fonts/icomoon.eot","hash":"8300564edcd12262c6b8b97ba225bc00b72f5984","modified":1518505949000},{"_id":"themes/myconcise/source/fonts/icomoon.ttf","hash":"6b577c657ccb5de7c321e51942a52922a4e95ec1","modified":1518505949000},{"_id":"themes/myconcise/source/fonts/icomoon.svg","hash":"821f98d8cdec4dbef208ed6f66c54e7b718fdd6a","modified":1518505949000},{"_id":"themes/myconcise/source/fonts/icomoon.woff","hash":"5a3b86fa1122f78d9b285fbd390bf142721c51ed","modified":1518505949000},{"_id":"themes/myconcise/source/js/photo-waterfall-carousel.js","hash":"204e801154d416e86adf8f9e1289947acf85cc6b","modified":1542521240058},{"_id":"themes/myconcise/source/js/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1542521240058},{"_id":"themes/myconcise/source/js/photo-waterfall.js","hash":"6f66bc0c0f1e00687b431f2f6ccdcdeec367b7c5","modified":1542521240058},{"_id":"themes/myconcise/source/js/script.js","hash":"2876e0b19ce557fca38d7c6f49ca55922ab666a1","modified":1542521240058},{"_id":"themes/myconcise/source/js/scrollspy.min.js","hash":"076f7db44c3a05147144f736cae159baf1612f72","modified":1542521240058},{"_id":"themes/myconcise/source/js/jquery.fancybox.min.js","hash":"5f2927d035f3b5e99a99ca0652584bff8aa49850","modified":1542521240058},{"_id":"source/images/densepose-ms/facebook.jpg~","hash":"107cf3c9d4a11622d61b41c4beb78d5b5c95a11c","modified":1542528180828},{"_id":"source/images/resnet/activations-experiment.png","hash":"2d11a22f89779c36f347c5fd9946dcd7df5b2c17","modified":1542519886101},{"_id":"source/images/resnet/residual_blocks.png","hash":"8ba126be7c0bedefa3980c24098ba71190ee50d0","modified":1542519886105},{"_id":"source/images/tftutorial/loss.png","hash":"a459ee1446ae988715e8656ddec27a3bfbb37eb1","modified":1542519886109},{"_id":"source/images/tftutorial/resnet56.png","hash":"800358dc64b3819d9fd6784df7630c427bd9177e","modified":1542519886109},{"_id":"source/images/vgg/conv2.png","hash":"6c28de6b3b875deca0c2bf82e4b977d3b332d8a0","modified":1542519886117},{"_id":"source/images/vgg/conv1.png","hash":"0f400ee8616672b7c5404140a6afcb41bb843bb0","modified":1542519886117},{"_id":"themes/myconcise/source/images/favicon.png","hash":"e1c0fb88ab6ed405b64009b6f3ce1d9a38f0e5fc","modified":1518511578000},{"_id":"source/images/resnet/activations.png","hash":"484fd30fac3a5ccb641517618e89584101fe2152","modified":1542519886101},{"_id":"source/images/resnet/pre-activation.png","hash":"e5a48d48c13e1aa75cafa6cde3abe38b5e2e65ca","modified":1542519886105},{"_id":"source/images/resnet/resnet-yooo.jpg","hash":"fd4cd53f877a8855d36a207c87dc7386eb82285b","modified":1542519886109},{"_id":"source/images/resnet/shortcut-connections.png","hash":"4da28d1804b67f4a75f58cdcb9cca8a55949f276","modified":1542519886109},{"_id":"source/images/resnet/training.png","hash":"161ca1d9b76900b889f71ff47cd61d1d85930d0d","modified":1542519886109},{"_id":"source/images/vgg/alexnet.png","hash":"33ea253bf43f9bb7530aa3ecae27f8955ebbd4ae","modified":1542519886113},{"_id":"source/images/vgg/lenet.png","hash":"1ef5f1e5e1f005813599142f42daca34a27627fb","modified":1542519886117},{"_id":"source/images/resnet/shortcut-connections-experiment.png","hash":"eb17c12e8e2580fdf6a698465e1a4ba85d4df1e4","modified":1542519886109},{"_id":"themes/myconcise/layout/_partial/post/tags.ejs","hash":"24abef606b55a9dda97dca79bb8ab46968919423","modified":1518505949000},{"_id":"themes/myconcise/layout/_partial/post/date.ejs","hash":"891db6745bac06df4be5a0f1cd69da1c0f90596a","modified":1518505949000},{"_id":"themes/myconcise/source/css/fonts/FontAwesome.otf","hash":"b5b4f9be85f91f10799e87a083da1d050f842734","modified":1542521239774},{"_id":"themes/myconcise/source/css/fonts/fontawesome-webfont.eot","hash":"7619748fe34c64fb157a57f6d4ef3678f63a8f5e","modified":1542521239774},{"_id":"themes/myconcise/source/css/fonts/fontawesome-webfont.woff","hash":"04c3bf56d87a0828935bd6b4aee859995f321693","modified":1542521239778},{"_id":"themes/myconcise/source/css/images/photo-waterfall.css","hash":"d1758950cef4f5e01d6419ad646541622e5c5342","modified":1542521239778},{"_id":"themes/myconcise/source/images/hello-osaka/output.json","hash":"754cb80c47680ec67492bacbe27a11ed3ba39502","modified":1542521240022},{"_id":"source/images/avatar.png","hash":"968779e038b1f43323806f6cdb221438b7572f84","modified":1542521239782},{"_id":"themes/myconcise/source/css/fonts/fontawesome-webfont.ttf","hash":"7f09c97f333917034ad08fa7295e916c9f72fd3f","modified":1542521239778},{"_id":"source/images/resnet/network_growth.jpg","hash":"4e3dbad7c70f55fb50f2e2cc3e01cb256f5050d1","modified":1542519886105},{"_id":"source/images/vgg/vggnets.png","hash":"6261a75847edf0f2284ee6787ab8a9978649c129","modified":1542519886117},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-1.jpg","hash":"75e90c0b66074f63bdb465ec0227a4eaf10f992f","modified":1542521240022},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-13.jpg","hash":"cd81d9c96366a509f2bf0369403f167ae6e74635","modified":1542521240026},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-14.jpg","hash":"b2621989b93fc7d0bba45ec624eb8a929e63031f","modified":1542521240026},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-15.jpg","hash":"b9359bcf5bc282f200b8066a1c7d9f3f3bf5fbd2","modified":1542521240026},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-17.jpg","hash":"e1cc289db07a29ac7dc17d4c4bc37d2cbd5be521","modified":1542521240026},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-2.jpg","hash":"60d80c91f95fe4059f710800fd8767e46977b136","modified":1542521240030},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-5.jpg","hash":"8254b0d7860a6e0497c0cf6fdfe116a3901a85e9","modified":1542521240034},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-7.jpg","hash":"80293b1d4a407ae52219f7891a29a6d79ac7cb12","modified":1542521240034},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-8.jpg","hash":"2beba43593101d37e819e631c855423913753efd","modified":1542521240034},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-9.jpg","hash":"fe7805b1717c6b7a8eafbf81e705c46ccea78fc2","modified":1542521240034},{"_id":"source/images/hello-osaka/osaka-20.jpg","hash":"8adcd789e85e4934bfbac45fd1c4f7e44e41529f","modified":1542519886017},{"_id":"source/images/resnet/architectures.png","hash":"b9f1a252b195e62fa3c8bb56d23ef9c61d6f05f1","modified":1542519886105},{"_id":"source/images/vgg/VGG-performance-comparison.png","hash":"9415026d608fdaa33d7e602777bf353004f6859e","modified":1542519886113},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-10.jpg","hash":"0f59f7a163efbc7e6d428d37bc62e0d4afca945d","modified":1542521240022},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-12.jpg","hash":"dc74edc34fd5f38f83ad7ac505f45445c1f9f221","modified":1542521240026},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-16.jpg","hash":"0f566e6cd63cd98f09be08f93aa32f5bfb92eefd","modified":1542521240026},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-4.jpg","hash":"fc4b95057f1b2cfaeadc424b0f04e07e739e9415","modified":1542521240034},{"_id":"themes/myconcise/source/css/fonts/fontawesome-webfont.svg","hash":"46fcc0194d75a0ddac0a038aee41b23456784814","modified":1542521239778},{"_id":"source/images/tftutorial/train.png","hash":"17607465dd2fefd9179de3f9fb3e38064d440f51","modified":1542519886113},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-20.jpg","hash":"7be3a1be9291e3d79ec231352e12dafe167d4dd2","modified":1542521240030},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-11.jpg","hash":"50619a330390ab65c09a425e151f9c00f6b6374b","modified":1542521240026},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-18.jpg","hash":"753747425301281ee4a55293767c0a0e5e88a271","modified":1542521240030},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-6.jpg","hash":"d963518ebe31f77df61be78fc96ac04de34735ac","modified":1542521240034},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-3.jpg","hash":"2377d9d4abe35ac07a975854c7ceba5ba27ffbb2","modified":1542521240034},{"_id":"themes/myconcise/source/images/hello-osaka/thumbnail/osaka-19.jpg","hash":"658743c7549bcfbada9ecb36cf733b6accb96818","modified":1542521240030},{"_id":"source/images/hello-osaka/osaka-19.jpg","hash":"36a4c7e346df670238c2c19002d20c10f63b8b0a","modified":1542519886005},{"_id":"source/images/hello-osaka/osaka-6.jpg","hash":"24f261be4b186842f7865e3524e24ab4643fdbba","modified":1542519886069},{"_id":"source/images/hello-osaka/osaka-15.jpg","hash":"a441cf1d1b5e09f37c1c68960ef77e6ce3fa6af4","modified":1542519885945},{"_id":"source/images/hello-osaka/osaka-9.jpg","hash":"95ee1e06f82df419622c34353a2994d6d50de5c6","modified":1542519886101},{"_id":"source/images/hello-osaka/osaka-13.jpg","hash":"1c14b4aedc531cc82725176544288d7d7a71eb88","modified":1542519885925},{"_id":"source/images/hello-osaka/osaka-5.jpg","hash":"e51cf4dbb75026235bd83ec44a7e2b8c757322ab","modified":1542519886065},{"_id":"source/images/hello-osaka/osaka-1.jpg","hash":"0405116ebc9737fcbb9d00157ad03dfcd6c24d9f","modified":1542519885865},{"_id":"source/images/hello-osaka/osaka-12.jpg","hash":"f6b475004ca5451f29fdec83e00116c0b831d92a","modified":1542519885921},{"_id":"source/images/hello-osaka/osaka-8.jpg","hash":"0c73562e9aee799d5a0e428c23790781ac3b5abf","modified":1542519886097},{"_id":"source/images/hello-osaka/osaka-2.jpg","hash":"ab5fe9b6f1ee31a3c7e0dc97890c3ccde5bf9781","modified":1542519886017},{"_id":"source/images/hello-osaka/osaka-18.jpg","hash":"29437344c3036c2e8b9814bc4b934f37adbabcfd","modified":1542519885997},{"_id":"source/images/hello-osaka/osaka-11.jpg","hash":"891b139bc73fe270aac0d6c225bb1879a29e15f4","modified":1542519885909},{"_id":"source/images/hello-osaka/osaka-14.jpg","hash":"6844e3e3f866ef60b72f3c30678520ce86a3dcf4","modified":1542519885937},{"_id":"source/images/hello-osaka/osaka-3.jpg","hash":"e3151c54f0b33e0dc11e8644f778a1620ab545ba","modified":1542519886029},{"_id":"source/images/hello-osaka/osaka-17.jpg","hash":"b487fa28bdd805a6127dfa08a5b431a48be018c9","modified":1542519885989},{"_id":"source/images/hello-osaka/osaka-7.jpg","hash":"f3d2b205243d691ff39e584f95bce0e3d5e75fd2","modified":1542519886085},{"_id":"source/images/hello-osaka/osaka-10.jpg","hash":"6129073ca834ba09adf8cf25406d4e2c2d10f0d8","modified":1542519885885},{"_id":"source/images/hello-osaka/osaka-4.jpg","hash":"7576736cc2d65b82e831472536842ee0f1db6fe1","modified":1542519886053},{"_id":"source/images/hello-osaka/osaka-16.jpg","hash":"efc4e6a8ead520e3f9ae1d7dd424cf6735a5b35b","modified":1542519885973}],"Category":[],"Data":[],"Page":[{"title":"About Me","_content":"\n## Info\n| Nickname          | Harper Long                                                     |\n|-------------------|-----------------------------------------------------------------|\n| Date of Birth     | November 25th, 1996                                             |\n| E-mail            | <linkinpark213@outlook.com>                                     |\n| Occupation        | Student                                                         |\n| Field of Interest | Computer Vision, Deep Learning, Software Engineering, Robotics  |\n| Relationship      | Available                                                       |\n| Motto             | All I want to do is trade this life for something new           |\n\n## Education\n* Currently - M1 student in the Robotics Lab & Robotics Vision Lab of Nara Institute of Science and Technology.\n* Bachelor - Software Engineering School of Xi'an Jiaotong University, June 2018.\n\n## Skills & Abilities\n* Programming in Java, C++ and Python\n* Basic usage of Javascript, PHP and MATLAB\n* Web app programming, from HTML to React\n* Solving problems using basic data structures and algorithms\n* Analyzing and designing software systems\n* Basic knowledge in machine learning and computer vision\n* Designing and training neural networks using Tensorflow\n\n## Experience\n* Internship, Qihoo 360 Corps (Beijing), Summer 2016 & 2017\n* Research Assistant, XJTU Spacial Vision Collaborative Laboratory, 2017","source":"about/index.md","raw":"---\ntitle: About Me\n---\n\n## Info\n| Nickname          | Harper Long                                                     |\n|-------------------|-----------------------------------------------------------------|\n| Date of Birth     | November 25th, 1996                                             |\n| E-mail            | <linkinpark213@outlook.com>                                     |\n| Occupation        | Student                                                         |\n| Field of Interest | Computer Vision, Deep Learning, Software Engineering, Robotics  |\n| Relationship      | Available                                                       |\n| Motto             | All I want to do is trade this life for something new           |\n\n## Education\n* Currently - M1 student in the Robotics Lab & Robotics Vision Lab of Nara Institute of Science and Technology.\n* Bachelor - Software Engineering School of Xi'an Jiaotong University, June 2018.\n\n## Skills & Abilities\n* Programming in Java, C++ and Python\n* Basic usage of Javascript, PHP and MATLAB\n* Web app programming, from HTML to React\n* Solving problems using basic data structures and algorithms\n* Analyzing and designing software systems\n* Basic knowledge in machine learning and computer vision\n* Designing and training neural networks using Tensorflow\n\n## Experience\n* Internship, Qihoo 360 Corps (Beijing), Summer 2016 & 2017\n* Research Assistant, XJTU Spacial Vision Collaborative Laboratory, 2017","date":"2018-11-18T05:44:45.857Z","updated":"2018-11-18T05:44:45.857Z","path":"about/index.html","comments":1,"layout":"page","_id":"cjomm0o0l0001wgwn2ybz5xb7","content":"<h2 id=\"Info\"><a href=\"#Info\" class=\"headerlink\" title=\"Info\"></a>Info</h2><table>\n<thead>\n<tr>\n<th>Nickname</th>\n<th>Harper Long</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Date of Birth</td>\n<td>November 25th, 1996</td>\n</tr>\n<tr>\n<td>E-mail</td>\n<td><a href=\"&#109;&#97;&#x69;&#108;&#116;&#111;&#58;&#x6c;&#x69;&#110;&#107;&#x69;&#110;&#x70;&#97;&#x72;&#x6b;&#x32;&#x31;&#x33;&#64;&#111;&#x75;&#x74;&#108;&#111;&#111;&#x6b;&#x2e;&#x63;&#x6f;&#109;\">&#x6c;&#x69;&#110;&#107;&#x69;&#110;&#x70;&#97;&#x72;&#x6b;&#x32;&#x31;&#x33;&#64;&#111;&#x75;&#x74;&#108;&#111;&#111;&#x6b;&#x2e;&#x63;&#x6f;&#109;</a></td>\n</tr>\n<tr>\n<td>Occupation</td>\n<td>Student</td>\n</tr>\n<tr>\n<td>Field of Interest</td>\n<td>Computer Vision, Deep Learning, Software Engineering, Robotics</td>\n</tr>\n<tr>\n<td>Relationship</td>\n<td>Available</td>\n</tr>\n<tr>\n<td>Motto</td>\n<td>All I want to do is trade this life for something new</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Education\"><a href=\"#Education\" class=\"headerlink\" title=\"Education\"></a>Education</h2><ul>\n<li>Currently - M1 student in the Robotics Lab &amp; Robotics Vision Lab of Nara Institute of Science and Technology.</li>\n<li>Bachelor - Software Engineering School of Xi&#39;an Jiaotong University, June 2018.</li>\n</ul>\n<h2 id=\"Skills-amp-Abilities\"><a href=\"#Skills-amp-Abilities\" class=\"headerlink\" title=\"Skills &amp; Abilities\"></a>Skills &amp; Abilities</h2><ul>\n<li>Programming in Java, C++ and Python</li>\n<li>Basic usage of Javascript, PHP and MATLAB</li>\n<li>Web app programming, from HTML to React</li>\n<li>Solving problems using basic data structures and algorithms</li>\n<li>Analyzing and designing software systems</li>\n<li>Basic knowledge in machine learning and computer vision</li>\n<li>Designing and training neural networks using Tensorflow</li>\n</ul>\n<h2 id=\"Experience\"><a href=\"#Experience\" class=\"headerlink\" title=\"Experience\"></a>Experience</h2><ul>\n<li>Internship, Qihoo 360 Corps (Beijing), Summer 2016 &amp; 2017</li>\n<li>Research Assistant, XJTU Spacial Vision Collaborative Laboratory, 2017</li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h2 id=\"Info\"><a href=\"#Info\" class=\"headerlink\" title=\"Info\"></a>Info</h2><table>\n<thead>\n<tr>\n<th>Nickname</th>\n<th>Harper Long</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Date of Birth</td>\n<td>November 25th, 1996</td>\n</tr>\n<tr>\n<td>E-mail</td>\n<td><a href=\"&#109;&#97;&#x69;&#108;&#116;&#111;&#58;&#x6c;&#x69;&#110;&#107;&#x69;&#110;&#x70;&#97;&#x72;&#x6b;&#x32;&#x31;&#x33;&#64;&#111;&#x75;&#x74;&#108;&#111;&#111;&#x6b;&#x2e;&#x63;&#x6f;&#109;\">&#x6c;&#x69;&#110;&#107;&#x69;&#110;&#x70;&#97;&#x72;&#x6b;&#x32;&#x31;&#x33;&#64;&#111;&#x75;&#x74;&#108;&#111;&#111;&#x6b;&#x2e;&#x63;&#x6f;&#109;</a></td>\n</tr>\n<tr>\n<td>Occupation</td>\n<td>Student</td>\n</tr>\n<tr>\n<td>Field of Interest</td>\n<td>Computer Vision, Deep Learning, Software Engineering, Robotics</td>\n</tr>\n<tr>\n<td>Relationship</td>\n<td>Available</td>\n</tr>\n<tr>\n<td>Motto</td>\n<td>All I want to do is trade this life for something new</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"Education\"><a href=\"#Education\" class=\"headerlink\" title=\"Education\"></a>Education</h2><ul>\n<li>Currently - M1 student in the Robotics Lab &amp; Robotics Vision Lab of Nara Institute of Science and Technology.</li>\n<li>Bachelor - Software Engineering School of Xi&#39;an Jiaotong University, June 2018.</li>\n</ul>\n<h2 id=\"Skills-amp-Abilities\"><a href=\"#Skills-amp-Abilities\" class=\"headerlink\" title=\"Skills &amp; Abilities\"></a>Skills &amp; Abilities</h2><ul>\n<li>Programming in Java, C++ and Python</li>\n<li>Basic usage of Javascript, PHP and MATLAB</li>\n<li>Web app programming, from HTML to React</li>\n<li>Solving problems using basic data structures and algorithms</li>\n<li>Analyzing and designing software systems</li>\n<li>Basic knowledge in machine learning and computer vision</li>\n<li>Designing and training neural networks using Tensorflow</li>\n</ul>\n<h2 id=\"Experience\"><a href=\"#Experience\" class=\"headerlink\" title=\"Experience\"></a>Experience</h2><ul>\n<li>Internship, Qihoo 360 Corps (Beijing), Summer 2016 &amp; 2017</li>\n<li>Research Assistant, XJTU Spacial Vision Collaborative Laboratory, 2017</li>\n</ul>\n"}],"Post":[{"title":"Hello New World!","date":"2018-07-26T01:12:52.000Z","_content":"\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/jquery.fancybox.min.css\"/>\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/images//photo-waterfall.css\"/>\n<script src=\"https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js\"></script>\n<script src=\"https://unpkg.com/minigrid@3.1.1/dist/minigrid.min.js\"></script>\n<script src=\"/js/jquery.fancybox.min.js\"></script>\n<script src=\"/js/photo-waterfall.js\"></script>\n\n\nHere are some photos that I took during my trip to Higashi-Osaka, Nara and Kyoto.\n\n<!-- more -->\n\n<div class=\"ImageGrid\"></div>\n\n<script>\nphoto_waterfall.init('/images/hello-osaka/');\n</script>\n<script src=\"/js/photo-waterfall-carousel.js\"></script>","source":"_posts/hello-osaka.md","raw":"---\ntitle: Hello New World!\ntags: Travel Gallery\ndate: 2018-07-26 10:12:52\n---\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/jquery.fancybox.min.css\"/>\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/images//photo-waterfall.css\"/>\n<script src=\"https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js\"></script>\n<script src=\"https://unpkg.com/minigrid@3.1.1/dist/minigrid.min.js\"></script>\n<script src=\"/js/jquery.fancybox.min.js\"></script>\n<script src=\"/js/photo-waterfall.js\"></script>\n\n\nHere are some photos that I took during my trip to Higashi-Osaka, Nara and Kyoto.\n\n<!-- more -->\n\n<div class=\"ImageGrid\"></div>\n\n<script>\nphoto_waterfall.init('/images/hello-osaka/');\n</script>\n<script src=\"/js/photo-waterfall-carousel.js\"></script>","slug":"hello-osaka","published":1,"updated":"2018-11-18T05:44:45.857Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomm0o0h0000wgwnvix643qg","content":"<p><link rel=\"stylesheet\" type=\"text/css\" href=\"/css/jquery.fancybox.min.css\"></p>\n<p><link rel=\"stylesheet\" type=\"text/css\" href=\"/css/images//photo-waterfall.css\"></p>\n<script src=\"https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js\"></script>\n<script src=\"https://unpkg.com/minigrid@3.1.1/dist/minigrid.min.js\"></script>\n<script src=\"/js/jquery.fancybox.min.js\"></script>\n<script src=\"/js/photo-waterfall.js\"></script>\n\n\n<p>Here are some photos that I took during my trip to Higashi-Osaka, Nara and Kyoto.</p>\n<a id=\"more\"></a>\n<div class=\"ImageGrid\"></div>\n\n<script>\nphoto_waterfall.init('/images/hello-osaka/');\n</script>\n<script src=\"/js/photo-waterfall-carousel.js\"></script>","site":{"data":{}},"excerpt":"<p><link rel=\"stylesheet\" type=\"text/css\" href=\"/css/jquery.fancybox.min.css\"></p>\n<p><link rel=\"stylesheet\" type=\"text/css\" href=\"/css/images//photo-waterfall.css\"></p>\n<script src=\"https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js\"></script>\n<script src=\"https://unpkg.com/minigrid@3.1.1/dist/minigrid.min.js\"></script>\n<script src=\"/js/jquery.fancybox.min.js\"></script>\n<script src=\"/js/photo-waterfall.js\"></script>\n\n\n<p>Here are some photos that I took during my trip to Higashi-Osaka, Nara and Kyoto.</p>","more":"<div class=\"ImageGrid\"></div>\n\n<script>\nphoto_waterfall.init('/images/hello-osaka/');\n</script>\n<script src=\"/js/photo-waterfall-carousel.js\"></script>"},{"title":"MathJax - Use Math in Hexo, Just Like Tex! (Including Problem Solutions)","date":"2018-04-24T05:43:41.000Z","_content":"<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nSometimes you may want to explain some algorithms or principles with beautiful formulae in your blog. How to do this? Edit them in Microsoft Word, take a screenshot, crop it and put it in the blog post? When you finish your article and find out that you missed a symbol in the pictures - oh man, gotta repeat that again? Stop using those images now! A beautiful math display engine - MathJax allows you to code math like a coder.\n\n<div style=\"font-size: 1.2em\">\n$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$\n</div>\n\n<!-- more -->\n\n## 1 Installation\n### 1.1 With npm (For those using Hexo like me)\nFirst, install *hexo-math* in your Hexo blog directory.\n```bash\n$ npm install hexo-math --save\n```\n\nThen, add *math* configurations in your *_config.yml* file.\n```yaml\nmath:\n  engine: 'mathjax'\n```\n\nFinally, also add to your *_config.yml* file in the **theme directory** these configurations below.\n```yaml\nmathjax:\n  enable: true\n  per_page: false\n  cdn: //cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML\n```\n\n### 1.2 Or by inserting a snippet in your HTML code\nMaybe you don't have to use math in every blog post. If so, insert the following snippet in your Markdown file also works.\n```html\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n```\n\n## 2 Usage\nMathJax supports the same grammar that LaTeX does. To learn more about LaTeX, please refer to Chapter 3 of [The Not So Short\nIntroduction to LATEX](https://tobi.oetiker.ch/lshort/lshort.pdf)(CN version also available [here](http://www.mohu.org/info/lshort-cn.pdf)).\n\nUse a \"\\\\\\\\\\(\" and a \"\\\\\\\\\\)\" to insert a formula in the line(they decide the boundary of the formula), or two \"$$\" to insert one that occupy a new line. I'll give a few examples below.\n\n\n```md\n\\\\(\\mathcal{F}(x)=\\mathcal{H}(x)-x\\\\)\n```\n\\\\(\\mathcal{F}(x)=\\mathcal{H}(x)-x\\\\)\n```md\n\\\\(E=mc^2\\\\)\n```\n\\\\(E=mc^2\\\\)\n```md\n$$\\lim_{n\\rightarrow \\infty}(1+2^n+3^n)^\\frac{1}{x+\\sin n}$$\n```\n$$\\lim_{n\\rightarrow \\infty}(1+2^n+3^n)^\\frac{1}{x+\\sin n}$$\n```md\n$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$\n```\n$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$\n\n## 3 Problems when using MathJax with Hexo & Solutions\nThis list will be appended whenever I find any more.\n### 3.1 Subscript symbol \"_\" gets mistaken as Markdown emphasize symbol\nThis is a tough problem. Hexo renderer would first render the .md file into a .html file, and the MathJax script will only work on the .html file. Therefore, when there are multiple subscript symbols, they might be rendered as &lt;em&gt;&lt;/em&gt; tags. \n\nFor example: when you actually need a full-line formula \\\\(x_{i+1}+y_j\\\\), perhaps you'll get a \"$$x<em>{i+1}+y</em>j$$\" instead. Look into the HTML code and you'll understand why.\n\nMy solution for now, is giving up this Markdown emphasize symbol, since both \"\\_\" and \"\\*\" can be used as emphasize tags, and the alternative symbol \"\\*\" will also work if we remove \"\\_\". Using \"\\\\\\_\" also works, but it would be frequently used(while \"\\*\" isn't), thus turning our math code into mess code.\n\nHow do we do this? Bravely look into the *node_modules* directory and find the renderer of the Hexo engine. My renderer is *marked*, which is the default for Hexo. There is a file named *marked.js* inside *node_modules/marked/lib/* directory. You can find two appearances of \"em:\". Like this: \n```js\nvar inline = {\n  ...\n  em: /^\\b_((?:[^_]|__)+?)_\\b|^\\*((?:\\*:\\*|[\\s\\S])+?)\\*(?!\\*)/,\n  ...\n};\n```\nand\n```js\ninline.pedantic = merge({}, inline.normal, {\n  ...\n  em: /^_(?=\\S)([\\s\\S]*?\\S)_(?!_)|^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/\n});\n```\n\nModify the regular expression after them - remove the one about \"\\_\"s and leave the one about \"\\*\"s. The new version would be:\n```js\nvar inline = {\n  ...\n  em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n  ...\n};\n```\nand\n```js\ninline.pedantic = merge({}, inline.normal, {\n  ...\n  em: /^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/\n});\n```\n\nFrom now on, you can use \"\\_\" as the subscript in MathJax freely. You don't have to worry about its becoming &lt;em&gt;&lt;/em&gt; tags anymore.\n\n### 3.2 Using \"&\" for aligning multi-line equations but getting a \"Misplaced &\"\nFor example, in my previous post about ResNet, I tried to use the following code to start a new line in an equation while aligning the lines to the equal sign:\n```md\n$$\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} & = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\\\\\n& = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)$$\n```\nThe \"&\" symbols were used to align the lines to a certain point. However, the result was a \"Misplaced &\" prompt.\n\nBy disabling MathJax, I found out that the rendered equation was correct, which means that **the problem isn't with Hexo renderer**. This was when I realized that although \n```md\n\\begin{equation}\n\\end{equation}\n```\nare not necessary, \n```md\n\\begin{split}\n\\end{split}\n```\nshouldn't be removed. Surround the equation with them will work. My code is here:\n```md\n$$\\begin{split}\n\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} & = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\\\\\n& = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)\n\\end{split}$$\n```\nAnd it runs like:\n$$\\begin{split}\n\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} & = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\\\\\n& = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)\n\\end{split}$$\n\n### 3.3 To be continued\nIf you encounter other issues while using MathJax with Hexo(with or without a solution), feel free to leave a comment below!","source":"_posts/mathjax.md","raw":"---\ntitle: 'MathJax - Use Math in Hexo, Just Like Tex! (Including Problem Solutions)'\ntags: Blogging\ndate: 2018-04-24 14:43:41\n---\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\nSometimes you may want to explain some algorithms or principles with beautiful formulae in your blog. How to do this? Edit them in Microsoft Word, take a screenshot, crop it and put it in the blog post? When you finish your article and find out that you missed a symbol in the pictures - oh man, gotta repeat that again? Stop using those images now! A beautiful math display engine - MathJax allows you to code math like a coder.\n\n<div style=\"font-size: 1.2em\">\n$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$\n</div>\n\n<!-- more -->\n\n## 1 Installation\n### 1.1 With npm (For those using Hexo like me)\nFirst, install *hexo-math* in your Hexo blog directory.\n```bash\n$ npm install hexo-math --save\n```\n\nThen, add *math* configurations in your *_config.yml* file.\n```yaml\nmath:\n  engine: 'mathjax'\n```\n\nFinally, also add to your *_config.yml* file in the **theme directory** these configurations below.\n```yaml\nmathjax:\n  enable: true\n  per_page: false\n  cdn: //cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML\n```\n\n### 1.2 Or by inserting a snippet in your HTML code\nMaybe you don't have to use math in every blog post. If so, insert the following snippet in your Markdown file also works.\n```html\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n```\n\n## 2 Usage\nMathJax supports the same grammar that LaTeX does. To learn more about LaTeX, please refer to Chapter 3 of [The Not So Short\nIntroduction to LATEX](https://tobi.oetiker.ch/lshort/lshort.pdf)(CN version also available [here](http://www.mohu.org/info/lshort-cn.pdf)).\n\nUse a \"\\\\\\\\\\(\" and a \"\\\\\\\\\\)\" to insert a formula in the line(they decide the boundary of the formula), or two \"$$\" to insert one that occupy a new line. I'll give a few examples below.\n\n\n```md\n\\\\(\\mathcal{F}(x)=\\mathcal{H}(x)-x\\\\)\n```\n\\\\(\\mathcal{F}(x)=\\mathcal{H}(x)-x\\\\)\n```md\n\\\\(E=mc^2\\\\)\n```\n\\\\(E=mc^2\\\\)\n```md\n$$\\lim_{n\\rightarrow \\infty}(1+2^n+3^n)^\\frac{1}{x+\\sin n}$$\n```\n$$\\lim_{n\\rightarrow \\infty}(1+2^n+3^n)^\\frac{1}{x+\\sin n}$$\n```md\n$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$\n```\n$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$\n\n## 3 Problems when using MathJax with Hexo & Solutions\nThis list will be appended whenever I find any more.\n### 3.1 Subscript symbol \"_\" gets mistaken as Markdown emphasize symbol\nThis is a tough problem. Hexo renderer would first render the .md file into a .html file, and the MathJax script will only work on the .html file. Therefore, when there are multiple subscript symbols, they might be rendered as &lt;em&gt;&lt;/em&gt; tags. \n\nFor example: when you actually need a full-line formula \\\\(x_{i+1}+y_j\\\\), perhaps you'll get a \"$$x<em>{i+1}+y</em>j$$\" instead. Look into the HTML code and you'll understand why.\n\nMy solution for now, is giving up this Markdown emphasize symbol, since both \"\\_\" and \"\\*\" can be used as emphasize tags, and the alternative symbol \"\\*\" will also work if we remove \"\\_\". Using \"\\\\\\_\" also works, but it would be frequently used(while \"\\*\" isn't), thus turning our math code into mess code.\n\nHow do we do this? Bravely look into the *node_modules* directory and find the renderer of the Hexo engine. My renderer is *marked*, which is the default for Hexo. There is a file named *marked.js* inside *node_modules/marked/lib/* directory. You can find two appearances of \"em:\". Like this: \n```js\nvar inline = {\n  ...\n  em: /^\\b_((?:[^_]|__)+?)_\\b|^\\*((?:\\*:\\*|[\\s\\S])+?)\\*(?!\\*)/,\n  ...\n};\n```\nand\n```js\ninline.pedantic = merge({}, inline.normal, {\n  ...\n  em: /^_(?=\\S)([\\s\\S]*?\\S)_(?!_)|^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/\n});\n```\n\nModify the regular expression after them - remove the one about \"\\_\"s and leave the one about \"\\*\"s. The new version would be:\n```js\nvar inline = {\n  ...\n  em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,\n  ...\n};\n```\nand\n```js\ninline.pedantic = merge({}, inline.normal, {\n  ...\n  em: /^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/\n});\n```\n\nFrom now on, you can use \"\\_\" as the subscript in MathJax freely. You don't have to worry about its becoming &lt;em&gt;&lt;/em&gt; tags anymore.\n\n### 3.2 Using \"&\" for aligning multi-line equations but getting a \"Misplaced &\"\nFor example, in my previous post about ResNet, I tried to use the following code to start a new line in an equation while aligning the lines to the equal sign:\n```md\n$$\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} & = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\\\\\n& = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)$$\n```\nThe \"&\" symbols were used to align the lines to a certain point. However, the result was a \"Misplaced &\" prompt.\n\nBy disabling MathJax, I found out that the rendered equation was correct, which means that **the problem isn't with Hexo renderer**. This was when I realized that although \n```md\n\\begin{equation}\n\\end{equation}\n```\nare not necessary, \n```md\n\\begin{split}\n\\end{split}\n```\nshouldn't be removed. Surround the equation with them will work. My code is here:\n```md\n$$\\begin{split}\n\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} & = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\\\\\n& = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)\n\\end{split}$$\n```\nAnd it runs like:\n$$\\begin{split}\n\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} & = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\\\\\n& = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)\n\\end{split}$$\n\n### 3.3 To be continued\nIf you encounter other issues while using MathJax with Hexo(with or without a solution), feel free to leave a comment below!","slug":"mathjax","published":1,"updated":"2018-11-18T05:44:45.857Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomm0o0l0002wgwnjo7w83om","content":"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML\" async></script>\n\n<p>Sometimes you may want to explain some algorithms or principles with beautiful formulae in your blog. How to do this? Edit them in Microsoft Word, take a screenshot, crop it and put it in the blog post? When you finish your article and find out that you missed a symbol in the pictures - oh man, gotta repeat that again? Stop using those images now! A beautiful math display engine - MathJax allows you to code math like a coder.</p>\n<div style=\"font-size: 1.2em\"><br>$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$<br></div>\n\n<a id=\"more\"></a>\n<h2 id=\"1-Installation\"><a href=\"#1-Installation\" class=\"headerlink\" title=\"1 Installation\"></a>1 Installation</h2><h3 id=\"1-1-With-npm-For-those-using-Hexo-like-me\"><a href=\"#1-1-With-npm-For-those-using-Hexo-like-me\" class=\"headerlink\" title=\"1.1 With npm (For those using Hexo like me)\"></a>1.1 With npm (For those using Hexo like me)</h3><p>First, install <em>hexo-math</em> in your Hexo blog directory.<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install hexo-math --save</span><br></pre></td></tr></table></figure></p>\n<p>Then, add <em>math</em> configurations in your <em>_config.yml</em> file.<br><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">math:</span></span><br><span class=\"line\"><span class=\"attr\">  engine:</span> <span class=\"string\">'mathjax'</span></span><br></pre></td></tr></table></figure></p>\n<p>Finally, also add to your <em>_config.yml</em> file in the <strong>theme directory</strong> these configurations below.<br><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">mathjax:</span></span><br><span class=\"line\"><span class=\"attr\">  enable:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">  per_page:</span> <span class=\"literal\">false</span></span><br><span class=\"line\"><span class=\"attr\">  cdn:</span> <span class=\"string\">//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"1-2-Or-by-inserting-a-snippet-in-your-HTML-code\"><a href=\"#1-2-Or-by-inserting-a-snippet-in-your-HTML-code\" class=\"headerlink\" title=\"1.2 Or by inserting a snippet in your HTML code\"></a>1.2 Or by inserting a snippet in your HTML code</h3><p>Maybe you don&#39;t have to use math in every blog post. If so, insert the following snippet in your Markdown file also works.<br><figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">script</span> <span class=\"attr\">src</span>=<span class=\"string\">'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML'</span> <span class=\"attr\">async</span>&gt;</span><span class=\"undefined\"></span><span class=\"tag\">&lt;/<span class=\"name\">script</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<h2 id=\"2-Usage\"><a href=\"#2-Usage\" class=\"headerlink\" title=\"2 Usage\"></a>2 Usage</h2><p>MathJax supports the same grammar that LaTeX does. To learn more about LaTeX, please refer to Chapter 3 of <a href=\"https://tobi.oetiker.ch/lshort/lshort.pdf\" target=\"_blank\" rel=\"noopener\">The Not So Short<br>Introduction to LATEX</a>(CN version also available <a href=\"http://www.mohu.org/info/lshort-cn.pdf\" target=\"_blank\" rel=\"noopener\">here</a>).</p>\n<p>Use a &quot;\\\\(&quot; and a &quot;\\\\)&quot; to insert a formula in the line(they decide the boundary of the formula), or two &quot;$$&quot; to insert one that occupy a new line. I&#39;ll give a few examples below.</p>\n<figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\\\(\\mathcal&#123;F&#125;(x)=\\mathcal&#123;H&#125;(x)-x\\\\)</span><br></pre></td></tr></table></figure>\n<p>\\(\\mathcal{F}(x)=\\mathcal{H}(x)-x\\)<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\\\(E=mc^2\\\\)</span><br></pre></td></tr></table></figure></p>\n<p>\\(E=mc^2\\)<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\lim_&#123;n\\rightarrow \\infty&#125;(1+2^n+3^n)^\\frac&#123;1&#125;&#123;x+\\sin n&#125;$$</span><br></pre></td></tr></table></figure></p>\n<p>$$\\lim_{n\\rightarrow \\infty}(1+2^n+3^n)^\\frac{1}{x+\\sin n}$$<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\mathcal&#123;C&#125;\\phi \\delta e \\mathfrak&#123;M&#125;\\alpha th \\mathit&#123;I&#125;n \\mathcal&#123;H&#125;ex\\sigma \\mathbb&#123;N&#125;o\\omega!$$</span><br></pre></td></tr></table></figure></p>\n<p>$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$</p>\n<h2 id=\"3-Problems-when-using-MathJax-with-Hexo-amp-Solutions\"><a href=\"#3-Problems-when-using-MathJax-with-Hexo-amp-Solutions\" class=\"headerlink\" title=\"3 Problems when using MathJax with Hexo &amp; Solutions\"></a>3 Problems when using MathJax with Hexo &amp; Solutions</h2><p>This list will be appended whenever I find any more.</p>\n<h3 id=\"3-1-Subscript-symbol-quot-quot-gets-mistaken-as-Markdown-emphasize-symbol\"><a href=\"#3-1-Subscript-symbol-quot-quot-gets-mistaken-as-Markdown-emphasize-symbol\" class=\"headerlink\" title=\"3.1 Subscript symbol &quot;_&quot; gets mistaken as Markdown emphasize symbol\"></a>3.1 Subscript symbol &quot;_&quot; gets mistaken as Markdown emphasize symbol</h3><p>This is a tough problem. Hexo renderer would first render the .md file into a .html file, and the MathJax script will only work on the .html file. Therefore, when there are multiple subscript symbols, they might be rendered as &lt;em&gt;&lt;/em&gt; tags. </p>\n<p>For example: when you actually need a full-line formula \\(x_{i+1}+y_j\\), perhaps you&#39;ll get a &quot;$$x<em>{i+1}+y</em>j$$&quot; instead. Look into the HTML code and you&#39;ll understand why.</p>\n<p>My solution for now, is giving up this Markdown emphasize symbol, since both &quot;_&quot; and &quot;*&quot; can be used as emphasize tags, and the alternative symbol &quot;*&quot; will also work if we remove &quot;_&quot;. Using &quot;\\_&quot; also works, but it would be frequently used(while &quot;*&quot; isn&#39;t), thus turning our math code into mess code.</p>\n<p>How do we do this? Bravely look into the <em>node_modules</em> directory and find the renderer of the Hexo engine. My renderer is <em>marked</em>, which is the default for Hexo. There is a file named <em>marked.js</em> inside <em>node_modules/marked/lib/</em> directory. You can find two appearances of &quot;em:&quot;. Like this:<br><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> inline = &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  em: <span class=\"regexp\">/^\\b_((?:[^_]|__)+?)_\\b|^\\*((?:\\*:\\*|[\\s\\S])+?)\\*(?!\\*)/</span>,</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure></p>\n<p>and<br><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline.pedantic = merge(&#123;&#125;, inline.normal, &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  em: <span class=\"regexp\">/^_(?=\\S)([\\s\\S]*?\\S)_(?!_)|^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/</span></span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>Modify the regular expression after them - remove the one about &quot;_&quot;s and leave the one about &quot;*&quot;s. The new version would be:<br><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> inline = &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  em: <span class=\"regexp\">/^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/</span>,</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure></p>\n<p>and<br><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline.pedantic = merge(&#123;&#125;, inline.normal, &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  em: <span class=\"regexp\">/^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/</span></span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>From now on, you can use &quot;_&quot; as the subscript in MathJax freely. You don&#39;t have to worry about its becoming &lt;em&gt;&lt;/em&gt; tags anymore.</p>\n<h3 id=\"3-2-Using-quot-amp-quot-for-aligning-multi-line-equations-but-getting-a-quot-Misplaced-amp-quot\"><a href=\"#3-2-Using-quot-amp-quot-for-aligning-multi-line-equations-but-getting-a-quot-Misplaced-amp-quot\" class=\"headerlink\" title=\"3.2 Using &quot;&amp;&quot; for aligning multi-line equations but getting a &quot;Misplaced &amp;&quot;\"></a>3.2 Using &quot;&amp;&quot; for aligning multi-line equations but getting a &quot;Misplaced &amp;&quot;</h3><p>For example, in my previous post about ResNet, I tried to use the following code to start a new line in an equation while aligning the lines to the equal sign:<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x<span class=\"emphasis\">_l&#125;&#125; &amp; = \\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x_</span>L&#125;&#125;\\frac&#123;\\partial&#123;x<span class=\"emphasis\">_L&#125;&#125;&#123;\\partial&#123;x_</span>l&#125;&#125;\\\\\\\\</span><br><span class=\"line\">&amp; = \\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x<span class=\"emphasis\">_L&#125;&#125;\\Big(1+\\frac&#123;\\partial&#123;&#125;&#125;&#123;\\partial&#123;x_</span>l&#125;&#125;\\sum<span class=\"emphasis\">_&#123;i=l&#125;^&#123;L-1&#125;\\mathcal&#123;F&#125;(x_</span>i,\\mathcal&#123;W&#125;_i)\\Big)$$</span><br></pre></td></tr></table></figure></p>\n<p>The &quot;&amp;&quot; symbols were used to align the lines to a certain point. However, the result was a &quot;Misplaced &amp;&quot; prompt.</p>\n<p>By disabling MathJax, I found out that the rendered equation was correct, which means that <strong>the problem isn&#39;t with Hexo renderer</strong>. This was when I realized that although<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\begin&#123;equation&#125;</span><br><span class=\"line\">\\end&#123;equation&#125;</span><br></pre></td></tr></table></figure></p>\n<p>are not necessary,<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\begin&#123;split&#125;</span><br><span class=\"line\">\\end&#123;split&#125;</span><br></pre></td></tr></table></figure></p>\n<p>shouldn&#39;t be removed. Surround the equation with them will work. My code is here:<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\begin&#123;split&#125;</span><br><span class=\"line\">\\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x<span class=\"emphasis\">_l&#125;&#125; &amp; = \\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x_</span>L&#125;&#125;\\frac&#123;\\partial&#123;x<span class=\"emphasis\">_L&#125;&#125;&#123;\\partial&#123;x_</span>l&#125;&#125;\\\\\\\\</span><br><span class=\"line\">&amp; = \\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x<span class=\"emphasis\">_L&#125;&#125;\\Big(1+\\frac&#123;\\partial&#123;&#125;&#125;&#123;\\partial&#123;x_</span>l&#125;&#125;\\sum<span class=\"emphasis\">_&#123;i=l&#125;^&#123;L-1&#125;\\mathcal&#123;F&#125;(x_</span>i,\\mathcal&#123;W&#125;_i)\\Big)</span><br><span class=\"line\">\\end&#123;split&#125;$$</span><br></pre></td></tr></table></figure></p>\n<p>And it runs like:<br>$$\\begin{split}<br>\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} &amp; = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\<br>&amp; = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x<em>l}}\\sum</em>{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)<br>\\end{split}$$</p>\n<h3 id=\"3-3-To-be-continued\"><a href=\"#3-3-To-be-continued\" class=\"headerlink\" title=\"3.3 To be continued\"></a>3.3 To be continued</h3><p>If you encounter other issues while using MathJax with Hexo(with or without a solution), feel free to leave a comment below!</p>\n","site":{"data":{}},"excerpt":"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML\" async></script>\n\n<p>Sometimes you may want to explain some algorithms or principles with beautiful formulae in your blog. How to do this? Edit them in Microsoft Word, take a screenshot, crop it and put it in the blog post? When you finish your article and find out that you missed a symbol in the pictures - oh man, gotta repeat that again? Stop using those images now! A beautiful math display engine - MathJax allows you to code math like a coder.</p>\n<div style=\"font-size: 1.2em\"><br>$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$<br></div>","more":"<h2 id=\"1-Installation\"><a href=\"#1-Installation\" class=\"headerlink\" title=\"1 Installation\"></a>1 Installation</h2><h3 id=\"1-1-With-npm-For-those-using-Hexo-like-me\"><a href=\"#1-1-With-npm-For-those-using-Hexo-like-me\" class=\"headerlink\" title=\"1.1 With npm (For those using Hexo like me)\"></a>1.1 With npm (For those using Hexo like me)</h3><p>First, install <em>hexo-math</em> in your Hexo blog directory.<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ npm install hexo-math --save</span><br></pre></td></tr></table></figure></p>\n<p>Then, add <em>math</em> configurations in your <em>_config.yml</em> file.<br><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">math:</span></span><br><span class=\"line\"><span class=\"attr\">  engine:</span> <span class=\"string\">'mathjax'</span></span><br></pre></td></tr></table></figure></p>\n<p>Finally, also add to your <em>_config.yml</em> file in the <strong>theme directory</strong> these configurations below.<br><figure class=\"highlight yaml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">mathjax:</span></span><br><span class=\"line\"><span class=\"attr\">  enable:</span> <span class=\"literal\">true</span></span><br><span class=\"line\"><span class=\"attr\">  per_page:</span> <span class=\"literal\">false</span></span><br><span class=\"line\"><span class=\"attr\">  cdn:</span> <span class=\"string\">//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML</span></span><br></pre></td></tr></table></figure></p>\n<h3 id=\"1-2-Or-by-inserting-a-snippet-in-your-HTML-code\"><a href=\"#1-2-Or-by-inserting-a-snippet-in-your-HTML-code\" class=\"headerlink\" title=\"1.2 Or by inserting a snippet in your HTML code\"></a>1.2 Or by inserting a snippet in your HTML code</h3><p>Maybe you don&#39;t have to use math in every blog post. If so, insert the following snippet in your Markdown file also works.<br><figure class=\"highlight html\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"tag\">&lt;<span class=\"name\">script</span> <span class=\"attr\">src</span>=<span class=\"string\">'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML'</span> <span class=\"attr\">async</span>&gt;</span><span class=\"undefined\"></span><span class=\"tag\">&lt;/<span class=\"name\">script</span>&gt;</span></span><br></pre></td></tr></table></figure></p>\n<h2 id=\"2-Usage\"><a href=\"#2-Usage\" class=\"headerlink\" title=\"2 Usage\"></a>2 Usage</h2><p>MathJax supports the same grammar that LaTeX does. To learn more about LaTeX, please refer to Chapter 3 of <a href=\"https://tobi.oetiker.ch/lshort/lshort.pdf\" target=\"_blank\" rel=\"noopener\">The Not So Short<br>Introduction to LATEX</a>(CN version also available <a href=\"http://www.mohu.org/info/lshort-cn.pdf\" target=\"_blank\" rel=\"noopener\">here</a>).</p>\n<p>Use a &quot;\\\\(&quot; and a &quot;\\\\)&quot; to insert a formula in the line(they decide the boundary of the formula), or two &quot;$$&quot; to insert one that occupy a new line. I&#39;ll give a few examples below.</p>\n<figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\\\(\\mathcal&#123;F&#125;(x)=\\mathcal&#123;H&#125;(x)-x\\\\)</span><br></pre></td></tr></table></figure>\n<p>\\(\\mathcal{F}(x)=\\mathcal{H}(x)-x\\)<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\\\(E=mc^2\\\\)</span><br></pre></td></tr></table></figure></p>\n<p>\\(E=mc^2\\)<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\lim_&#123;n\\rightarrow \\infty&#125;(1+2^n+3^n)^\\frac&#123;1&#125;&#123;x+\\sin n&#125;$$</span><br></pre></td></tr></table></figure></p>\n<p>$$\\lim_{n\\rightarrow \\infty}(1+2^n+3^n)^\\frac{1}{x+\\sin n}$$<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\mathcal&#123;C&#125;\\phi \\delta e \\mathfrak&#123;M&#125;\\alpha th \\mathit&#123;I&#125;n \\mathcal&#123;H&#125;ex\\sigma \\mathbb&#123;N&#125;o\\omega!$$</span><br></pre></td></tr></table></figure></p>\n<p>$$\\mathcal{C}\\phi \\delta e \\mathfrak{M}\\alpha th \\mathit{I}n \\mathcal{H}ex\\sigma \\mathbb{N}o\\omega!$$</p>\n<h2 id=\"3-Problems-when-using-MathJax-with-Hexo-amp-Solutions\"><a href=\"#3-Problems-when-using-MathJax-with-Hexo-amp-Solutions\" class=\"headerlink\" title=\"3 Problems when using MathJax with Hexo &amp; Solutions\"></a>3 Problems when using MathJax with Hexo &amp; Solutions</h2><p>This list will be appended whenever I find any more.</p>\n<h3 id=\"3-1-Subscript-symbol-quot-quot-gets-mistaken-as-Markdown-emphasize-symbol\"><a href=\"#3-1-Subscript-symbol-quot-quot-gets-mistaken-as-Markdown-emphasize-symbol\" class=\"headerlink\" title=\"3.1 Subscript symbol &quot;_&quot; gets mistaken as Markdown emphasize symbol\"></a>3.1 Subscript symbol &quot;_&quot; gets mistaken as Markdown emphasize symbol</h3><p>This is a tough problem. Hexo renderer would first render the .md file into a .html file, and the MathJax script will only work on the .html file. Therefore, when there are multiple subscript symbols, they might be rendered as &lt;em&gt;&lt;/em&gt; tags. </p>\n<p>For example: when you actually need a full-line formula \\(x_{i+1}+y_j\\), perhaps you&#39;ll get a &quot;$$x<em>{i+1}+y</em>j$$&quot; instead. Look into the HTML code and you&#39;ll understand why.</p>\n<p>My solution for now, is giving up this Markdown emphasize symbol, since both &quot;_&quot; and &quot;*&quot; can be used as emphasize tags, and the alternative symbol &quot;*&quot; will also work if we remove &quot;_&quot;. Using &quot;\\_&quot; also works, but it would be frequently used(while &quot;*&quot; isn&#39;t), thus turning our math code into mess code.</p>\n<p>How do we do this? Bravely look into the <em>node_modules</em> directory and find the renderer of the Hexo engine. My renderer is <em>marked</em>, which is the default for Hexo. There is a file named <em>marked.js</em> inside <em>node_modules/marked/lib/</em> directory. You can find two appearances of &quot;em:&quot;. Like this:<br><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> inline = &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  em: <span class=\"regexp\">/^\\b_((?:[^_]|__)+?)_\\b|^\\*((?:\\*:\\*|[\\s\\S])+?)\\*(?!\\*)/</span>,</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure></p>\n<p>and<br><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline.pedantic = merge(&#123;&#125;, inline.normal, &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  em: <span class=\"regexp\">/^_(?=\\S)([\\s\\S]*?\\S)_(?!_)|^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/</span></span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>Modify the regular expression after them - remove the one about &quot;_&quot;s and leave the one about &quot;*&quot;s. The new version would be:<br><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">var</span> inline = &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  em: <span class=\"regexp\">/^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/</span>,</span><br><span class=\"line\">  ...</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure></p>\n<p>and<br><figure class=\"highlight js\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">inline.pedantic = merge(&#123;&#125;, inline.normal, &#123;</span><br><span class=\"line\">  ...</span><br><span class=\"line\">  em: <span class=\"regexp\">/^\\*(?=\\S)([\\s\\S]*?\\S)\\*(?!\\*)/</span></span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure></p>\n<p>From now on, you can use &quot;_&quot; as the subscript in MathJax freely. You don&#39;t have to worry about its becoming &lt;em&gt;&lt;/em&gt; tags anymore.</p>\n<h3 id=\"3-2-Using-quot-amp-quot-for-aligning-multi-line-equations-but-getting-a-quot-Misplaced-amp-quot\"><a href=\"#3-2-Using-quot-amp-quot-for-aligning-multi-line-equations-but-getting-a-quot-Misplaced-amp-quot\" class=\"headerlink\" title=\"3.2 Using &quot;&amp;&quot; for aligning multi-line equations but getting a &quot;Misplaced &amp;&quot;\"></a>3.2 Using &quot;&amp;&quot; for aligning multi-line equations but getting a &quot;Misplaced &amp;&quot;</h3><p>For example, in my previous post about ResNet, I tried to use the following code to start a new line in an equation while aligning the lines to the equal sign:<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x<span class=\"emphasis\">_l&#125;&#125; &amp; = \\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x_</span>L&#125;&#125;\\frac&#123;\\partial&#123;x<span class=\"emphasis\">_L&#125;&#125;&#123;\\partial&#123;x_</span>l&#125;&#125;\\\\\\\\</span><br><span class=\"line\">&amp; = \\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x<span class=\"emphasis\">_L&#125;&#125;\\Big(1+\\frac&#123;\\partial&#123;&#125;&#125;&#123;\\partial&#123;x_</span>l&#125;&#125;\\sum<span class=\"emphasis\">_&#123;i=l&#125;^&#123;L-1&#125;\\mathcal&#123;F&#125;(x_</span>i,\\mathcal&#123;W&#125;_i)\\Big)$$</span><br></pre></td></tr></table></figure></p>\n<p>The &quot;&amp;&quot; symbols were used to align the lines to a certain point. However, the result was a &quot;Misplaced &amp;&quot; prompt.</p>\n<p>By disabling MathJax, I found out that the rendered equation was correct, which means that <strong>the problem isn&#39;t with Hexo renderer</strong>. This was when I realized that although<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\begin&#123;equation&#125;</span><br><span class=\"line\">\\end&#123;equation&#125;</span><br></pre></td></tr></table></figure></p>\n<p>are not necessary,<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">\\begin&#123;split&#125;</span><br><span class=\"line\">\\end&#123;split&#125;</span><br></pre></td></tr></table></figure></p>\n<p>shouldn&#39;t be removed. Surround the equation with them will work. My code is here:<br><figure class=\"highlight md\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$$\\begin&#123;split&#125;</span><br><span class=\"line\">\\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x<span class=\"emphasis\">_l&#125;&#125; &amp; = \\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x_</span>L&#125;&#125;\\frac&#123;\\partial&#123;x<span class=\"emphasis\">_L&#125;&#125;&#123;\\partial&#123;x_</span>l&#125;&#125;\\\\\\\\</span><br><span class=\"line\">&amp; = \\frac&#123;\\partial&#123;\\mathcal&#123;E&#125;&#125;&#125;&#123;\\partial&#123;x<span class=\"emphasis\">_L&#125;&#125;\\Big(1+\\frac&#123;\\partial&#123;&#125;&#125;&#123;\\partial&#123;x_</span>l&#125;&#125;\\sum<span class=\"emphasis\">_&#123;i=l&#125;^&#123;L-1&#125;\\mathcal&#123;F&#125;(x_</span>i,\\mathcal&#123;W&#125;_i)\\Big)</span><br><span class=\"line\">\\end&#123;split&#125;$$</span><br></pre></td></tr></table></figure></p>\n<p>And it runs like:<br>$$\\begin{split}<br>\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} &amp; = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\<br>&amp; = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x<em>l}}\\sum</em>{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)<br>\\end{split}$$</p>\n<h3 id=\"3-3-To-be-continued\"><a href=\"#3-3-To-be-continued\" class=\"headerlink\" title=\"3.3 To be continued\"></a>3.3 To be continued</h3><p>If you encounter other issues while using MathJax with Hexo(with or without a solution), feel free to leave a comment below!</p>"},{"title":"Cheers for the 8th International linkinpark213 Day!","date":"2018-02-13T09:40:21.000Z","_content":"\nCheers for the 8th International linkinpark213 Day!\n\n```bash\nfor i in 'Harper' 'Sweet' 'Kobayashi' 'Kawasaki'\ndo\n    echo \"I'm $i, cheers!\"\ndone\n```\n\n## What is linkinpark213 Day?\n\nInternational linkinpark213 Day is a global anniversary set up by Harper Long in 2011 A.D., celebrated on February 13th every year. This anniversary is officially written as 'linkinpark213 Day', the first letter of which is a lower-case. The establishment of the anniversary dates back to the early 10s in the 21st century.\n\nTill now, the population for this anniversary has already reached 1e-6 million, and the distribution also expanded from a small county to the whole middle-China area, including Hebei, Henan, Shanxi and Shaanxi province. There will also be some Japanese resident who plan to celebrate this day in 2019.\n\n<!-- more -->\n\n## Why 13th Feb?\n\nAccording to the modern Chinese habit of writing, '13th Feb' is usually written as '2.13'. Also, '13' and 'B' look similar and are often regarded to be equal. In conclusion, '13 Feb' can be transformed to '2B', which is a common word in Chinese. Although the word is sometimes classified as \"offensive\", it reflects feelings of optimism, bravery and entertainment.\n\n## How do people celebrate the day?\n\nWhen the anniversary was first set up, there was no officially specified ways of celebration. People gathered, held parties and enjoyed spending time together. \n\nOn the 3th linkinpark213 Day, a proposal by a high school Chinese teacher was taken as the official way of celebration -- on each linkinpark213 Day, a number of people participate in the Pigeon-Flying Competition funded by Harper Long. This way of celebration prevailed until now, and all participants except Harper Long won the competition every year.\n\nPigeon-Flying is a broadly-accepted traditional Chinese custom, the exact origin of which is too ancient to be revealed. Modern scholars tend to believe that this custom became well-known no later than 206 A.D. in China. In modern times, pigeon-flying is an activity involving \"making a promise\" and \"not keeping it\". According to some folk stories, this activity was firstly named when it happened to a pigeon keeper when he forgot to keep a promise that he made with a friend. \n\n```java\npublic static void main(String[] args) {\n    Promise promise = new Promise();\n    promise.make(\"I will come!\");\n    System.exit(0);\n    promise.keep();\n}\n```\n\n(Please notice that pigeon-flying mentioned here is not the same activity as what happens every Olympics since 1896 A.D.)\n\nJoin our celebration today! You can easily participate in the Pigeon-Flying Competition by not participating.","source":"_posts/linkinpark213-day.md","raw":"---\ntitle: Cheers for the 8th International linkinpark213 Day!\ntag: Other\ndate: 2018-02-13 18:40:21\n---\n\nCheers for the 8th International linkinpark213 Day!\n\n```bash\nfor i in 'Harper' 'Sweet' 'Kobayashi' 'Kawasaki'\ndo\n    echo \"I'm $i, cheers!\"\ndone\n```\n\n## What is linkinpark213 Day?\n\nInternational linkinpark213 Day is a global anniversary set up by Harper Long in 2011 A.D., celebrated on February 13th every year. This anniversary is officially written as 'linkinpark213 Day', the first letter of which is a lower-case. The establishment of the anniversary dates back to the early 10s in the 21st century.\n\nTill now, the population for this anniversary has already reached 1e-6 million, and the distribution also expanded from a small county to the whole middle-China area, including Hebei, Henan, Shanxi and Shaanxi province. There will also be some Japanese resident who plan to celebrate this day in 2019.\n\n<!-- more -->\n\n## Why 13th Feb?\n\nAccording to the modern Chinese habit of writing, '13th Feb' is usually written as '2.13'. Also, '13' and 'B' look similar and are often regarded to be equal. In conclusion, '13 Feb' can be transformed to '2B', which is a common word in Chinese. Although the word is sometimes classified as \"offensive\", it reflects feelings of optimism, bravery and entertainment.\n\n## How do people celebrate the day?\n\nWhen the anniversary was first set up, there was no officially specified ways of celebration. People gathered, held parties and enjoyed spending time together. \n\nOn the 3th linkinpark213 Day, a proposal by a high school Chinese teacher was taken as the official way of celebration -- on each linkinpark213 Day, a number of people participate in the Pigeon-Flying Competition funded by Harper Long. This way of celebration prevailed until now, and all participants except Harper Long won the competition every year.\n\nPigeon-Flying is a broadly-accepted traditional Chinese custom, the exact origin of which is too ancient to be revealed. Modern scholars tend to believe that this custom became well-known no later than 206 A.D. in China. In modern times, pigeon-flying is an activity involving \"making a promise\" and \"not keeping it\". According to some folk stories, this activity was firstly named when it happened to a pigeon keeper when he forgot to keep a promise that he made with a friend. \n\n```java\npublic static void main(String[] args) {\n    Promise promise = new Promise();\n    promise.make(\"I will come!\");\n    System.exit(0);\n    promise.keep();\n}\n```\n\n(Please notice that pigeon-flying mentioned here is not the same activity as what happens every Olympics since 1896 A.D.)\n\nJoin our celebration today! You can easily participate in the Pigeon-Flying Competition by not participating.","slug":"linkinpark213-day","published":1,"updated":"2018-11-18T06:04:41.608Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomm0o0o0004wgwngibl0t8l","content":"<p>Cheers for the 8th International linkinpark213 Day!</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"string\">'Harper'</span> <span class=\"string\">'Sweet'</span> <span class=\"string\">'Kobayashi'</span> <span class=\"string\">'Kawasaki'</span></span><br><span class=\"line\"><span class=\"keyword\">do</span></span><br><span class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">\"I'm <span class=\"variable\">$i</span>, cheers!\"</span></span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"What-is-linkinpark213-Day\"><a href=\"#What-is-linkinpark213-Day\" class=\"headerlink\" title=\"What is linkinpark213 Day?\"></a>What is linkinpark213 Day?</h2><p>International linkinpark213 Day is a global anniversary set up by Harper Long in 2011 A.D., celebrated on February 13th every year. This anniversary is officially written as &#39;linkinpark213 Day&#39;, the first letter of which is a lower-case. The establishment of the anniversary dates back to the early 10s in the 21st century.</p>\n<p>Till now, the population for this anniversary has already reached 1e-6 million, and the distribution also expanded from a small county to the whole middle-China area, including Hebei, Henan, Shanxi and Shaanxi province. There will also be some Japanese resident who plan to celebrate this day in 2019.</p>\n<a id=\"more\"></a>\n<h2 id=\"Why-13th-Feb\"><a href=\"#Why-13th-Feb\" class=\"headerlink\" title=\"Why 13th Feb?\"></a>Why 13th Feb?</h2><p>According to the modern Chinese habit of writing, &#39;13th Feb&#39; is usually written as &#39;2.13&#39;. Also, &#39;13&#39; and &#39;B&#39; look similar and are often regarded to be equal. In conclusion, &#39;13 Feb&#39; can be transformed to &#39;2B&#39;, which is a common word in Chinese. Although the word is sometimes classified as &quot;offensive&quot;, it reflects feelings of optimism, bravery and entertainment.</p>\n<h2 id=\"How-do-people-celebrate-the-day\"><a href=\"#How-do-people-celebrate-the-day\" class=\"headerlink\" title=\"How do people celebrate the day?\"></a>How do people celebrate the day?</h2><p>When the anniversary was first set up, there was no officially specified ways of celebration. People gathered, held parties and enjoyed spending time together. </p>\n<p>On the 3th linkinpark213 Day, a proposal by a high school Chinese teacher was taken as the official way of celebration -- on each linkinpark213 Day, a number of people participate in the Pigeon-Flying Competition funded by Harper Long. This way of celebration prevailed until now, and all participants except Harper Long won the competition every year.</p>\n<p>Pigeon-Flying is a broadly-accepted traditional Chinese custom, the exact origin of which is too ancient to be revealed. Modern scholars tend to believe that this custom became well-known no later than 206 A.D. in China. In modern times, pigeon-flying is an activity involving &quot;making a promise&quot; and &quot;not keeping it&quot;. According to some folk stories, this activity was firstly named when it happened to a pigeon keeper when he forgot to keep a promise that he made with a friend. </p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">    Promise promise = <span class=\"keyword\">new</span> Promise();</span><br><span class=\"line\">    promise.make(<span class=\"string\">\"I will come!\"</span>);</span><br><span class=\"line\">    System.exit(<span class=\"number\">0</span>);</span><br><span class=\"line\">    promise.keep();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>(Please notice that pigeon-flying mentioned here is not the same activity as what happens every Olympics since 1896 A.D.)</p>\n<p>Join our celebration today! You can easily participate in the Pigeon-Flying Competition by not participating.</p>\n","site":{"data":{}},"excerpt":"<p>Cheers for the 8th International linkinpark213 Day!</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"string\">'Harper'</span> <span class=\"string\">'Sweet'</span> <span class=\"string\">'Kobayashi'</span> <span class=\"string\">'Kawasaki'</span></span><br><span class=\"line\"><span class=\"keyword\">do</span></span><br><span class=\"line\">    <span class=\"built_in\">echo</span> <span class=\"string\">\"I'm <span class=\"variable\">$i</span>, cheers!\"</span></span><br><span class=\"line\"><span class=\"keyword\">done</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"What-is-linkinpark213-Day\"><a href=\"#What-is-linkinpark213-Day\" class=\"headerlink\" title=\"What is linkinpark213 Day?\"></a>What is linkinpark213 Day?</h2><p>International linkinpark213 Day is a global anniversary set up by Harper Long in 2011 A.D., celebrated on February 13th every year. This anniversary is officially written as &#39;linkinpark213 Day&#39;, the first letter of which is a lower-case. The establishment of the anniversary dates back to the early 10s in the 21st century.</p>\n<p>Till now, the population for this anniversary has already reached 1e-6 million, and the distribution also expanded from a small county to the whole middle-China area, including Hebei, Henan, Shanxi and Shaanxi province. There will also be some Japanese resident who plan to celebrate this day in 2019.</p>","more":"<h2 id=\"Why-13th-Feb\"><a href=\"#Why-13th-Feb\" class=\"headerlink\" title=\"Why 13th Feb?\"></a>Why 13th Feb?</h2><p>According to the modern Chinese habit of writing, &#39;13th Feb&#39; is usually written as &#39;2.13&#39;. Also, &#39;13&#39; and &#39;B&#39; look similar and are often regarded to be equal. In conclusion, &#39;13 Feb&#39; can be transformed to &#39;2B&#39;, which is a common word in Chinese. Although the word is sometimes classified as &quot;offensive&quot;, it reflects feelings of optimism, bravery and entertainment.</p>\n<h2 id=\"How-do-people-celebrate-the-day\"><a href=\"#How-do-people-celebrate-the-day\" class=\"headerlink\" title=\"How do people celebrate the day?\"></a>How do people celebrate the day?</h2><p>When the anniversary was first set up, there was no officially specified ways of celebration. People gathered, held parties and enjoyed spending time together. </p>\n<p>On the 3th linkinpark213 Day, a proposal by a high school Chinese teacher was taken as the official way of celebration -- on each linkinpark213 Day, a number of people participate in the Pigeon-Flying Competition funded by Harper Long. This way of celebration prevailed until now, and all participants except Harper Long won the competition every year.</p>\n<p>Pigeon-Flying is a broadly-accepted traditional Chinese custom, the exact origin of which is too ancient to be revealed. Modern scholars tend to believe that this custom became well-known no later than 206 A.D. in China. In modern times, pigeon-flying is an activity involving &quot;making a promise&quot; and &quot;not keeping it&quot;. According to some folk stories, this activity was firstly named when it happened to a pigeon keeper when he forgot to keep a promise that he made with a friend. </p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title\">main</span><span class=\"params\">(String[] args)</span> </span>&#123;</span><br><span class=\"line\">    Promise promise = <span class=\"keyword\">new</span> Promise();</span><br><span class=\"line\">    promise.make(<span class=\"string\">\"I will come!\"</span>);</span><br><span class=\"line\">    System.exit(<span class=\"number\">0</span>);</span><br><span class=\"line\">    promise.keep();</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>(Please notice that pigeon-flying mentioned here is not the same activity as what happens every Olympics since 1896 A.D.)</p>\n<p>Join our celebration today! You can easily participate in the Pigeon-Flying Competition by not participating.</p>"},{"title":"Smartypants is NOT SO SMART","date":"2018-03-20T02:00:22.000Z","_content":"When blogging with Hexo, every time I type a single quotation mark(also called apostrophe) like this: \n```\n'\n```\n, Hexo would convert it to a symbol like this\n```\n\n```\nYou would say that this is also an apotrophe, but it really looks UNBEARABLE in the articles. It's been a problem bothering me for more than a month.(I'm not saying that this is the reason for not updating my blog, but I don't mind if you think so!)\n\n<!-- more -->\n\n\n![apostrophe](/images/smartypants/apostrophe.png)\n\nTherefore, I Googled about this problem and tried to find other victims. According to their sharings, this problem is caused by *marked* -- the default markdown renderer of Hexo.The *\"smatrypants\"* function of marked was turned on by default.\n\nNow take a look at the introduction of *smartypants* on the *hexo-renderer-marked* page:\n> *smartypants* - Use \"smart\" typograhic punctuation for things like quotes and dashes.\n\nC'mon, seriously? \n\nThere are a few bloggers who solved this by adding the code below to the _config.yml file in the blog directory. \n\n```yml\nmark:\n  smartypants: false\n```\n\nThis worked for most victims (perhaps all of them), but not for me. I have no idea why those config wasn't working, so if anyone finds out the reason, please contact me by e-mail.\n\nIf you're sure *smartypants* is causing the problem, and the solution above didn't work for you either, maybe you can try my solution.\n\nSince *hexo-renderer-marked* is installed in the blog's *node_modules* directory(may also be in your Node.js directory if installed globally), isn't it possible that we change its own configurations? I looked at the *index.js* file in the *node_modules/hexo-renderer-marked/* directory. There you are, smartypants!\n\n```javascript\nhexo.config.marked = assign({\n  gfm: true,\n  pedantic: false,\n  sanitize: false,\n  tables: true,\n  breaks: true,\n  smartLists: true,\n  smartypants: true,\n  modifyAnchors: '',\n  autolink: true\n}, hexo.config.marked);\n```\n\nNow you know what to do.\n\nAaaaaaaaaaaaaaaand many thanks to Xizi Wu, the artist of my new avatar! I love it!\n![Harper Long by Xizi Wu](/images/long_nobg.png)","source":"_posts/smartypants.md","raw":"---\ntitle: Smartypants is NOT SO SMART\ndate: 2018-03-20 11:00:22\ntags: Blogging\n---\nWhen blogging with Hexo, every time I type a single quotation mark(also called apostrophe) like this: \n```\n'\n```\n, Hexo would convert it to a symbol like this\n```\n\n```\nYou would say that this is also an apotrophe, but it really looks UNBEARABLE in the articles. It's been a problem bothering me for more than a month.(I'm not saying that this is the reason for not updating my blog, but I don't mind if you think so!)\n\n<!-- more -->\n\n\n![apostrophe](/images/smartypants/apostrophe.png)\n\nTherefore, I Googled about this problem and tried to find other victims. According to their sharings, this problem is caused by *marked* -- the default markdown renderer of Hexo.The *\"smatrypants\"* function of marked was turned on by default.\n\nNow take a look at the introduction of *smartypants* on the *hexo-renderer-marked* page:\n> *smartypants* - Use \"smart\" typograhic punctuation for things like quotes and dashes.\n\nC'mon, seriously? \n\nThere are a few bloggers who solved this by adding the code below to the _config.yml file in the blog directory. \n\n```yml\nmark:\n  smartypants: false\n```\n\nThis worked for most victims (perhaps all of them), but not for me. I have no idea why those config wasn't working, so if anyone finds out the reason, please contact me by e-mail.\n\nIf you're sure *smartypants* is causing the problem, and the solution above didn't work for you either, maybe you can try my solution.\n\nSince *hexo-renderer-marked* is installed in the blog's *node_modules* directory(may also be in your Node.js directory if installed globally), isn't it possible that we change its own configurations? I looked at the *index.js* file in the *node_modules/hexo-renderer-marked/* directory. There you are, smartypants!\n\n```javascript\nhexo.config.marked = assign({\n  gfm: true,\n  pedantic: false,\n  sanitize: false,\n  tables: true,\n  breaks: true,\n  smartLists: true,\n  smartypants: true,\n  modifyAnchors: '',\n  autolink: true\n}, hexo.config.marked);\n```\n\nNow you know what to do.\n\nAaaaaaaaaaaaaaaand many thanks to Xizi Wu, the artist of my new avatar! I love it!\n![Harper Long by Xizi Wu](/images/long_nobg.png)","slug":"smartypants","published":1,"updated":"2018-11-18T05:44:45.857Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomm0o0p0005wgwn7qj40jhu","content":"<p>When blogging with Hexo, every time I type a single quotation mark(also called apostrophe) like this:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure></p>\n<p>, Hexo would convert it to a symbol like this<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br></pre></td></tr></table></figure></p>\n<p>You would say that this is also an apotrophe, but it really looks UNBEARABLE in the articles. It&#39;s been a problem bothering me for more than a month.(I&#39;m not saying that this is the reason for not updating my blog, but I don&#39;t mind if you think so!)</p>\n<a id=\"more\"></a>\n<p><img src=\"/images/smartypants/apostrophe.png\" alt=\"apostrophe\"></p>\n<p>Therefore, I Googled about this problem and tried to find other victims. According to their sharings, this problem is caused by <em>marked</em> -- the default markdown renderer of Hexo.The <em>&quot;smatrypants&quot;</em> function of marked was turned on by default.</p>\n<p>Now take a look at the introduction of <em>smartypants</em> on the <em>hexo-renderer-marked</em> page:</p>\n<blockquote>\n<p><em>smartypants</em> - Use &quot;smart&quot; typograhic punctuation for things like quotes and dashes.</p>\n</blockquote>\n<p>C&#39;mon, seriously? </p>\n<p>There are a few bloggers who solved this by adding the code below to the _config.yml file in the blog directory. </p>\n<figure class=\"highlight yml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">mark:</span></span><br><span class=\"line\"><span class=\"attr\">  smartypants:</span> <span class=\"literal\">false</span></span><br></pre></td></tr></table></figure>\n<p>This worked for most victims (perhaps all of them), but not for me. I have no idea why those config wasn&#39;t working, so if anyone finds out the reason, please contact me by e-mail.</p>\n<p>If you&#39;re sure <em>smartypants</em> is causing the problem, and the solution above didn&#39;t work for you either, maybe you can try my solution.</p>\n<p>Since <em>hexo-renderer-marked</em> is installed in the blog&#39;s <em>node_modules</em> directory(may also be in your Node.js directory if installed globally), isn&#39;t it possible that we change its own configurations? I looked at the <em>index.js</em> file in the <em>node_modules/hexo-renderer-marked/</em> directory. There you are, smartypants!</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo.config.marked = assign(&#123;</span><br><span class=\"line\">  gfm: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  pedantic: <span class=\"literal\">false</span>,</span><br><span class=\"line\">  sanitize: <span class=\"literal\">false</span>,</span><br><span class=\"line\">  tables: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  breaks: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  smartLists: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  smartypants: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  modifyAnchors: <span class=\"string\">''</span>,</span><br><span class=\"line\">  autolink: <span class=\"literal\">true</span></span><br><span class=\"line\">&#125;, hexo.config.marked);</span><br></pre></td></tr></table></figure>\n<p>Now you know what to do.</p>\n<p>Aaaaaaaaaaaaaaaand many thanks to Xizi Wu, the artist of my new avatar! I love it!<br><img src=\"/images/long_nobg.png\" alt=\"Harper Long by Xizi Wu\"></p>\n","site":{"data":{}},"excerpt":"<p>When blogging with Hexo, every time I type a single quotation mark(also called apostrophe) like this:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&apos;</span><br></pre></td></tr></table></figure></p>\n<p>, Hexo would convert it to a symbol like this<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br></pre></td></tr></table></figure></p>\n<p>You would say that this is also an apotrophe, but it really looks UNBEARABLE in the articles. It&#39;s been a problem bothering me for more than a month.(I&#39;m not saying that this is the reason for not updating my blog, but I don&#39;t mind if you think so!)</p>","more":"<p><img src=\"/images/smartypants/apostrophe.png\" alt=\"apostrophe\"></p>\n<p>Therefore, I Googled about this problem and tried to find other victims. According to their sharings, this problem is caused by <em>marked</em> -- the default markdown renderer of Hexo.The <em>&quot;smatrypants&quot;</em> function of marked was turned on by default.</p>\n<p>Now take a look at the introduction of <em>smartypants</em> on the <em>hexo-renderer-marked</em> page:</p>\n<blockquote>\n<p><em>smartypants</em> - Use &quot;smart&quot; typograhic punctuation for things like quotes and dashes.</p>\n</blockquote>\n<p>C&#39;mon, seriously? </p>\n<p>There are a few bloggers who solved this by adding the code below to the _config.yml file in the blog directory. </p>\n<figure class=\"highlight yml\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"attr\">mark:</span></span><br><span class=\"line\"><span class=\"attr\">  smartypants:</span> <span class=\"literal\">false</span></span><br></pre></td></tr></table></figure>\n<p>This worked for most victims (perhaps all of them), but not for me. I have no idea why those config wasn&#39;t working, so if anyone finds out the reason, please contact me by e-mail.</p>\n<p>If you&#39;re sure <em>smartypants</em> is causing the problem, and the solution above didn&#39;t work for you either, maybe you can try my solution.</p>\n<p>Since <em>hexo-renderer-marked</em> is installed in the blog&#39;s <em>node_modules</em> directory(may also be in your Node.js directory if installed globally), isn&#39;t it possible that we change its own configurations? I looked at the <em>index.js</em> file in the <em>node_modules/hexo-renderer-marked/</em> directory. There you are, smartypants!</p>\n<figure class=\"highlight javascript\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo.config.marked = assign(&#123;</span><br><span class=\"line\">  gfm: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  pedantic: <span class=\"literal\">false</span>,</span><br><span class=\"line\">  sanitize: <span class=\"literal\">false</span>,</span><br><span class=\"line\">  tables: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  breaks: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  smartLists: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  smartypants: <span class=\"literal\">true</span>,</span><br><span class=\"line\">  modifyAnchors: <span class=\"string\">''</span>,</span><br><span class=\"line\">  autolink: <span class=\"literal\">true</span></span><br><span class=\"line\">&#125;, hexo.config.marked);</span><br></pre></td></tr></table></figure>\n<p>Now you know what to do.</p>\n<p>Aaaaaaaaaaaaaaaand many thanks to Xizi Wu, the artist of my new avatar! I love it!<br><img src=\"/images/long_nobg.png\" alt=\"Harper Long by Xizi Wu\"></p>"},{"title":"A Review of VGG net - Very Deep Convolutional Neural Networks","date":"2018-04-21T07:15:55.000Z","_content":"\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n## 0 Introduction \nConvolutional neural networks(CNN) have enjoyed great success in computer vision research fields in the past few years. A number of attempts are made based on the original CNN architecture to improve its accuracy and performance. In 2014, Karen Simonyan et al. did an investigation on the effect of depth on CNNs' accuracy in large-scale image recognition (thus also proposing a series of very deep CNNs which are usually called VGG nets). The result confirmed the importance of CNN depth in visual representations.\n\n## 1 Background: VGG net's ancestors\nBefore introducing VGG net, let's take a glance at prior convolutional neural networks. \n\n### 1.1 LeNet: The Origin\nBasic neural network structures(for example, multi-layer perceptron) learn patterns on 1D vectors, which cannot cope with 2D features in images well. In 1986, Lecun et al. proposed a convolution network model called LeNet-5. Its structure is fairly simple: two convolution layers, two subsampling layers and a few fully connected layers. This network was used to solve a number recognition problem. (If you need to learn more about the convolution operation, please refer to Google or *Digital Image Processing* by Rafael C. Gonzalez)\n\n<!-- more -->\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/lenet.png\" width=\"80%\" height=\"60%\" alt=\"LeNet\">\n\n    Fig. 1 Architecture of LeNet\n\n</div>\n\n### 1.2 AlexNet: The Powerful Convolution\nIn 2012, Alex Krizhevsky et al. won the first place in ILSVRC-2012(ImageNet Large-Scale Visual Recognition Challenge 2012) and achieved the highest top-5 error rate of 15.3% with a convolutional network model, while the second-best entry only achieved 26.2%. The network, namely AlexNet, was trained on two GTX580 3GB GPUs in parallel. Since a single GTX580 GPU has only 3GB memory, the maximum size of networks is limited. This model proved the effectiveness of CNNs under complicated circumstances and the power of GPUs. So what if the network can go deeper? Will the top-5 error rate get even lower?\n\n<div align=\"center\" class=\"figure\">\n![AlexNet](/images/vgg/alexnet.png)\n\nFig. 2 Architecture of AlexNet\n</div>\n\n## 2 Main Contributions of VGG Nets\nHere comes our hero - VGG nets. By the way, VGG is not the name of the network, but the name of the authors' group - *Visual Geometry Group*, from Department of Engineering Science, University of Oxford. The networks they proposed were therefore named after the group. The main contributions of VGG nets are: 1. more but smaller convolution filters; 2. great depth of networks.\n\n### 2.1 Stacks of Smaller Convolution Filters\nRather than using relatively large receptive fields in the first convolution layers, Simonyan et al. selected very small 3x3 receptive fields throughout the whole net, which are convolved with the input at every pixel with a stride of 1. As is shown in the figures below, a stack of two 3x3 convolution layers has an effective receptive field of 5x5. We can also conclude that a stack of three 3x3 convolution filters has an effective receptive field of 7x7.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/conv1.png\" width=\"50%\" height=\"50%\" alt=\"Conv5x5\">\n\n    Fig. 3 A convolution layer with one 5x5 conv. filter has a receptive field of 5x5\n\n    <img src=\"/images/vgg/conv2.png\" width=\"60%\" height=\"60%\" alt=\"Conv3x3x2\">\n\n    Fig. 4 A convolution layer stack with two 3x3 conv. filter also has a effective receptive field of 5x5\n\n</div>\n\nNow that we're clear that stacks of small-kernel convolution layers have equal sized receptive fields, why are they the better choice? Well, the first advantage is incorporating more rectification layers instead of a single one, since every convolution layer includes an activation function(usually ReLU). More rectification brings more non-linearity, and more non-linearity makes the decision function more discriminative and fit better. Also, when the receptive field isn't too large, a stack of 3x3 convolution filters have fewer parameters to train. Assuming the number of input and output channels of a convolution layer stack are equal(let's call it C) and the receptive field is 5x5, we have \\\\(2\\*3\\*3\\*C\\*C=18C^2\\\\) instead of \\\\(5\\*5\\*C\\*C=25C^2\\\\) parameters here. Similarly, when the receptive field is 7x7, we have \\\\(3\\*3\\*3\\*C\\*C=27C^2\\\\) instead of \\\\(7\\*7\\*C\\*C=49C^2\\\\). When the field gets even larger? A function with \\\\(O(n)\\\\) complexity only has greater advantage against an \\\\(O(n^2)\\\\) when \\\\(n\\\\) grows.\n\n### 2.2 Deep Dark Fantasy\nCliches time. Just like any blogger mentioning VGG nets would do, here are the network structures proposed by Simonyan et al.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/vggnets.png\" width=\"60%\" height=\"60%\" alt=\"VGG Nets\">\n\n    Table. 1 VGG nets of various depths\n\n</div>\n\nLook at the table column-by-column. Each column(A, A-LRN, B, C, D, E) corresponds to one network structure. As you can see, their networks grew from 11 layers(in net A) to 19 layers(in net E). Each time something is added to the previous net, it would appear bold. Clearly, LRN(Local Response Normalization) didn't work well in this case(actually, A-LRN net performed worse than A, while consuming much more memory and computation time), and was thus removed. \n\nWhat's worth mentioning are the 1x1 convolution layers appearing in network C. This is a way to increase non-linearity(also by introducing activation functions) of the decision function while also keeping the size of the receptive fields unchanged.\n\n## 3 Training & Evaluation\n\nBad initialization could stall learning due to the instability of gradient deep networks. Therefore, the authors first trained the network A, which is shallow enough to be trained with random initialization. Then, the next networks (B to E) are initialized with the pre-trained models, and only weights of the new layers are randomly initialized.\n\nIn spite of the larger number of parameters and the greater depth of our nets compared to AlexNet, the nets required less epochs to converge due to the implicit regularization imposed by greater depth, smaller convolution filter sizes and the pre-initialization of certain layers. They also generalize well to other datasets, achieving state-of-the-art performances. Results of VGG nets in comparison against other models in ILSVRC are shown in the table below.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/VGG-performance-comparison.png\" width=\"70%\" height=\"70%\" alt=\"VGG net results\">\n    \n    Table. 2 VGG net performance, in comparison with the state of the art in ILSVRC classification\n\n</div>\n\nIn conclusion, the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. \n\nYou might ask: Why not even deeper, with more powerful GPUs(the authors used Titan Black), we can absolutely train deeper networks that perform better! Not exactly. Problems arose as the networks get too deep, and this is where ResNet comes in.\n\n## References\n\n[1] [Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.](https://arxiv.org/pdf/1409.1556/)","source":"_posts/vgg.md","raw":"---\ntitle: A Review of VGG net - Very Deep Convolutional Neural Networks\ndate: 2018-04-21 16:15:55\ntags: [Deep Learning, Computer Vision]\n---\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n## 0 Introduction \nConvolutional neural networks(CNN) have enjoyed great success in computer vision research fields in the past few years. A number of attempts are made based on the original CNN architecture to improve its accuracy and performance. In 2014, Karen Simonyan et al. did an investigation on the effect of depth on CNNs' accuracy in large-scale image recognition (thus also proposing a series of very deep CNNs which are usually called VGG nets). The result confirmed the importance of CNN depth in visual representations.\n\n## 1 Background: VGG net's ancestors\nBefore introducing VGG net, let's take a glance at prior convolutional neural networks. \n\n### 1.1 LeNet: The Origin\nBasic neural network structures(for example, multi-layer perceptron) learn patterns on 1D vectors, which cannot cope with 2D features in images well. In 1986, Lecun et al. proposed a convolution network model called LeNet-5. Its structure is fairly simple: two convolution layers, two subsampling layers and a few fully connected layers. This network was used to solve a number recognition problem. (If you need to learn more about the convolution operation, please refer to Google or *Digital Image Processing* by Rafael C. Gonzalez)\n\n<!-- more -->\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/lenet.png\" width=\"80%\" height=\"60%\" alt=\"LeNet\">\n\n    Fig. 1 Architecture of LeNet\n\n</div>\n\n### 1.2 AlexNet: The Powerful Convolution\nIn 2012, Alex Krizhevsky et al. won the first place in ILSVRC-2012(ImageNet Large-Scale Visual Recognition Challenge 2012) and achieved the highest top-5 error rate of 15.3% with a convolutional network model, while the second-best entry only achieved 26.2%. The network, namely AlexNet, was trained on two GTX580 3GB GPUs in parallel. Since a single GTX580 GPU has only 3GB memory, the maximum size of networks is limited. This model proved the effectiveness of CNNs under complicated circumstances and the power of GPUs. So what if the network can go deeper? Will the top-5 error rate get even lower?\n\n<div align=\"center\" class=\"figure\">\n![AlexNet](/images/vgg/alexnet.png)\n\nFig. 2 Architecture of AlexNet\n</div>\n\n## 2 Main Contributions of VGG Nets\nHere comes our hero - VGG nets. By the way, VGG is not the name of the network, but the name of the authors' group - *Visual Geometry Group*, from Department of Engineering Science, University of Oxford. The networks they proposed were therefore named after the group. The main contributions of VGG nets are: 1. more but smaller convolution filters; 2. great depth of networks.\n\n### 2.1 Stacks of Smaller Convolution Filters\nRather than using relatively large receptive fields in the first convolution layers, Simonyan et al. selected very small 3x3 receptive fields throughout the whole net, which are convolved with the input at every pixel with a stride of 1. As is shown in the figures below, a stack of two 3x3 convolution layers has an effective receptive field of 5x5. We can also conclude that a stack of three 3x3 convolution filters has an effective receptive field of 7x7.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/conv1.png\" width=\"50%\" height=\"50%\" alt=\"Conv5x5\">\n\n    Fig. 3 A convolution layer with one 5x5 conv. filter has a receptive field of 5x5\n\n    <img src=\"/images/vgg/conv2.png\" width=\"60%\" height=\"60%\" alt=\"Conv3x3x2\">\n\n    Fig. 4 A convolution layer stack with two 3x3 conv. filter also has a effective receptive field of 5x5\n\n</div>\n\nNow that we're clear that stacks of small-kernel convolution layers have equal sized receptive fields, why are they the better choice? Well, the first advantage is incorporating more rectification layers instead of a single one, since every convolution layer includes an activation function(usually ReLU). More rectification brings more non-linearity, and more non-linearity makes the decision function more discriminative and fit better. Also, when the receptive field isn't too large, a stack of 3x3 convolution filters have fewer parameters to train. Assuming the number of input and output channels of a convolution layer stack are equal(let's call it C) and the receptive field is 5x5, we have \\\\(2\\*3\\*3\\*C\\*C=18C^2\\\\) instead of \\\\(5\\*5\\*C\\*C=25C^2\\\\) parameters here. Similarly, when the receptive field is 7x7, we have \\\\(3\\*3\\*3\\*C\\*C=27C^2\\\\) instead of \\\\(7\\*7\\*C\\*C=49C^2\\\\). When the field gets even larger? A function with \\\\(O(n)\\\\) complexity only has greater advantage against an \\\\(O(n^2)\\\\) when \\\\(n\\\\) grows.\n\n### 2.2 Deep Dark Fantasy\nCliches time. Just like any blogger mentioning VGG nets would do, here are the network structures proposed by Simonyan et al.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/vggnets.png\" width=\"60%\" height=\"60%\" alt=\"VGG Nets\">\n\n    Table. 1 VGG nets of various depths\n\n</div>\n\nLook at the table column-by-column. Each column(A, A-LRN, B, C, D, E) corresponds to one network structure. As you can see, their networks grew from 11 layers(in net A) to 19 layers(in net E). Each time something is added to the previous net, it would appear bold. Clearly, LRN(Local Response Normalization) didn't work well in this case(actually, A-LRN net performed worse than A, while consuming much more memory and computation time), and was thus removed. \n\nWhat's worth mentioning are the 1x1 convolution layers appearing in network C. This is a way to increase non-linearity(also by introducing activation functions) of the decision function while also keeping the size of the receptive fields unchanged.\n\n## 3 Training & Evaluation\n\nBad initialization could stall learning due to the instability of gradient deep networks. Therefore, the authors first trained the network A, which is shallow enough to be trained with random initialization. Then, the next networks (B to E) are initialized with the pre-trained models, and only weights of the new layers are randomly initialized.\n\nIn spite of the larger number of parameters and the greater depth of our nets compared to AlexNet, the nets required less epochs to converge due to the implicit regularization imposed by greater depth, smaller convolution filter sizes and the pre-initialization of certain layers. They also generalize well to other datasets, achieving state-of-the-art performances. Results of VGG nets in comparison against other models in ILSVRC are shown in the table below.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/vgg/VGG-performance-comparison.png\" width=\"70%\" height=\"70%\" alt=\"VGG net results\">\n    \n    Table. 2 VGG net performance, in comparison with the state of the art in ILSVRC classification\n\n</div>\n\nIn conclusion, the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. \n\nYou might ask: Why not even deeper, with more powerful GPUs(the authors used Titan Black), we can absolutely train deeper networks that perform better! Not exactly. Problems arose as the networks get too deep, and this is where ResNet comes in.\n\n## References\n\n[1] [Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.](https://arxiv.org/pdf/1409.1556/)","slug":"vgg","published":1,"updated":"2018-11-18T05:44:45.857Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomm0o0q0006wgwndyv0t8dm","content":"<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n<h2 id=\"0-Introduction\"><a href=\"#0-Introduction\" class=\"headerlink\" title=\"0 Introduction\"></a>0 Introduction</h2><p>Convolutional neural networks(CNN) have enjoyed great success in computer vision research fields in the past few years. A number of attempts are made based on the original CNN architecture to improve its accuracy and performance. In 2014, Karen Simonyan et al. did an investigation on the effect of depth on CNNs&#39; accuracy in large-scale image recognition (thus also proposing a series of very deep CNNs which are usually called VGG nets). The result confirmed the importance of CNN depth in visual representations.</p>\n<h2 id=\"1-Background-VGG-net-39-s-ancestors\"><a href=\"#1-Background-VGG-net-39-s-ancestors\" class=\"headerlink\" title=\"1 Background: VGG net&#39;s ancestors\"></a>1 Background: VGG net&#39;s ancestors</h2><p>Before introducing VGG net, let&#39;s take a glance at prior convolutional neural networks. </p>\n<h3 id=\"1-1-LeNet-The-Origin\"><a href=\"#1-1-LeNet-The-Origin\" class=\"headerlink\" title=\"1.1 LeNet: The Origin\"></a>1.1 LeNet: The Origin</h3><p>Basic neural network structures(for example, multi-layer perceptron) learn patterns on 1D vectors, which cannot cope with 2D features in images well. In 1986, Lecun et al. proposed a convolution network model called LeNet-5. Its structure is fairly simple: two convolution layers, two subsampling layers and a few fully connected layers. This network was used to solve a number recognition problem. (If you need to learn more about the convolution operation, please refer to Google or <em>Digital Image Processing</em> by Rafael C. Gonzalez)</p>\n<a id=\"more\"></a>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/vgg/lenet.png\" width=\"80%\" height=\"60%\" alt=\"LeNet\"><br><br>    Fig. 1 Architecture of LeNet<br><br></div>\n\n<h3 id=\"1-2-AlexNet-The-Powerful-Convolution\"><a href=\"#1-2-AlexNet-The-Powerful-Convolution\" class=\"headerlink\" title=\"1.2 AlexNet: The Powerful Convolution\"></a>1.2 AlexNet: The Powerful Convolution</h3><p>In 2012, Alex Krizhevsky et al. won the first place in ILSVRC-2012(ImageNet Large-Scale Visual Recognition Challenge 2012) and achieved the highest top-5 error rate of 15.3% with a convolutional network model, while the second-best entry only achieved 26.2%. The network, namely AlexNet, was trained on two GTX580 3GB GPUs in parallel. Since a single GTX580 GPU has only 3GB memory, the maximum size of networks is limited. This model proved the effectiveness of CNNs under complicated circumstances and the power of GPUs. So what if the network can go deeper? Will the top-5 error rate get even lower?</p>\n<div align=\"center\" class=\"figure\"><br><img src=\"/images/vgg/alexnet.png\" alt=\"AlexNet\"><br><br>Fig. 2 Architecture of AlexNet<br></div>\n\n<h2 id=\"2-Main-Contributions-of-VGG-Nets\"><a href=\"#2-Main-Contributions-of-VGG-Nets\" class=\"headerlink\" title=\"2 Main Contributions of VGG Nets\"></a>2 Main Contributions of VGG Nets</h2><p>Here comes our hero - VGG nets. By the way, VGG is not the name of the network, but the name of the authors&#39; group - <em>Visual Geometry Group</em>, from Department of Engineering Science, University of Oxford. The networks they proposed were therefore named after the group. The main contributions of VGG nets are: 1. more but smaller convolution filters; 2. great depth of networks.</p>\n<h3 id=\"2-1-Stacks-of-Smaller-Convolution-Filters\"><a href=\"#2-1-Stacks-of-Smaller-Convolution-Filters\" class=\"headerlink\" title=\"2.1 Stacks of Smaller Convolution Filters\"></a>2.1 Stacks of Smaller Convolution Filters</h3><p>Rather than using relatively large receptive fields in the first convolution layers, Simonyan et al. selected very small 3x3 receptive fields throughout the whole net, which are convolved with the input at every pixel with a stride of 1. As is shown in the figures below, a stack of two 3x3 convolution layers has an effective receptive field of 5x5. We can also conclude that a stack of three 3x3 convolution filters has an effective receptive field of 7x7.</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/vgg/conv1.png\" width=\"50%\" height=\"50%\" alt=\"Conv5x5\"><br><br>    Fig. 3 A convolution layer with one 5x5 conv. filter has a receptive field of 5x5<br><br>    <img src=\"/images/vgg/conv2.png\" width=\"60%\" height=\"60%\" alt=\"Conv3x3x2\"><br><br>    Fig. 4 A convolution layer stack with two 3x3 conv. filter also has a effective receptive field of 5x5<br><br></div>\n\n<p>Now that we&#39;re clear that stacks of small-kernel convolution layers have equal sized receptive fields, why are they the better choice? Well, the first advantage is incorporating more rectification layers instead of a single one, since every convolution layer includes an activation function(usually ReLU). More rectification brings more non-linearity, and more non-linearity makes the decision function more discriminative and fit better. Also, when the receptive field isn&#39;t too large, a stack of 3x3 convolution filters have fewer parameters to train. Assuming the number of input and output channels of a convolution layer stack are equal(let&#39;s call it C) and the receptive field is 5x5, we have \\(2*3*3*C*C=18C^2\\) instead of \\(5*5*C*C=25C^2\\) parameters here. Similarly, when the receptive field is 7x7, we have \\(3*3*3*C*C=27C^2\\) instead of \\(7*7*C*C=49C^2\\). When the field gets even larger? A function with \\(O(n)\\) complexity only has greater advantage against an \\(O(n^2)\\) when \\(n\\) grows.</p>\n<h3 id=\"2-2-Deep-Dark-Fantasy\"><a href=\"#2-2-Deep-Dark-Fantasy\" class=\"headerlink\" title=\"2.2 Deep Dark Fantasy\"></a>2.2 Deep Dark Fantasy</h3><p>Cliches time. Just like any blogger mentioning VGG nets would do, here are the network structures proposed by Simonyan et al.</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/vgg/vggnets.png\" width=\"60%\" height=\"60%\" alt=\"VGG Nets\"><br><br>    Table. 1 VGG nets of various depths<br><br></div>\n\n<p>Look at the table column-by-column. Each column(A, A-LRN, B, C, D, E) corresponds to one network structure. As you can see, their networks grew from 11 layers(in net A) to 19 layers(in net E). Each time something is added to the previous net, it would appear bold. Clearly, LRN(Local Response Normalization) didn&#39;t work well in this case(actually, A-LRN net performed worse than A, while consuming much more memory and computation time), and was thus removed. </p>\n<p>What&#39;s worth mentioning are the 1x1 convolution layers appearing in network C. This is a way to increase non-linearity(also by introducing activation functions) of the decision function while also keeping the size of the receptive fields unchanged.</p>\n<h2 id=\"3-Training-amp-Evaluation\"><a href=\"#3-Training-amp-Evaluation\" class=\"headerlink\" title=\"3 Training &amp; Evaluation\"></a>3 Training &amp; Evaluation</h2><p>Bad initialization could stall learning due to the instability of gradient deep networks. Therefore, the authors first trained the network A, which is shallow enough to be trained with random initialization. Then, the next networks (B to E) are initialized with the pre-trained models, and only weights of the new layers are randomly initialized.</p>\n<p>In spite of the larger number of parameters and the greater depth of our nets compared to AlexNet, the nets required less epochs to converge due to the implicit regularization imposed by greater depth, smaller convolution filter sizes and the pre-initialization of certain layers. They also generalize well to other datasets, achieving state-of-the-art performances. Results of VGG nets in comparison against other models in ILSVRC are shown in the table below.</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/vgg/VGG-performance-comparison.png\" width=\"70%\" height=\"70%\" alt=\"VGG net results\"><br><br>    Table. 2 VGG net performance, in comparison with the state of the art in ILSVRC classification<br><br></div>\n\n<p>In conclusion, the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. </p>\n<p>You might ask: Why not even deeper, with more powerful GPUs(the authors used Titan Black), we can absolutely train deeper networks that perform better! Not exactly. Problems arose as the networks get too deep, and this is where ResNet comes in.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><p>[1] <a href=\"https://arxiv.org/pdf/1409.1556/\" target=\"_blank\" rel=\"noopener\">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.</a></p>\n","site":{"data":{}},"excerpt":"<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n<h2 id=\"0-Introduction\"><a href=\"#0-Introduction\" class=\"headerlink\" title=\"0 Introduction\"></a>0 Introduction</h2><p>Convolutional neural networks(CNN) have enjoyed great success in computer vision research fields in the past few years. A number of attempts are made based on the original CNN architecture to improve its accuracy and performance. In 2014, Karen Simonyan et al. did an investigation on the effect of depth on CNNs&#39; accuracy in large-scale image recognition (thus also proposing a series of very deep CNNs which are usually called VGG nets). The result confirmed the importance of CNN depth in visual representations.</p>\n<h2 id=\"1-Background-VGG-net-39-s-ancestors\"><a href=\"#1-Background-VGG-net-39-s-ancestors\" class=\"headerlink\" title=\"1 Background: VGG net&#39;s ancestors\"></a>1 Background: VGG net&#39;s ancestors</h2><p>Before introducing VGG net, let&#39;s take a glance at prior convolutional neural networks. </p>\n<h3 id=\"1-1-LeNet-The-Origin\"><a href=\"#1-1-LeNet-The-Origin\" class=\"headerlink\" title=\"1.1 LeNet: The Origin\"></a>1.1 LeNet: The Origin</h3><p>Basic neural network structures(for example, multi-layer perceptron) learn patterns on 1D vectors, which cannot cope with 2D features in images well. In 1986, Lecun et al. proposed a convolution network model called LeNet-5. Its structure is fairly simple: two convolution layers, two subsampling layers and a few fully connected layers. This network was used to solve a number recognition problem. (If you need to learn more about the convolution operation, please refer to Google or <em>Digital Image Processing</em> by Rafael C. Gonzalez)</p>","more":"<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/vgg/lenet.png\" width=\"80%\" height=\"60%\" alt=\"LeNet\"><br><br>    Fig. 1 Architecture of LeNet<br><br></div>\n\n<h3 id=\"1-2-AlexNet-The-Powerful-Convolution\"><a href=\"#1-2-AlexNet-The-Powerful-Convolution\" class=\"headerlink\" title=\"1.2 AlexNet: The Powerful Convolution\"></a>1.2 AlexNet: The Powerful Convolution</h3><p>In 2012, Alex Krizhevsky et al. won the first place in ILSVRC-2012(ImageNet Large-Scale Visual Recognition Challenge 2012) and achieved the highest top-5 error rate of 15.3% with a convolutional network model, while the second-best entry only achieved 26.2%. The network, namely AlexNet, was trained on two GTX580 3GB GPUs in parallel. Since a single GTX580 GPU has only 3GB memory, the maximum size of networks is limited. This model proved the effectiveness of CNNs under complicated circumstances and the power of GPUs. So what if the network can go deeper? Will the top-5 error rate get even lower?</p>\n<div align=\"center\" class=\"figure\"><br><img src=\"/images/vgg/alexnet.png\" alt=\"AlexNet\"><br><br>Fig. 2 Architecture of AlexNet<br></div>\n\n<h2 id=\"2-Main-Contributions-of-VGG-Nets\"><a href=\"#2-Main-Contributions-of-VGG-Nets\" class=\"headerlink\" title=\"2 Main Contributions of VGG Nets\"></a>2 Main Contributions of VGG Nets</h2><p>Here comes our hero - VGG nets. By the way, VGG is not the name of the network, but the name of the authors&#39; group - <em>Visual Geometry Group</em>, from Department of Engineering Science, University of Oxford. The networks they proposed were therefore named after the group. The main contributions of VGG nets are: 1. more but smaller convolution filters; 2. great depth of networks.</p>\n<h3 id=\"2-1-Stacks-of-Smaller-Convolution-Filters\"><a href=\"#2-1-Stacks-of-Smaller-Convolution-Filters\" class=\"headerlink\" title=\"2.1 Stacks of Smaller Convolution Filters\"></a>2.1 Stacks of Smaller Convolution Filters</h3><p>Rather than using relatively large receptive fields in the first convolution layers, Simonyan et al. selected very small 3x3 receptive fields throughout the whole net, which are convolved with the input at every pixel with a stride of 1. As is shown in the figures below, a stack of two 3x3 convolution layers has an effective receptive field of 5x5. We can also conclude that a stack of three 3x3 convolution filters has an effective receptive field of 7x7.</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/vgg/conv1.png\" width=\"50%\" height=\"50%\" alt=\"Conv5x5\"><br><br>    Fig. 3 A convolution layer with one 5x5 conv. filter has a receptive field of 5x5<br><br>    <img src=\"/images/vgg/conv2.png\" width=\"60%\" height=\"60%\" alt=\"Conv3x3x2\"><br><br>    Fig. 4 A convolution layer stack with two 3x3 conv. filter also has a effective receptive field of 5x5<br><br></div>\n\n<p>Now that we&#39;re clear that stacks of small-kernel convolution layers have equal sized receptive fields, why are they the better choice? Well, the first advantage is incorporating more rectification layers instead of a single one, since every convolution layer includes an activation function(usually ReLU). More rectification brings more non-linearity, and more non-linearity makes the decision function more discriminative and fit better. Also, when the receptive field isn&#39;t too large, a stack of 3x3 convolution filters have fewer parameters to train. Assuming the number of input and output channels of a convolution layer stack are equal(let&#39;s call it C) and the receptive field is 5x5, we have \\(2*3*3*C*C=18C^2\\) instead of \\(5*5*C*C=25C^2\\) parameters here. Similarly, when the receptive field is 7x7, we have \\(3*3*3*C*C=27C^2\\) instead of \\(7*7*C*C=49C^2\\). When the field gets even larger? A function with \\(O(n)\\) complexity only has greater advantage against an \\(O(n^2)\\) when \\(n\\) grows.</p>\n<h3 id=\"2-2-Deep-Dark-Fantasy\"><a href=\"#2-2-Deep-Dark-Fantasy\" class=\"headerlink\" title=\"2.2 Deep Dark Fantasy\"></a>2.2 Deep Dark Fantasy</h3><p>Cliches time. Just like any blogger mentioning VGG nets would do, here are the network structures proposed by Simonyan et al.</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/vgg/vggnets.png\" width=\"60%\" height=\"60%\" alt=\"VGG Nets\"><br><br>    Table. 1 VGG nets of various depths<br><br></div>\n\n<p>Look at the table column-by-column. Each column(A, A-LRN, B, C, D, E) corresponds to one network structure. As you can see, their networks grew from 11 layers(in net A) to 19 layers(in net E). Each time something is added to the previous net, it would appear bold. Clearly, LRN(Local Response Normalization) didn&#39;t work well in this case(actually, A-LRN net performed worse than A, while consuming much more memory and computation time), and was thus removed. </p>\n<p>What&#39;s worth mentioning are the 1x1 convolution layers appearing in network C. This is a way to increase non-linearity(also by introducing activation functions) of the decision function while also keeping the size of the receptive fields unchanged.</p>\n<h2 id=\"3-Training-amp-Evaluation\"><a href=\"#3-Training-amp-Evaluation\" class=\"headerlink\" title=\"3 Training &amp; Evaluation\"></a>3 Training &amp; Evaluation</h2><p>Bad initialization could stall learning due to the instability of gradient deep networks. Therefore, the authors first trained the network A, which is shallow enough to be trained with random initialization. Then, the next networks (B to E) are initialized with the pre-trained models, and only weights of the new layers are randomly initialized.</p>\n<p>In spite of the larger number of parameters and the greater depth of our nets compared to AlexNet, the nets required less epochs to converge due to the implicit regularization imposed by greater depth, smaller convolution filter sizes and the pre-initialization of certain layers. They also generalize well to other datasets, achieving state-of-the-art performances. Results of VGG nets in comparison against other models in ILSVRC are shown in the table below.</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/vgg/VGG-performance-comparison.png\" width=\"70%\" height=\"70%\" alt=\"VGG net results\"><br><br>    Table. 2 VGG net performance, in comparison with the state of the art in ILSVRC classification<br><br></div>\n\n<p>In conclusion, the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. </p>\n<p>You might ask: Why not even deeper, with more powerful GPUs(the authors used Titan Black), we can absolutely train deeper networks that perform better! Not exactly. Problems arose as the networks get too deep, and this is where ResNet comes in.</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><p>[1] <a href=\"https://arxiv.org/pdf/1409.1556/\" target=\"_blank\" rel=\"noopener\">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.</a></p>"},{"title":"[MineSweeping] The Long Struggle of DensePose Installation","date":"2018-11-18T07:59:43.000Z","_content":"## Motivation\nFrom now on, I decide to keep a record (tag: MineSweeping) of the issues I meet while working with environments and also their solutions. \n\n\nDoing configurations in order to run others' code may be a difficult task, and is sometimes depressing, \nsince various issues could arise, and the it's impossible for the authors to keep providing solutions for every user in the community.\nWhat's worse, after fixing some problems with a lot of struggle, one may have to waste the same amount of time on the same issue\nthe next time he/she run it again.\nThat's why I decide to keep this record: to avoid wasting time twice, while also helping others deal with problems if possible.\n\n\nDensePose is a great work in real-time human pose estimation, which is based on Caffe2 and Detectron framework. \nIt extracts dense human body 3D surface based on RGB images.\nThe installation instructions are provided [here](https://github.com/facebookresearch/DensePose/blob/master/INSTALL.md).\n\nDuring the installation process, these are the problems that took me some time to tackle. I spent on week to finally figure out solutions to all the issues. So lucky of me not to give up too early...\n\n![Greetings from Facebook AI Research](/images/densepose-ms/facebook.jpg)\n\n<!-- more -->\n\n## Environment\n- System: Ubuntu 18.04\n- Linux kernel: 4.15.0-29-generic\n- Graphics card: NVIDIA GeForce 1080Ti\n- Graphics driver: 410.48\n- CUDA: 10.0.130\n- cuDNN: 7.3.1\n- Caffe2: Built from source\n- Python: 2.7.15, based on Anaconda 4.5.11\n\n## Problems & Solutions\n### Caffe2 module not found\n#### Details\nOccurred when running `make`.\n\nMain error message:\n```\nCould not find a package configuration file provided by \"Caffe2\" with any  \nof the following names: \n    Caffe2Config.cmake \n    caffe2-config.cmake \n```\n#### Cause\nCaffe2 build path isn't known by CMake.\n#### Solution\nAdded one line in the beginning of CMakeLists.txt: \n```\nset(Caffe2_DIR \"/path/to/pytorch/torch/share/cmake/Caffe2/\") \n```\n(Note: `set(Caffe2_DIR \"/path/to/pytorch/build/\")` can also fix this issue but may cause other issues.) \n\n### Detectron ops lib not found\n#### Details\nOccurred when running `python2 $DENSEPOSE/detectron/tests/test_spatial_narrow_as_op.py` after `make`.\n\nMain error message:\n```\nDetectron ops lib not found; make sure that your Caffe2 version includes Detectron module. \n```\n#### Cause\nSeems that the Python part of DensePose couldn't recognize Caffe2.\n#### Solution\nAdd `/path/to/pytorch` to `PYTHONPATH` environment variable. \nCould be added by directly `export PYTHONPATH=$PYTHONPATH:/path/to/pytorch` instruction or by adding this line to `~/.bashrc`.\nRemember to run `source ~/.bashrc` after the modification.\n\n### *.cmake files not found & Unknown CMake command \"caffe2_interface_library\"\n#### Details\nOccurred when running `make ops`.\n\nMain error message: \n```\nCMake Error at /path/to/pytorch/build/Caffe2Config.cmake:14 (include):\n  include could not find load file:\n\n    /path/to/pytorch/build/public/utils.cmake\n    /path/to/pytorch/build/public/threads.cmake\n    /path/to/pytorch/build/public/cuda.cmake\n    /path/to/pytorch/build/public/mkl.cmake\n    /path/to/pytorch/build/Caffe2Targets.cmake\n\nCall Stack (most recent call first):\n  CMakeLists.txt:8 (find_package)\n\nCMake Error at /path/to/pytorch/build/Caffe2Config.cmake:117 (caffe2_interface_library):\n  Unknown CMake command \"caffe2_interface_library\".\nCall Stack (most recent call first):\n  CMakeLists.txt:8 (find_package)  \n```\n(Several `*.cmake` files, I only showed a few.)\n#### Cause\nThese files are not in the `pytorch/build` directory. By searching, I found that they are in the `pytorch/torch/share/cmake/Caffe2` directory.\n#### Solution\nAdded one line in the beginning of CMakeLists.txt: \n```\nset(Caffe2_DIR \"/path/to/pytorch/torch/share/cmake/Caffe2/\")\n```\n\n### \"context_gpu.h\" not found.\n#### Details\nOccurred when running `make ops`.\n\nI forgot to record the error messages, but it should be obvious that some header files(not just `context_gpu.h`) are missing.\n#### Cause\nThis time it's the include path not recognized...\n#### Solution\nAdded one line in the beginning of CMakeLists.txt: \n```\ninclude_directories(\"/path/to/pytorch/torch/lib/include\")\n```\n\n### \"mkl_cblas.h\" not found.\n#### Details\nOccurred when running `make ops`.\n\nI forgot to record the error messages, but it should be obvious too.\n#### Cause\nIntel Math Kernel Library was turned on but not found. (Why is it enabled when I didn't even install it???)\n#### Solution\nInstall Intel Math Kernel Library [here](https://software.intel.com/en-us/mkl/choose-download/linux) and add `/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/include` to `C_PATH` environment variable. The exact path may vary according to the MKL version and your configuration.\nMaybe try `find / -name mkl_cblas.h` to make sure of its location after the installation.\n\n### GetSingleArgument<float> is not a member of caffe2::PoolPointsInterpOp<T, Context>\n#### Details\nOccurred when running `make ops`.\nMain error message:\n```\n/path/to/pytorch/caffe2/operators/accumulate_op.h: In constructor caffe2::AccumulateOp<T, Context>::AccumulateOp(const caffe2::OperatorDef&, caffe2::Workspace*):\n/path/to/pytorch/caffe2/operators/accumulate_op.h:13:187: error: GetSingleArgument<float> is not a member of caffe2::AccumulateOp<T, Context>\n   AccumulateOp(const OperatorDef& operator_def, Workspace* ws)\n                                                                                                                                                                                           ^                        \n/path/to/pytorch/caffe2/operators/elementwise_ops.h: In constructor caffe2::BinaryElementwiseWithArgsOp<InputTypes, Context, Functor, OutputTypeMap>::BinaryElementwiseWithArgsOp(const caffe2::OperatorDef&, caffe2::Workspace*):\n/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:189: error: GetSingleArgument<bool> is not a member of caffe2::BinaryElementwiseWithArgsOp<InputTypes, Context, Functor, OutputTypeMap>\n   BinaryElementwiseWithArgsOp(const OperatorDef& operator_def, Workspace* ws)\n                                                                                                                                                                                             ^                       \n/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:272: error: GetSingleArgument<int> is not a member of caffe2::BinaryElementwiseWithArgsOp<InputTypes, Context, Functor, OutputTypeMap>\n   BinaryElementwiseWithArgsOp(const OperatorDef& operator_def, Workspace* ws)\n                                                                                                                                                                                                                                                                                ^                      \n/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:350: error: GetSingleArgument<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > is not a member of caffe2::BinaryElementwiseWithArgsOp<InputTypes, Context, Functor, OutputTypeMap>\n   BinaryElementwiseWithArgsOp(const OperatorDef& operator_def, Workspace* ws)\n```\n#### Cause\nI'm not sure. Could be that `GetSingleArgument()` is defined elsewhere?\n#### Solution\nModify `/path/to/densepose/detectron/ops/pool_points_interp.h`. Change `OperatorBase::GetSingleArgument<float> ` to `this->template GetSingleArgument<float>`\n\n(Thanks to badpx@Github: https://github.com/facebookresearch/DensePose/pull/137/commits/51389c6a02173a25e9429825db452beb5e1cf3be) \n\n### Undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE \n#### Details\nOccurred when running `python detectron/tests/test_zero_even_op.py`.\n\nMain error message:\n```\nOSError: /path/to/densepose/build/libcaffe2_detectron_custom_ops_gpu.so: undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE \n```\n#### Cause\nWTF is this!???\nAs can be seen, this symbol has something to do with Google, and protobuf.\nI guess this is caused by a different protobuf version.\nGood news is that a proper version of protobuf was also built with Caffe2, so why not tell this to DensePose?\n\n#### Solution\nIn `/path/to/densepose/CMakeLists.txt`, Add a few lines in the beginning: \n```\nadd_library(libprotobuf STATIC IMPORTED) \n\nset(PROTOBUF_LIB \"/path/to/pytorch/torch/lib/libprotobuf.a\") \n\nset_property(TARGET libprotobuf PROPERTY IMPORTED_LOCATION \"${PROTOBUF_LIB}\") \n```\nYou can find two `target_link_libraries` lines in this file(they are not adjacent):\n```\ntarget_link_libraries(caffe2_detectron_custom_ops caffe2_library) \ntarget_link_libraries(caffe2_detectron_custom_ops_gpu caffe2_gpu_library) \n```\nEdit the two lines, adding a \"libprotobuf\" at the end to each of them: \n```\ntarget_link_libraries(caffe2_detectron_custom_ops caffe2_library libprotobuf) \ntarget_link_libraries(caffe2_detectron_custom_ops_gpu caffe2_gpu_library libprotobuf) \n```\nThen run `make ops` again, and `python detectron/tests/test_zero_even_op.py` again.\n\n(Thanks to hyounsamk@Github: https://github.com/facebookresearch/DensePose/issues/119)\n\nAfter fixing this issue, my DensePose passed tests and was running flawlessly. If any more issues remain, don't hesitate to comment here~","source":"_drafts/densepose-minesweeping.md","raw":"---\ntitle: \"[MineSweeping] The Long Struggle of DensePose Installation\"\ntags:\n  - Deep Learning\n  - MineSweeping\ndate: 2018-11-18 16:59:43\n---\n## Motivation\nFrom now on, I decide to keep a record (tag: MineSweeping) of the issues I meet while working with environments and also their solutions. \n\n\nDoing configurations in order to run others' code may be a difficult task, and is sometimes depressing, \nsince various issues could arise, and the it's impossible for the authors to keep providing solutions for every user in the community.\nWhat's worse, after fixing some problems with a lot of struggle, one may have to waste the same amount of time on the same issue\nthe next time he/she run it again.\nThat's why I decide to keep this record: to avoid wasting time twice, while also helping others deal with problems if possible.\n\n\nDensePose is a great work in real-time human pose estimation, which is based on Caffe2 and Detectron framework. \nIt extracts dense human body 3D surface based on RGB images.\nThe installation instructions are provided [here](https://github.com/facebookresearch/DensePose/blob/master/INSTALL.md).\n\nDuring the installation process, these are the problems that took me some time to tackle. I spent on week to finally figure out solutions to all the issues. So lucky of me not to give up too early...\n\n![Greetings from Facebook AI Research](/images/densepose-ms/facebook.jpg)\n\n<!-- more -->\n\n## Environment\n- System: Ubuntu 18.04\n- Linux kernel: 4.15.0-29-generic\n- Graphics card: NVIDIA GeForce 1080Ti\n- Graphics driver: 410.48\n- CUDA: 10.0.130\n- cuDNN: 7.3.1\n- Caffe2: Built from source\n- Python: 2.7.15, based on Anaconda 4.5.11\n\n## Problems & Solutions\n### Caffe2 module not found\n#### Details\nOccurred when running `make`.\n\nMain error message:\n```\nCould not find a package configuration file provided by \"Caffe2\" with any  \nof the following names: \n    Caffe2Config.cmake \n    caffe2-config.cmake \n```\n#### Cause\nCaffe2 build path isn't known by CMake.\n#### Solution\nAdded one line in the beginning of CMakeLists.txt: \n```\nset(Caffe2_DIR \"/path/to/pytorch/torch/share/cmake/Caffe2/\") \n```\n(Note: `set(Caffe2_DIR \"/path/to/pytorch/build/\")` can also fix this issue but may cause other issues.) \n\n### Detectron ops lib not found\n#### Details\nOccurred when running `python2 $DENSEPOSE/detectron/tests/test_spatial_narrow_as_op.py` after `make`.\n\nMain error message:\n```\nDetectron ops lib not found; make sure that your Caffe2 version includes Detectron module. \n```\n#### Cause\nSeems that the Python part of DensePose couldn't recognize Caffe2.\n#### Solution\nAdd `/path/to/pytorch` to `PYTHONPATH` environment variable. \nCould be added by directly `export PYTHONPATH=$PYTHONPATH:/path/to/pytorch` instruction or by adding this line to `~/.bashrc`.\nRemember to run `source ~/.bashrc` after the modification.\n\n### *.cmake files not found & Unknown CMake command \"caffe2_interface_library\"\n#### Details\nOccurred when running `make ops`.\n\nMain error message: \n```\nCMake Error at /path/to/pytorch/build/Caffe2Config.cmake:14 (include):\n  include could not find load file:\n\n    /path/to/pytorch/build/public/utils.cmake\n    /path/to/pytorch/build/public/threads.cmake\n    /path/to/pytorch/build/public/cuda.cmake\n    /path/to/pytorch/build/public/mkl.cmake\n    /path/to/pytorch/build/Caffe2Targets.cmake\n\nCall Stack (most recent call first):\n  CMakeLists.txt:8 (find_package)\n\nCMake Error at /path/to/pytorch/build/Caffe2Config.cmake:117 (caffe2_interface_library):\n  Unknown CMake command \"caffe2_interface_library\".\nCall Stack (most recent call first):\n  CMakeLists.txt:8 (find_package)  \n```\n(Several `*.cmake` files, I only showed a few.)\n#### Cause\nThese files are not in the `pytorch/build` directory. By searching, I found that they are in the `pytorch/torch/share/cmake/Caffe2` directory.\n#### Solution\nAdded one line in the beginning of CMakeLists.txt: \n```\nset(Caffe2_DIR \"/path/to/pytorch/torch/share/cmake/Caffe2/\")\n```\n\n### \"context_gpu.h\" not found.\n#### Details\nOccurred when running `make ops`.\n\nI forgot to record the error messages, but it should be obvious that some header files(not just `context_gpu.h`) are missing.\n#### Cause\nThis time it's the include path not recognized...\n#### Solution\nAdded one line in the beginning of CMakeLists.txt: \n```\ninclude_directories(\"/path/to/pytorch/torch/lib/include\")\n```\n\n### \"mkl_cblas.h\" not found.\n#### Details\nOccurred when running `make ops`.\n\nI forgot to record the error messages, but it should be obvious too.\n#### Cause\nIntel Math Kernel Library was turned on but not found. (Why is it enabled when I didn't even install it???)\n#### Solution\nInstall Intel Math Kernel Library [here](https://software.intel.com/en-us/mkl/choose-download/linux) and add `/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/include` to `C_PATH` environment variable. The exact path may vary according to the MKL version and your configuration.\nMaybe try `find / -name mkl_cblas.h` to make sure of its location after the installation.\n\n### GetSingleArgument<float> is not a member of caffe2::PoolPointsInterpOp<T, Context>\n#### Details\nOccurred when running `make ops`.\nMain error message:\n```\n/path/to/pytorch/caffe2/operators/accumulate_op.h: In constructor caffe2::AccumulateOp<T, Context>::AccumulateOp(const caffe2::OperatorDef&, caffe2::Workspace*):\n/path/to/pytorch/caffe2/operators/accumulate_op.h:13:187: error: GetSingleArgument<float> is not a member of caffe2::AccumulateOp<T, Context>\n   AccumulateOp(const OperatorDef& operator_def, Workspace* ws)\n                                                                                                                                                                                           ^                        \n/path/to/pytorch/caffe2/operators/elementwise_ops.h: In constructor caffe2::BinaryElementwiseWithArgsOp<InputTypes, Context, Functor, OutputTypeMap>::BinaryElementwiseWithArgsOp(const caffe2::OperatorDef&, caffe2::Workspace*):\n/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:189: error: GetSingleArgument<bool> is not a member of caffe2::BinaryElementwiseWithArgsOp<InputTypes, Context, Functor, OutputTypeMap>\n   BinaryElementwiseWithArgsOp(const OperatorDef& operator_def, Workspace* ws)\n                                                                                                                                                                                             ^                       \n/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:272: error: GetSingleArgument<int> is not a member of caffe2::BinaryElementwiseWithArgsOp<InputTypes, Context, Functor, OutputTypeMap>\n   BinaryElementwiseWithArgsOp(const OperatorDef& operator_def, Workspace* ws)\n                                                                                                                                                                                                                                                                                ^                      \n/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:350: error: GetSingleArgument<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > is not a member of caffe2::BinaryElementwiseWithArgsOp<InputTypes, Context, Functor, OutputTypeMap>\n   BinaryElementwiseWithArgsOp(const OperatorDef& operator_def, Workspace* ws)\n```\n#### Cause\nI'm not sure. Could be that `GetSingleArgument()` is defined elsewhere?\n#### Solution\nModify `/path/to/densepose/detectron/ops/pool_points_interp.h`. Change `OperatorBase::GetSingleArgument<float> ` to `this->template GetSingleArgument<float>`\n\n(Thanks to badpx@Github: https://github.com/facebookresearch/DensePose/pull/137/commits/51389c6a02173a25e9429825db452beb5e1cf3be) \n\n### Undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE \n#### Details\nOccurred when running `python detectron/tests/test_zero_even_op.py`.\n\nMain error message:\n```\nOSError: /path/to/densepose/build/libcaffe2_detectron_custom_ops_gpu.so: undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE \n```\n#### Cause\nWTF is this!???\nAs can be seen, this symbol has something to do with Google, and protobuf.\nI guess this is caused by a different protobuf version.\nGood news is that a proper version of protobuf was also built with Caffe2, so why not tell this to DensePose?\n\n#### Solution\nIn `/path/to/densepose/CMakeLists.txt`, Add a few lines in the beginning: \n```\nadd_library(libprotobuf STATIC IMPORTED) \n\nset(PROTOBUF_LIB \"/path/to/pytorch/torch/lib/libprotobuf.a\") \n\nset_property(TARGET libprotobuf PROPERTY IMPORTED_LOCATION \"${PROTOBUF_LIB}\") \n```\nYou can find two `target_link_libraries` lines in this file(they are not adjacent):\n```\ntarget_link_libraries(caffe2_detectron_custom_ops caffe2_library) \ntarget_link_libraries(caffe2_detectron_custom_ops_gpu caffe2_gpu_library) \n```\nEdit the two lines, adding a \"libprotobuf\" at the end to each of them: \n```\ntarget_link_libraries(caffe2_detectron_custom_ops caffe2_library libprotobuf) \ntarget_link_libraries(caffe2_detectron_custom_ops_gpu caffe2_gpu_library libprotobuf) \n```\nThen run `make ops` again, and `python detectron/tests/test_zero_even_op.py` again.\n\n(Thanks to hyounsamk@Github: https://github.com/facebookresearch/DensePose/issues/119)\n\nAfter fixing this issue, my DensePose passed tests and was running flawlessly. If any more issues remain, don't hesitate to comment here~","slug":"densepose-minesweeping","published":0,"updated":"2018-11-18T07:59:07.053Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomm0o14000iwgwnlcjpthjj","content":"<h2 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h2><p>From now on, I decide to keep a record (tag: MineSweeping) of the issues I meet while working with environments and also their solutions. </p>\n<p>Doing configurations in order to run others&#39; code may be a difficult task, and is sometimes depressing,<br>since various issues could arise, and the it&#39;s impossible for the authors to keep providing solutions for every user in the community.<br>What&#39;s worse, after fixing some problems with a lot of struggle, one may have to waste the same amount of time on the same issue<br>the next time he/she run it again.<br>That&#39;s why I decide to keep this record: to avoid wasting time twice, while also helping others deal with problems if possible.</p>\n<p>DensePose is a great work in real-time human pose estimation, which is based on Caffe2 and Detectron framework.<br>It extracts dense human body 3D surface based on RGB images.<br>The installation instructions are provided <a href=\"https://github.com/facebookresearch/DensePose/blob/master/INSTALL.md\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<p>During the installation process, these are the problems that took me some time to tackle. I spent on week to finally figure out solutions to all the issues. So lucky of me not to give up too early...</p>\n<p><img src=\"/images/densepose-ms/facebook.jpg\" alt=\"Greetings from Facebook AI Research\"></p>\n<a id=\"more\"></a>\n<h2 id=\"Environment\"><a href=\"#Environment\" class=\"headerlink\" title=\"Environment\"></a>Environment</h2><ul>\n<li>System: Ubuntu 18.04</li>\n<li>Linux kernel: 4.15.0-29-generic</li>\n<li>Graphics card: NVIDIA GeForce 1080Ti</li>\n<li>Graphics driver: 410.48</li>\n<li>CUDA: 10.0.130</li>\n<li>cuDNN: 7.3.1</li>\n<li>Caffe2: Built from source</li>\n<li>Python: 2.7.15, based on Anaconda 4.5.11</li>\n</ul>\n<h2 id=\"Problems-amp-Solutions\"><a href=\"#Problems-amp-Solutions\" class=\"headerlink\" title=\"Problems &amp; Solutions\"></a>Problems &amp; Solutions</h2><h3 id=\"Caffe2-module-not-found\"><a href=\"#Caffe2-module-not-found\" class=\"headerlink\" title=\"Caffe2 module not found\"></a>Caffe2 module not found</h3><h4 id=\"Details\"><a href=\"#Details\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make</code>.</p>\n<p>Main error message:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Could not find a package configuration file provided by &quot;Caffe2&quot; with any  </span><br><span class=\"line\">of the following names: </span><br><span class=\"line\">    Caffe2Config.cmake </span><br><span class=\"line\">    caffe2-config.cmake</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"Cause\"><a href=\"#Cause\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>Caffe2 build path isn&#39;t known by CMake.</p>\n<h4 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(Caffe2_DIR &quot;/path/to/pytorch/torch/share/cmake/Caffe2/&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>(Note: <code>set(Caffe2_DIR &quot;/path/to/pytorch/build/&quot;)</code> can also fix this issue but may cause other issues.) </p>\n<h3 id=\"Detectron-ops-lib-not-found\"><a href=\"#Detectron-ops-lib-not-found\" class=\"headerlink\" title=\"Detectron ops lib not found\"></a>Detectron ops lib not found</h3><h4 id=\"Details-1\"><a href=\"#Details-1\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>python2 $DENSEPOSE/detectron/tests/test_spatial_narrow_as_op.py</code> after <code>make</code>.</p>\n<p>Main error message:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Detectron ops lib not found; make sure that your Caffe2 version includes Detectron module.</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"Cause-1\"><a href=\"#Cause-1\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>Seems that the Python part of DensePose couldn&#39;t recognize Caffe2.</p>\n<h4 id=\"Solution-1\"><a href=\"#Solution-1\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Add <code>/path/to/pytorch</code> to <code>PYTHONPATH</code> environment variable.<br>Could be added by directly <code>export PYTHONPATH=$PYTHONPATH:/path/to/pytorch</code> instruction or by adding this line to <code>~/.bashrc</code>.<br>Remember to run <code>source ~/.bashrc</code> after the modification.</p>\n<h3 id=\"cmake-files-not-found-amp-Unknown-CMake-command-quot-caffe2-interface-library-quot\"><a href=\"#cmake-files-not-found-amp-Unknown-CMake-command-quot-caffe2-interface-library-quot\" class=\"headerlink\" title=\"*.cmake files not found &amp; Unknown CMake command &quot;caffe2_interface_library&quot;\"></a>*.cmake files not found &amp; Unknown CMake command &quot;caffe2_interface_library&quot;</h3><h4 id=\"Details-2\"><a href=\"#Details-2\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p>\n<p>Main error message:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CMake Error at /path/to/pytorch/build/Caffe2Config.cmake:14 (include):</span><br><span class=\"line\">  include could not find load file:</span><br><span class=\"line\"></span><br><span class=\"line\">    /path/to/pytorch/build/public/utils.cmake</span><br><span class=\"line\">    /path/to/pytorch/build/public/threads.cmake</span><br><span class=\"line\">    /path/to/pytorch/build/public/cuda.cmake</span><br><span class=\"line\">    /path/to/pytorch/build/public/mkl.cmake</span><br><span class=\"line\">    /path/to/pytorch/build/Caffe2Targets.cmake</span><br><span class=\"line\"></span><br><span class=\"line\">Call Stack (most recent call first):</span><br><span class=\"line\">  CMakeLists.txt:8 (find_package)</span><br><span class=\"line\"></span><br><span class=\"line\">CMake Error at /path/to/pytorch/build/Caffe2Config.cmake:117 (caffe2_interface_library):</span><br><span class=\"line\">  Unknown CMake command &quot;caffe2_interface_library&quot;.</span><br><span class=\"line\">Call Stack (most recent call first):</span><br><span class=\"line\">  CMakeLists.txt:8 (find_package)</span><br></pre></td></tr></table></figure></p>\n<p>(Several <code>*.cmake</code> files, I only showed a few.)</p>\n<h4 id=\"Cause-2\"><a href=\"#Cause-2\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>These files are not in the <code>pytorch/build</code> directory. By searching, I found that they are in the <code>pytorch/torch/share/cmake/Caffe2</code> directory.</p>\n<h4 id=\"Solution-2\"><a href=\"#Solution-2\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(Caffe2_DIR &quot;/path/to/pytorch/torch/share/cmake/Caffe2/&quot;)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"quot-context-gpu-h-quot-not-found\"><a href=\"#quot-context-gpu-h-quot-not-found\" class=\"headerlink\" title=\"&quot;context_gpu.h&quot; not found.\"></a>&quot;context_gpu.h&quot; not found.</h3><h4 id=\"Details-3\"><a href=\"#Details-3\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p>\n<p>I forgot to record the error messages, but it should be obvious that some header files(not just <code>context_gpu.h</code>) are missing.</p>\n<h4 id=\"Cause-3\"><a href=\"#Cause-3\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>This time it&#39;s the include path not recognized...</p>\n<h4 id=\"Solution-3\"><a href=\"#Solution-3\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include_directories(&quot;/path/to/pytorch/torch/lib/include&quot;)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"quot-mkl-cblas-h-quot-not-found\"><a href=\"#quot-mkl-cblas-h-quot-not-found\" class=\"headerlink\" title=\"&quot;mkl_cblas.h&quot; not found.\"></a>&quot;mkl_cblas.h&quot; not found.</h3><h4 id=\"Details-4\"><a href=\"#Details-4\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p>\n<p>I forgot to record the error messages, but it should be obvious too.</p>\n<h4 id=\"Cause-4\"><a href=\"#Cause-4\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>Intel Math Kernel Library was turned on but not found. (Why is it enabled when I didn&#39;t even install it???)</p>\n<h4 id=\"Solution-4\"><a href=\"#Solution-4\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Install Intel Math Kernel Library <a href=\"https://software.intel.com/en-us/mkl/choose-download/linux\" target=\"_blank\" rel=\"noopener\">here</a> and add <code>/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/include</code> to <code>C_PATH</code> environment variable. The exact path may vary according to the MKL version and your configuration.<br>Maybe try <code>find / -name mkl_cblas.h</code> to make sure of its location after the installation.</p>\n<h3 id=\"GetSingleArgument-is-not-a-member-of-caffe2-PoolPointsInterpOp\"><a href=\"#GetSingleArgument-is-not-a-member-of-caffe2-PoolPointsInterpOp\" class=\"headerlink\" title=\"GetSingleArgument is not a member of caffe2::PoolPointsInterpOp\"></a>GetSingleArgument<float> is not a member of caffe2::PoolPointsInterpOp<t, context=\"\"></t,></float></h3><h4 id=\"Details-5\"><a href=\"#Details-5\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make ops</code>.<br>Main error message:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/path/to/pytorch/caffe2/operators/accumulate_op.h: In constructor caffe2::AccumulateOp&lt;T, Context&gt;::AccumulateOp(const caffe2::OperatorDef&amp;, caffe2::Workspace*):</span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/accumulate_op.h:13:187: error: GetSingleArgument&lt;float&gt; is not a member of caffe2::AccumulateOp&lt;T, Context&gt;</span><br><span class=\"line\">   AccumulateOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class=\"line\">                                                                                                                                                                                           ^                        </span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/elementwise_ops.h: In constructor caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;::BinaryElementwiseWithArgsOp(const caffe2::OperatorDef&amp;, caffe2::Workspace*):</span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:189: error: GetSingleArgument&lt;bool&gt; is not a member of caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;</span><br><span class=\"line\">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class=\"line\">                                                                                                                                                                                             ^                       </span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:272: error: GetSingleArgument&lt;int&gt; is not a member of caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;</span><br><span class=\"line\">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class=\"line\">                                                                                                                                                                                                                                                                                ^                      </span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:350: error: GetSingleArgument&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; is not a member of caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;</span><br><span class=\"line\">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"Cause-5\"><a href=\"#Cause-5\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>I&#39;m not sure. Could be that <code>GetSingleArgument()</code> is defined elsewhere?</p>\n<h4 id=\"Solution-5\"><a href=\"#Solution-5\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Modify <code>/path/to/densepose/detectron/ops/pool_points_interp.h</code>. Change <code>OperatorBase::GetSingleArgument&lt;float&gt;</code> to <code>this-&gt;template GetSingleArgument&lt;float&gt;</code></p>\n<p>(Thanks to badpx@Github: <a href=\"https://github.com/facebookresearch/DensePose/pull/137/commits/51389c6a02173a25e9429825db452beb5e1cf3be\" target=\"_blank\" rel=\"noopener\">https://github.com/facebookresearch/DensePose/pull/137/commits/51389c6a02173a25e9429825db452beb5e1cf3be</a>) </p>\n<h3 id=\"Undefined-symbol-ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE\"><a href=\"#Undefined-symbol-ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE\" class=\"headerlink\" title=\"Undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE\"></a>Undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE</h3><h4 id=\"Details-6\"><a href=\"#Details-6\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>python detectron/tests/test_zero_even_op.py</code>.</p>\n<p>Main error message:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">OSError: /path/to/densepose/build/libcaffe2_detectron_custom_ops_gpu.so: undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"Cause-6\"><a href=\"#Cause-6\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>WTF is this!???<br>As can be seen, this symbol has something to do with Google, and protobuf.<br>I guess this is caused by a different protobuf version.<br>Good news is that a proper version of protobuf was also built with Caffe2, so why not tell this to DensePose?</p>\n<h4 id=\"Solution-6\"><a href=\"#Solution-6\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>In <code>/path/to/densepose/CMakeLists.txt</code>, Add a few lines in the beginning:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(libprotobuf STATIC IMPORTED) </span><br><span class=\"line\"></span><br><span class=\"line\">set(PROTOBUF_LIB &quot;/path/to/pytorch/torch/lib/libprotobuf.a&quot;) </span><br><span class=\"line\"></span><br><span class=\"line\">set_property(TARGET libprotobuf PROPERTY IMPORTED_LOCATION &quot;$&#123;PROTOBUF_LIB&#125;&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>You can find two <code>target_link_libraries</code> lines in this file(they are not adjacent):<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_link_libraries(caffe2_detectron_custom_ops caffe2_library) </span><br><span class=\"line\">target_link_libraries(caffe2_detectron_custom_ops_gpu caffe2_gpu_library)</span><br></pre></td></tr></table></figure></p>\n<p>Edit the two lines, adding a &quot;libprotobuf&quot; at the end to each of them:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_link_libraries(caffe2_detectron_custom_ops caffe2_library libprotobuf) </span><br><span class=\"line\">target_link_libraries(caffe2_detectron_custom_ops_gpu caffe2_gpu_library libprotobuf)</span><br></pre></td></tr></table></figure></p>\n<p>Then run <code>make ops</code> again, and <code>python detectron/tests/test_zero_even_op.py</code> again.</p>\n<p>(Thanks to hyounsamk@Github: <a href=\"https://github.com/facebookresearch/DensePose/issues/119\" target=\"_blank\" rel=\"noopener\">https://github.com/facebookresearch/DensePose/issues/119</a>)</p>\n<p>After fixing this issue, my DensePose passed tests and was running flawlessly. If any more issues remain, don&#39;t hesitate to comment here~</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"Motivation\"><a href=\"#Motivation\" class=\"headerlink\" title=\"Motivation\"></a>Motivation</h2><p>From now on, I decide to keep a record (tag: MineSweeping) of the issues I meet while working with environments and also their solutions. </p>\n<p>Doing configurations in order to run others&#39; code may be a difficult task, and is sometimes depressing,<br>since various issues could arise, and the it&#39;s impossible for the authors to keep providing solutions for every user in the community.<br>What&#39;s worse, after fixing some problems with a lot of struggle, one may have to waste the same amount of time on the same issue<br>the next time he/she run it again.<br>That&#39;s why I decide to keep this record: to avoid wasting time twice, while also helping others deal with problems if possible.</p>\n<p>DensePose is a great work in real-time human pose estimation, which is based on Caffe2 and Detectron framework.<br>It extracts dense human body 3D surface based on RGB images.<br>The installation instructions are provided <a href=\"https://github.com/facebookresearch/DensePose/blob/master/INSTALL.md\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<p>During the installation process, these are the problems that took me some time to tackle. I spent on week to finally figure out solutions to all the issues. So lucky of me not to give up too early...</p>\n<p><img src=\"/images/densepose-ms/facebook.jpg\" alt=\"Greetings from Facebook AI Research\"></p>","more":"<h2 id=\"Environment\"><a href=\"#Environment\" class=\"headerlink\" title=\"Environment\"></a>Environment</h2><ul>\n<li>System: Ubuntu 18.04</li>\n<li>Linux kernel: 4.15.0-29-generic</li>\n<li>Graphics card: NVIDIA GeForce 1080Ti</li>\n<li>Graphics driver: 410.48</li>\n<li>CUDA: 10.0.130</li>\n<li>cuDNN: 7.3.1</li>\n<li>Caffe2: Built from source</li>\n<li>Python: 2.7.15, based on Anaconda 4.5.11</li>\n</ul>\n<h2 id=\"Problems-amp-Solutions\"><a href=\"#Problems-amp-Solutions\" class=\"headerlink\" title=\"Problems &amp; Solutions\"></a>Problems &amp; Solutions</h2><h3 id=\"Caffe2-module-not-found\"><a href=\"#Caffe2-module-not-found\" class=\"headerlink\" title=\"Caffe2 module not found\"></a>Caffe2 module not found</h3><h4 id=\"Details\"><a href=\"#Details\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make</code>.</p>\n<p>Main error message:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Could not find a package configuration file provided by &quot;Caffe2&quot; with any  </span><br><span class=\"line\">of the following names: </span><br><span class=\"line\">    Caffe2Config.cmake </span><br><span class=\"line\">    caffe2-config.cmake</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"Cause\"><a href=\"#Cause\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>Caffe2 build path isn&#39;t known by CMake.</p>\n<h4 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(Caffe2_DIR &quot;/path/to/pytorch/torch/share/cmake/Caffe2/&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>(Note: <code>set(Caffe2_DIR &quot;/path/to/pytorch/build/&quot;)</code> can also fix this issue but may cause other issues.) </p>\n<h3 id=\"Detectron-ops-lib-not-found\"><a href=\"#Detectron-ops-lib-not-found\" class=\"headerlink\" title=\"Detectron ops lib not found\"></a>Detectron ops lib not found</h3><h4 id=\"Details-1\"><a href=\"#Details-1\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>python2 $DENSEPOSE/detectron/tests/test_spatial_narrow_as_op.py</code> after <code>make</code>.</p>\n<p>Main error message:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Detectron ops lib not found; make sure that your Caffe2 version includes Detectron module.</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"Cause-1\"><a href=\"#Cause-1\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>Seems that the Python part of DensePose couldn&#39;t recognize Caffe2.</p>\n<h4 id=\"Solution-1\"><a href=\"#Solution-1\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Add <code>/path/to/pytorch</code> to <code>PYTHONPATH</code> environment variable.<br>Could be added by directly <code>export PYTHONPATH=$PYTHONPATH:/path/to/pytorch</code> instruction or by adding this line to <code>~/.bashrc</code>.<br>Remember to run <code>source ~/.bashrc</code> after the modification.</p>\n<h3 id=\"cmake-files-not-found-amp-Unknown-CMake-command-quot-caffe2-interface-library-quot\"><a href=\"#cmake-files-not-found-amp-Unknown-CMake-command-quot-caffe2-interface-library-quot\" class=\"headerlink\" title=\"*.cmake files not found &amp; Unknown CMake command &quot;caffe2_interface_library&quot;\"></a>*.cmake files not found &amp; Unknown CMake command &quot;caffe2_interface_library&quot;</h3><h4 id=\"Details-2\"><a href=\"#Details-2\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p>\n<p>Main error message:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CMake Error at /path/to/pytorch/build/Caffe2Config.cmake:14 (include):</span><br><span class=\"line\">  include could not find load file:</span><br><span class=\"line\"></span><br><span class=\"line\">    /path/to/pytorch/build/public/utils.cmake</span><br><span class=\"line\">    /path/to/pytorch/build/public/threads.cmake</span><br><span class=\"line\">    /path/to/pytorch/build/public/cuda.cmake</span><br><span class=\"line\">    /path/to/pytorch/build/public/mkl.cmake</span><br><span class=\"line\">    /path/to/pytorch/build/Caffe2Targets.cmake</span><br><span class=\"line\"></span><br><span class=\"line\">Call Stack (most recent call first):</span><br><span class=\"line\">  CMakeLists.txt:8 (find_package)</span><br><span class=\"line\"></span><br><span class=\"line\">CMake Error at /path/to/pytorch/build/Caffe2Config.cmake:117 (caffe2_interface_library):</span><br><span class=\"line\">  Unknown CMake command &quot;caffe2_interface_library&quot;.</span><br><span class=\"line\">Call Stack (most recent call first):</span><br><span class=\"line\">  CMakeLists.txt:8 (find_package)</span><br></pre></td></tr></table></figure></p>\n<p>(Several <code>*.cmake</code> files, I only showed a few.)</p>\n<h4 id=\"Cause-2\"><a href=\"#Cause-2\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>These files are not in the <code>pytorch/build</code> directory. By searching, I found that they are in the <code>pytorch/torch/share/cmake/Caffe2</code> directory.</p>\n<h4 id=\"Solution-2\"><a href=\"#Solution-2\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">set(Caffe2_DIR &quot;/path/to/pytorch/torch/share/cmake/Caffe2/&quot;)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"quot-context-gpu-h-quot-not-found\"><a href=\"#quot-context-gpu-h-quot-not-found\" class=\"headerlink\" title=\"&quot;context_gpu.h&quot; not found.\"></a>&quot;context_gpu.h&quot; not found.</h3><h4 id=\"Details-3\"><a href=\"#Details-3\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p>\n<p>I forgot to record the error messages, but it should be obvious that some header files(not just <code>context_gpu.h</code>) are missing.</p>\n<h4 id=\"Cause-3\"><a href=\"#Cause-3\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>This time it&#39;s the include path not recognized...</p>\n<h4 id=\"Solution-3\"><a href=\"#Solution-3\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">include_directories(&quot;/path/to/pytorch/torch/lib/include&quot;)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"quot-mkl-cblas-h-quot-not-found\"><a href=\"#quot-mkl-cblas-h-quot-not-found\" class=\"headerlink\" title=\"&quot;mkl_cblas.h&quot; not found.\"></a>&quot;mkl_cblas.h&quot; not found.</h3><h4 id=\"Details-4\"><a href=\"#Details-4\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p>\n<p>I forgot to record the error messages, but it should be obvious too.</p>\n<h4 id=\"Cause-4\"><a href=\"#Cause-4\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>Intel Math Kernel Library was turned on but not found. (Why is it enabled when I didn&#39;t even install it???)</p>\n<h4 id=\"Solution-4\"><a href=\"#Solution-4\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Install Intel Math Kernel Library <a href=\"https://software.intel.com/en-us/mkl/choose-download/linux\" target=\"_blank\" rel=\"noopener\">here</a> and add <code>/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/include</code> to <code>C_PATH</code> environment variable. The exact path may vary according to the MKL version and your configuration.<br>Maybe try <code>find / -name mkl_cblas.h</code> to make sure of its location after the installation.</p>\n<h3 id=\"GetSingleArgument-is-not-a-member-of-caffe2-PoolPointsInterpOp\"><a href=\"#GetSingleArgument-is-not-a-member-of-caffe2-PoolPointsInterpOp\" class=\"headerlink\" title=\"GetSingleArgument is not a member of caffe2::PoolPointsInterpOp\"></a>GetSingleArgument<float> is not a member of caffe2::PoolPointsInterpOp<t, context=\"\"></t,></float></h3><h4 id=\"Details-5\"><a href=\"#Details-5\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make ops</code>.<br>Main error message:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/path/to/pytorch/caffe2/operators/accumulate_op.h: In constructor caffe2::AccumulateOp&lt;T, Context&gt;::AccumulateOp(const caffe2::OperatorDef&amp;, caffe2::Workspace*):</span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/accumulate_op.h:13:187: error: GetSingleArgument&lt;float&gt; is not a member of caffe2::AccumulateOp&lt;T, Context&gt;</span><br><span class=\"line\">   AccumulateOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class=\"line\">                                                                                                                                                                                           ^                        </span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/elementwise_ops.h: In constructor caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;::BinaryElementwiseWithArgsOp(const caffe2::OperatorDef&amp;, caffe2::Workspace*):</span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:189: error: GetSingleArgument&lt;bool&gt; is not a member of caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;</span><br><span class=\"line\">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class=\"line\">                                                                                                                                                                                             ^                       </span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:272: error: GetSingleArgument&lt;int&gt; is not a member of caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;</span><br><span class=\"line\">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class=\"line\">                                                                                                                                                                                                                                                                                ^                      </span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:350: error: GetSingleArgument&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; is not a member of caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;</span><br><span class=\"line\">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"Cause-5\"><a href=\"#Cause-5\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>I&#39;m not sure. Could be that <code>GetSingleArgument()</code> is defined elsewhere?</p>\n<h4 id=\"Solution-5\"><a href=\"#Solution-5\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Modify <code>/path/to/densepose/detectron/ops/pool_points_interp.h</code>. Change <code>OperatorBase::GetSingleArgument&lt;float&gt;</code> to <code>this-&gt;template GetSingleArgument&lt;float&gt;</code></p>\n<p>(Thanks to badpx@Github: <a href=\"https://github.com/facebookresearch/DensePose/pull/137/commits/51389c6a02173a25e9429825db452beb5e1cf3be\" target=\"_blank\" rel=\"noopener\">https://github.com/facebookresearch/DensePose/pull/137/commits/51389c6a02173a25e9429825db452beb5e1cf3be</a>) </p>\n<h3 id=\"Undefined-symbol-ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE\"><a href=\"#Undefined-symbol-ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE\" class=\"headerlink\" title=\"Undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE\"></a>Undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE</h3><h4 id=\"Details-6\"><a href=\"#Details-6\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>python detectron/tests/test_zero_even_op.py</code>.</p>\n<p>Main error message:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">OSError: /path/to/densepose/build/libcaffe2_detectron_custom_ops_gpu.so: undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"Cause-6\"><a href=\"#Cause-6\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>WTF is this!???<br>As can be seen, this symbol has something to do with Google, and protobuf.<br>I guess this is caused by a different protobuf version.<br>Good news is that a proper version of protobuf was also built with Caffe2, so why not tell this to DensePose?</p>\n<h4 id=\"Solution-6\"><a href=\"#Solution-6\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>In <code>/path/to/densepose/CMakeLists.txt</code>, Add a few lines in the beginning:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">add_library(libprotobuf STATIC IMPORTED) </span><br><span class=\"line\"></span><br><span class=\"line\">set(PROTOBUF_LIB &quot;/path/to/pytorch/torch/lib/libprotobuf.a&quot;) </span><br><span class=\"line\"></span><br><span class=\"line\">set_property(TARGET libprotobuf PROPERTY IMPORTED_LOCATION &quot;$&#123;PROTOBUF_LIB&#125;&quot;)</span><br></pre></td></tr></table></figure></p>\n<p>You can find two <code>target_link_libraries</code> lines in this file(they are not adjacent):<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_link_libraries(caffe2_detectron_custom_ops caffe2_library) </span><br><span class=\"line\">target_link_libraries(caffe2_detectron_custom_ops_gpu caffe2_gpu_library)</span><br></pre></td></tr></table></figure></p>\n<p>Edit the two lines, adding a &quot;libprotobuf&quot; at the end to each of them:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">target_link_libraries(caffe2_detectron_custom_ops caffe2_library libprotobuf) </span><br><span class=\"line\">target_link_libraries(caffe2_detectron_custom_ops_gpu caffe2_gpu_library libprotobuf)</span><br></pre></td></tr></table></figure></p>\n<p>Then run <code>make ops</code> again, and <code>python detectron/tests/test_zero_even_op.py</code> again.</p>\n<p>(Thanks to hyounsamk@Github: <a href=\"https://github.com/facebookresearch/DensePose/issues/119\" target=\"_blank\" rel=\"noopener\">https://github.com/facebookresearch/DensePose/issues/119</a>)</p>\n<p>After fixing this issue, my DensePose passed tests and was running flawlessly. If any more issues remain, don&#39;t hesitate to comment here~</p>"},{"title":"[MineSweeping] The Long Struggle of DensePose Installation","date":"2018-11-18T08:00:14.000Z","_content":"<br />\nDensePose is a great work in real-time human pose estimation, which is based on Caffe2 and Detectron framework. It extracts dense human body 3D surface based on RGB images. The installation instructions are provided [here](https://github.com/facebookresearch/DensePose/blob/master/INSTALL.md).\n\nDuring my installation process, these are the problems that took me some time to tackle. I spent on week to finally figure out solutions to all the issues. So lucky of me not to give up too early...\n\n<div align=\"center\">\n    <img src=\"/images/densepose-ms/facebook.jpg\" width=\"15%\" height=\"15%\" alt=\"Greetings from Facebook AI Research\">\n</div>\n\n<!-- more -->\n\n## 1 Environment\n- System: Ubuntu 18.04\n- Linux kernel: 4.15.0-29-generic\n- Graphics card: NVIDIA GeForce 1080Ti\n- Graphics driver: 410.48\n- CUDA: 10.0.130\n- cuDNN: 7.3.1\n- Caffe2: Built from source\n- Python: 2.7.15, based on Anaconda 4.5.11\n\n## 2 Problems & Solutions\n### 2.1 Caffe2 module not found\n#### Details\nOccurred when running `make`.\n\nMain error message:\n```bash\nCould not find a package configuration file provided by \"Caffe2\" with any  \nof the following names: \n    Caffe2Config.cmake \n    caffe2-config.cmake \n```\n#### Cause\nCaffe2 build path isn't known by CMake.\n#### Solution\nAdded one line in the beginning of CMakeLists.txt: \n```CMake\nset(Caffe2_DIR \"/path/to/pytorch/torch/share/cmake/Caffe2/\") \n```\n(Note: `set(Caffe2_DIR \"/path/to/pytorch/build/\")` can also fix this issue but may cause other issues.) \n\n### 2.2 Detectron ops lib not found\n#### Details\nOccurred when running `python2 $DENSEPOSE/detectron/tests/test_spatial_narrow_as_op.py` after `make`.\n\nMain error message:\n```bash\nDetectron ops lib not found; make sure that your Caffe2 version includes Detectron module. \n```\n#### Cause\nSeems that the Python part of DensePose couldn't recognize Caffe2.\n#### Solution\nAdd `/path/to/pytorch` to `PYTHONPATH` environment variable. Could be added by directly `export PYTHONPATH=$PYTHONPATH:/path/to/pytorch` instruction or by adding this line to `~/.bashrc`. Remember to run `source ~/.bashrc` after the modification.\n\n### 2.3 *.cmake files not found & Unknown CMake command \"caffe2_interface_library\"\n#### Details\nOccurred when running `make ops`.\n\nMain error message: \n```CMake\nCMake Error at /path/to/pytorch/build/Caffe2Config.cmake:14 (include):\n  include could not find load file:\n\n    /path/to/pytorch/build/public/utils.cmake\n    /path/to/pytorch/build/public/threads.cmake\n    /path/to/pytorch/build/public/cuda.cmake\n    /path/to/pytorch/build/public/mkl.cmake\n    /path/to/pytorch/build/Caffe2Targets.cmake\n\nCall Stack (most recent call first):\n  CMakeLists.txt:8 (find_package)\n\nCMake Error at /path/to/pytorch/build/Caffe2Config.cmake:117 (caffe2_interface_library):\n  Unknown CMake command \"caffe2_interface_library\".\nCall Stack (most recent call first):\n  CMakeLists.txt:8 (find_package)  \n```\n(Several `*.cmake` files, I only showed a few.)\n#### Cause\nThese files are not in the `pytorch/build` directory. By searching, I found that they are in the `pytorch/torch/share/cmake/Caffe2` directory.\n#### Solution\nAdded one line in the beginning of CMakeLists.txt: \n```CMake\nset(Caffe2_DIR \"/path/to/pytorch/torch/share/cmake/Caffe2/\")\n```\n\n### 2.4 \"context_gpu.h\" not found.\n#### Details\nOccurred when running `make ops`.\n\nI forgot to record the error messages, but it should be obvious that some header files(not just `context_gpu.h`) are missing.\n#### Cause\nThis time it's the include path not recognized...\n#### Solution\nAdded one line in the beginning of CMakeLists.txt: \n```CMake\ninclude_directories(\"/path/to/pytorch/torch/lib/include\")\n```\n\n### 2.5 \"mkl_cblas.h\" not found.\n#### Details\nOccurred when running `make ops`.\n\nI forgot to record the error messages, but it should be obvious too.\n#### Cause\nIntel Math Kernel Library was turned on but not found. (Why is it enabled when I didn't even install it???)\n#### Solution\nInstall Intel Math Kernel Library [here](https://software.intel.com/en-us/mkl/choose-download/linux) and add `/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/include` to `C_PATH` environment variable. The exact path may vary according to the MKL version and your configuration.\nMaybe try `find / -name mkl_cblas.h` to make sure of its location after the installation.\n\n### 2.6 GetSingleArgument<float> is not a member of caffe2::PoolPointsInterpOp<T, Context>\n#### Details\nOccurred when running `make ops`.\nMain error message:\n```bash\n/path/to/pytorch/caffe2/operators/accumulate_op.h: In constructor caffe2::AccumulateOp<T, Context>::AccumulateOp(const caffe2::OperatorDef&, caffe2::Workspace*):\n/path/to/pytorch/caffe2/operators/accumulate_op.h:13:187: error: GetSingleArgument<float> is not a member of caffe2::AccumulateOp<T, Context>\n   AccumulateOp(const OperatorDef& operator_def, Workspace* ws)\n                                                                                                                                                                                           ^                        \n/path/to/pytorch/caffe2/operators/elementwise_ops.h: In constructor caffe2::BinaryElementwiseWithArgsOp<InputTypes, Context, Functor, OutputTypeMap>::BinaryElementwiseWithArgsOp(const caffe2::OperatorDef&, caffe2::Workspace*):\n/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:189: error: GetSingleArgument<bool> is not a member of caffe2::BinaryElementwiseWithArgsOp<InputTypes, Context, Functor, OutputTypeMap>\n   BinaryElementwiseWithArgsOp(const OperatorDef& operator_def, Workspace* ws)\n                                                                                                                                                                                             ^                       \n/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:272: error: GetSingleArgument<int> is not a member of caffe2::BinaryElementwiseWithArgsOp<InputTypes, Context, Functor, OutputTypeMap>\n   BinaryElementwiseWithArgsOp(const OperatorDef& operator_def, Workspace* ws)\n                                                                                                                                                                                                                                                                                ^                      \n/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:350: error: GetSingleArgument<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > is not a member of caffe2::BinaryElementwiseWithArgsOp<InputTypes, Context, Functor, OutputTypeMap>\n   BinaryElementwiseWithArgsOp(const OperatorDef& operator_def, Workspace* ws)\n```\n#### Cause\nI'm not sure. Could be that `GetSingleArgument()` is defined elsewhere?\n#### Solution\nModify `/path/to/densepose/detectron/ops/pool_points_interp.h`. Change `OperatorBase::GetSingleArgument<float> ` to `this->template GetSingleArgument<float>`\n\n(Thanks to badpx@Github: https://github.com/facebookresearch/DensePose/pull/137/commits/51389c6a02173a25e9429825db452beb5e1cf3be) \n\n### 2.7 Undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE \n#### Details\nOccurred when running `python detectron/tests/test_zero_even_op.py`.\n\nMain error message:\n```\nOSError: /path/to/densepose/build/libcaffe2_detectron_custom_ops_gpu.so: undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE \n```\n#### Cause\nWTF is this!???\nAs can be seen, this symbol has something to do with Google, and protobuf.\nI guess this is caused by a different protobuf version. Good news is that a proper version of protobuf was also built with Caffe2, so why not tell this to DensePose?\n\n#### Solution\nIn `/path/to/densepose/CMakeLists.txt`, Add a few lines in the beginning: \n```CMake\nadd_library(libprotobuf STATIC IMPORTED) \n\nset(PROTOBUF_LIB \"/path/to/pytorch/torch/lib/libprotobuf.a\") \n\nset_property(TARGET libprotobuf PROPERTY IMPORTED_LOCATION \"${PROTOBUF_LIB}\") \n```\nYou can find two `target_link_libraries` lines in this file(they are not adjacent):\n```CMake\ntarget_link_libraries(caffe2_detectron_custom_ops caffe2_library) \ntarget_link_libraries(caffe2_detectron_custom_ops_gpu caffe2_gpu_library) \n```\nEdit the two lines, adding a \"libprotobuf\" at the end to each of them: \n```CMake\ntarget_link_libraries(caffe2_detectron_custom_ops caffe2_library libprotobuf) \ntarget_link_libraries(caffe2_detectron_custom_ops_gpu caffe2_gpu_library libprotobuf) \n```\nThen run `make ops` again, and `python detectron/tests/test_zero_even_op.py` again.\n\n(Thanks to hyounsamk@Github: https://github.com/facebookresearch/DensePose/issues/119)\n\nAfter fixing this issue, my DensePose passed tests and was running flawlessly. If any more issues remain, don't hesitate to comment here~\n\n\n## 0 Motivation\nStarting from this post, I decide to keep a record (tag: MineSweeping) of the issues I meet while working with environments and also their solutions. \n\n\nDoing configurations in order to run others' code may be a difficult task, and is sometimes depressing, since various issues could arise, and the it's impossible for the authors to keep providing solutions for every user in the community. What's worse, after fixing some problems with a lot of struggle, one may have to waste the same amount of time on the same issue the next time he/she run it again. That's why I decide to keep this record: to avoid wasting time twice, while also helping others deal with problems if possible.\n","source":"_posts/densepose-minesweeping.md","raw":"---\ntitle: '[MineSweeping] The Long Struggle of DensePose Installation'\ntags:\n  - Deep Learning\n  - MineSweeping\ndate: 2018-11-18 17:00:14\n---\n<br />\nDensePose is a great work in real-time human pose estimation, which is based on Caffe2 and Detectron framework. It extracts dense human body 3D surface based on RGB images. The installation instructions are provided [here](https://github.com/facebookresearch/DensePose/blob/master/INSTALL.md).\n\nDuring my installation process, these are the problems that took me some time to tackle. I spent on week to finally figure out solutions to all the issues. So lucky of me not to give up too early...\n\n<div align=\"center\">\n    <img src=\"/images/densepose-ms/facebook.jpg\" width=\"15%\" height=\"15%\" alt=\"Greetings from Facebook AI Research\">\n</div>\n\n<!-- more -->\n\n## 1 Environment\n- System: Ubuntu 18.04\n- Linux kernel: 4.15.0-29-generic\n- Graphics card: NVIDIA GeForce 1080Ti\n- Graphics driver: 410.48\n- CUDA: 10.0.130\n- cuDNN: 7.3.1\n- Caffe2: Built from source\n- Python: 2.7.15, based on Anaconda 4.5.11\n\n## 2 Problems & Solutions\n### 2.1 Caffe2 module not found\n#### Details\nOccurred when running `make`.\n\nMain error message:\n```bash\nCould not find a package configuration file provided by \"Caffe2\" with any  \nof the following names: \n    Caffe2Config.cmake \n    caffe2-config.cmake \n```\n#### Cause\nCaffe2 build path isn't known by CMake.\n#### Solution\nAdded one line in the beginning of CMakeLists.txt: \n```CMake\nset(Caffe2_DIR \"/path/to/pytorch/torch/share/cmake/Caffe2/\") \n```\n(Note: `set(Caffe2_DIR \"/path/to/pytorch/build/\")` can also fix this issue but may cause other issues.) \n\n### 2.2 Detectron ops lib not found\n#### Details\nOccurred when running `python2 $DENSEPOSE/detectron/tests/test_spatial_narrow_as_op.py` after `make`.\n\nMain error message:\n```bash\nDetectron ops lib not found; make sure that your Caffe2 version includes Detectron module. \n```\n#### Cause\nSeems that the Python part of DensePose couldn't recognize Caffe2.\n#### Solution\nAdd `/path/to/pytorch` to `PYTHONPATH` environment variable. Could be added by directly `export PYTHONPATH=$PYTHONPATH:/path/to/pytorch` instruction or by adding this line to `~/.bashrc`. Remember to run `source ~/.bashrc` after the modification.\n\n### 2.3 *.cmake files not found & Unknown CMake command \"caffe2_interface_library\"\n#### Details\nOccurred when running `make ops`.\n\nMain error message: \n```CMake\nCMake Error at /path/to/pytorch/build/Caffe2Config.cmake:14 (include):\n  include could not find load file:\n\n    /path/to/pytorch/build/public/utils.cmake\n    /path/to/pytorch/build/public/threads.cmake\n    /path/to/pytorch/build/public/cuda.cmake\n    /path/to/pytorch/build/public/mkl.cmake\n    /path/to/pytorch/build/Caffe2Targets.cmake\n\nCall Stack (most recent call first):\n  CMakeLists.txt:8 (find_package)\n\nCMake Error at /path/to/pytorch/build/Caffe2Config.cmake:117 (caffe2_interface_library):\n  Unknown CMake command \"caffe2_interface_library\".\nCall Stack (most recent call first):\n  CMakeLists.txt:8 (find_package)  \n```\n(Several `*.cmake` files, I only showed a few.)\n#### Cause\nThese files are not in the `pytorch/build` directory. By searching, I found that they are in the `pytorch/torch/share/cmake/Caffe2` directory.\n#### Solution\nAdded one line in the beginning of CMakeLists.txt: \n```CMake\nset(Caffe2_DIR \"/path/to/pytorch/torch/share/cmake/Caffe2/\")\n```\n\n### 2.4 \"context_gpu.h\" not found.\n#### Details\nOccurred when running `make ops`.\n\nI forgot to record the error messages, but it should be obvious that some header files(not just `context_gpu.h`) are missing.\n#### Cause\nThis time it's the include path not recognized...\n#### Solution\nAdded one line in the beginning of CMakeLists.txt: \n```CMake\ninclude_directories(\"/path/to/pytorch/torch/lib/include\")\n```\n\n### 2.5 \"mkl_cblas.h\" not found.\n#### Details\nOccurred when running `make ops`.\n\nI forgot to record the error messages, but it should be obvious too.\n#### Cause\nIntel Math Kernel Library was turned on but not found. (Why is it enabled when I didn't even install it???)\n#### Solution\nInstall Intel Math Kernel Library [here](https://software.intel.com/en-us/mkl/choose-download/linux) and add `/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/include` to `C_PATH` environment variable. The exact path may vary according to the MKL version and your configuration.\nMaybe try `find / -name mkl_cblas.h` to make sure of its location after the installation.\n\n### 2.6 GetSingleArgument<float> is not a member of caffe2::PoolPointsInterpOp<T, Context>\n#### Details\nOccurred when running `make ops`.\nMain error message:\n```bash\n/path/to/pytorch/caffe2/operators/accumulate_op.h: In constructor caffe2::AccumulateOp<T, Context>::AccumulateOp(const caffe2::OperatorDef&, caffe2::Workspace*):\n/path/to/pytorch/caffe2/operators/accumulate_op.h:13:187: error: GetSingleArgument<float> is not a member of caffe2::AccumulateOp<T, Context>\n   AccumulateOp(const OperatorDef& operator_def, Workspace* ws)\n                                                                                                                                                                                           ^                        \n/path/to/pytorch/caffe2/operators/elementwise_ops.h: In constructor caffe2::BinaryElementwiseWithArgsOp<InputTypes, Context, Functor, OutputTypeMap>::BinaryElementwiseWithArgsOp(const caffe2::OperatorDef&, caffe2::Workspace*):\n/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:189: error: GetSingleArgument<bool> is not a member of caffe2::BinaryElementwiseWithArgsOp<InputTypes, Context, Functor, OutputTypeMap>\n   BinaryElementwiseWithArgsOp(const OperatorDef& operator_def, Workspace* ws)\n                                                                                                                                                                                             ^                       \n/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:272: error: GetSingleArgument<int> is not a member of caffe2::BinaryElementwiseWithArgsOp<InputTypes, Context, Functor, OutputTypeMap>\n   BinaryElementwiseWithArgsOp(const OperatorDef& operator_def, Workspace* ws)\n                                                                                                                                                                                                                                                                                ^                      \n/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:350: error: GetSingleArgument<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > is not a member of caffe2::BinaryElementwiseWithArgsOp<InputTypes, Context, Functor, OutputTypeMap>\n   BinaryElementwiseWithArgsOp(const OperatorDef& operator_def, Workspace* ws)\n```\n#### Cause\nI'm not sure. Could be that `GetSingleArgument()` is defined elsewhere?\n#### Solution\nModify `/path/to/densepose/detectron/ops/pool_points_interp.h`. Change `OperatorBase::GetSingleArgument<float> ` to `this->template GetSingleArgument<float>`\n\n(Thanks to badpx@Github: https://github.com/facebookresearch/DensePose/pull/137/commits/51389c6a02173a25e9429825db452beb5e1cf3be) \n\n### 2.7 Undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE \n#### Details\nOccurred when running `python detectron/tests/test_zero_even_op.py`.\n\nMain error message:\n```\nOSError: /path/to/densepose/build/libcaffe2_detectron_custom_ops_gpu.so: undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE \n```\n#### Cause\nWTF is this!???\nAs can be seen, this symbol has something to do with Google, and protobuf.\nI guess this is caused by a different protobuf version. Good news is that a proper version of protobuf was also built with Caffe2, so why not tell this to DensePose?\n\n#### Solution\nIn `/path/to/densepose/CMakeLists.txt`, Add a few lines in the beginning: \n```CMake\nadd_library(libprotobuf STATIC IMPORTED) \n\nset(PROTOBUF_LIB \"/path/to/pytorch/torch/lib/libprotobuf.a\") \n\nset_property(TARGET libprotobuf PROPERTY IMPORTED_LOCATION \"${PROTOBUF_LIB}\") \n```\nYou can find two `target_link_libraries` lines in this file(they are not adjacent):\n```CMake\ntarget_link_libraries(caffe2_detectron_custom_ops caffe2_library) \ntarget_link_libraries(caffe2_detectron_custom_ops_gpu caffe2_gpu_library) \n```\nEdit the two lines, adding a \"libprotobuf\" at the end to each of them: \n```CMake\ntarget_link_libraries(caffe2_detectron_custom_ops caffe2_library libprotobuf) \ntarget_link_libraries(caffe2_detectron_custom_ops_gpu caffe2_gpu_library libprotobuf) \n```\nThen run `make ops` again, and `python detectron/tests/test_zero_even_op.py` again.\n\n(Thanks to hyounsamk@Github: https://github.com/facebookresearch/DensePose/issues/119)\n\nAfter fixing this issue, my DensePose passed tests and was running flawlessly. If any more issues remain, don't hesitate to comment here~\n\n\n## 0 Motivation\nStarting from this post, I decide to keep a record (tag: MineSweeping) of the issues I meet while working with environments and also their solutions. \n\n\nDoing configurations in order to run others' code may be a difficult task, and is sometimes depressing, since various issues could arise, and the it's impossible for the authors to keep providing solutions for every user in the community. What's worse, after fixing some problems with a lot of struggle, one may have to waste the same amount of time on the same issue the next time he/she run it again. That's why I decide to keep this record: to avoid wasting time twice, while also helping others deal with problems if possible.\n","slug":"densepose-minesweeping","published":1,"updated":"2018-11-18T08:18:31.951Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomm0o15000jwgwnh5f71q4o","content":"<p><br><br>DensePose is a great work in real-time human pose estimation, which is based on Caffe2 and Detectron framework. It extracts dense human body 3D surface based on RGB images. The installation instructions are provided <a href=\"https://github.com/facebookresearch/DensePose/blob/master/INSTALL.md\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<p>During my installation process, these are the problems that took me some time to tackle. I spent on week to finally figure out solutions to all the issues. So lucky of me not to give up too early...</p>\n<div align=\"center\"><br>    <img src=\"/images/densepose-ms/facebook.jpg\" width=\"15%\" height=\"15%\" alt=\"Greetings from Facebook AI Research\"><br></div>\n\n<a id=\"more\"></a>\n<h2 id=\"1-Environment\"><a href=\"#1-Environment\" class=\"headerlink\" title=\"1 Environment\"></a>1 Environment</h2><ul>\n<li>System: Ubuntu 18.04</li>\n<li>Linux kernel: 4.15.0-29-generic</li>\n<li>Graphics card: NVIDIA GeForce 1080Ti</li>\n<li>Graphics driver: 410.48</li>\n<li>CUDA: 10.0.130</li>\n<li>cuDNN: 7.3.1</li>\n<li>Caffe2: Built from source</li>\n<li>Python: 2.7.15, based on Anaconda 4.5.11</li>\n</ul>\n<h2 id=\"2-Problems-amp-Solutions\"><a href=\"#2-Problems-amp-Solutions\" class=\"headerlink\" title=\"2 Problems &amp; Solutions\"></a>2 Problems &amp; Solutions</h2><h3 id=\"2-1-Caffe2-module-not-found\"><a href=\"#2-1-Caffe2-module-not-found\" class=\"headerlink\" title=\"2.1 Caffe2 module not found\"></a>2.1 Caffe2 module not found</h3><h4 id=\"Details\"><a href=\"#Details\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make</code>.</p>\n<p>Main error message:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Could not find a package configuration file provided by <span class=\"string\">\"Caffe2\"</span> with any  </span><br><span class=\"line\">of the following names: </span><br><span class=\"line\">    Caffe2Config.cmake </span><br><span class=\"line\">    caffe2-config.cmake</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"Cause\"><a href=\"#Cause\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>Caffe2 build path isn&#39;t known by CMake.</p>\n<h4 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">set</span>(Caffe2_DIR <span class=\"string\">\"/path/to/pytorch/torch/share/cmake/Caffe2/\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>(Note: <code>set(Caffe2_DIR &quot;/path/to/pytorch/build/&quot;)</code> can also fix this issue but may cause other issues.) </p>\n<h3 id=\"2-2-Detectron-ops-lib-not-found\"><a href=\"#2-2-Detectron-ops-lib-not-found\" class=\"headerlink\" title=\"2.2 Detectron ops lib not found\"></a>2.2 Detectron ops lib not found</h3><h4 id=\"Details-1\"><a href=\"#Details-1\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>python2 $DENSEPOSE/detectron/tests/test_spatial_narrow_as_op.py</code> after <code>make</code>.</p>\n<p>Main error message:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Detectron ops lib not found; make sure that your Caffe2 version includes Detectron module.</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"Cause-1\"><a href=\"#Cause-1\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>Seems that the Python part of DensePose couldn&#39;t recognize Caffe2.</p>\n<h4 id=\"Solution-1\"><a href=\"#Solution-1\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Add <code>/path/to/pytorch</code> to <code>PYTHONPATH</code> environment variable. Could be added by directly <code>export PYTHONPATH=$PYTHONPATH:/path/to/pytorch</code> instruction or by adding this line to <code>~/.bashrc</code>. Remember to run <code>source ~/.bashrc</code> after the modification.</p>\n<h3 id=\"2-3-cmake-files-not-found-amp-Unknown-CMake-command-quot-caffe2-interface-library-quot\"><a href=\"#2-3-cmake-files-not-found-amp-Unknown-CMake-command-quot-caffe2-interface-library-quot\" class=\"headerlink\" title=\"2.3 *.cmake files not found &amp; Unknown CMake command &quot;caffe2_interface_library&quot;\"></a>2.3 *.cmake files not found &amp; Unknown CMake command &quot;caffe2_interface_library&quot;</h3><h4 id=\"Details-2\"><a href=\"#Details-2\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p>\n<p>Main error message:<br><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CMake Error at /path/to/pytorch/build/Caffe2Config.cmake:<span class=\"number\">14</span> (<span class=\"keyword\">include</span>):</span><br><span class=\"line\">  <span class=\"keyword\">include</span> could not find load file:</span><br><span class=\"line\"></span><br><span class=\"line\">    /path/to/pytorch/build/public/utils.cmake</span><br><span class=\"line\">    /path/to/pytorch/build/public/threads.cmake</span><br><span class=\"line\">    /path/to/pytorch/build/public/cuda.cmake</span><br><span class=\"line\">    /path/to/pytorch/build/public/mkl.cmake</span><br><span class=\"line\">    /path/to/pytorch/build/Caffe2Targets.cmake</span><br><span class=\"line\"></span><br><span class=\"line\">Call Stack (most recent call first):</span><br><span class=\"line\">  CMakeLists.txt:<span class=\"number\">8</span> (<span class=\"keyword\">find_package</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">CMake Error at /path/to/pytorch/build/Caffe2Config.cmake:<span class=\"number\">117</span> (caffe2_interface_library):</span><br><span class=\"line\">  Unknown CMake command <span class=\"string\">\"caffe2_interface_library\"</span>.</span><br><span class=\"line\">Call Stack (most recent call first):</span><br><span class=\"line\">  CMakeLists.txt:<span class=\"number\">8</span> (<span class=\"keyword\">find_package</span>)</span><br></pre></td></tr></table></figure></p>\n<p>(Several <code>*.cmake</code> files, I only showed a few.)</p>\n<h4 id=\"Cause-2\"><a href=\"#Cause-2\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>These files are not in the <code>pytorch/build</code> directory. By searching, I found that they are in the <code>pytorch/torch/share/cmake/Caffe2</code> directory.</p>\n<h4 id=\"Solution-2\"><a href=\"#Solution-2\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">set</span>(Caffe2_DIR <span class=\"string\">\"/path/to/pytorch/torch/share/cmake/Caffe2/\"</span>)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-4-quot-context-gpu-h-quot-not-found\"><a href=\"#2-4-quot-context-gpu-h-quot-not-found\" class=\"headerlink\" title=\"2.4 &quot;context_gpu.h&quot; not found.\"></a>2.4 &quot;context_gpu.h&quot; not found.</h3><h4 id=\"Details-3\"><a href=\"#Details-3\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p>\n<p>I forgot to record the error messages, but it should be obvious that some header files(not just <code>context_gpu.h</code>) are missing.</p>\n<h4 id=\"Cause-3\"><a href=\"#Cause-3\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>This time it&#39;s the include path not recognized...</p>\n<h4 id=\"Solution-3\"><a href=\"#Solution-3\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">include_directories</span>(<span class=\"string\">\"/path/to/pytorch/torch/lib/include\"</span>)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-5-quot-mkl-cblas-h-quot-not-found\"><a href=\"#2-5-quot-mkl-cblas-h-quot-not-found\" class=\"headerlink\" title=\"2.5 &quot;mkl_cblas.h&quot; not found.\"></a>2.5 &quot;mkl_cblas.h&quot; not found.</h3><h4 id=\"Details-4\"><a href=\"#Details-4\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p>\n<p>I forgot to record the error messages, but it should be obvious too.</p>\n<h4 id=\"Cause-4\"><a href=\"#Cause-4\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>Intel Math Kernel Library was turned on but not found. (Why is it enabled when I didn&#39;t even install it???)</p>\n<h4 id=\"Solution-4\"><a href=\"#Solution-4\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Install Intel Math Kernel Library <a href=\"https://software.intel.com/en-us/mkl/choose-download/linux\" target=\"_blank\" rel=\"noopener\">here</a> and add <code>/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/include</code> to <code>C_PATH</code> environment variable. The exact path may vary according to the MKL version and your configuration.<br>Maybe try <code>find / -name mkl_cblas.h</code> to make sure of its location after the installation.</p>\n<h3 id=\"2-6-GetSingleArgument-is-not-a-member-of-caffe2-PoolPointsInterpOp\"><a href=\"#2-6-GetSingleArgument-is-not-a-member-of-caffe2-PoolPointsInterpOp\" class=\"headerlink\" title=\"2.6 GetSingleArgument is not a member of caffe2::PoolPointsInterpOp\"></a>2.6 GetSingleArgument<float> is not a member of caffe2::PoolPointsInterpOp<t, context=\"\"></t,></float></h3><h4 id=\"Details-5\"><a href=\"#Details-5\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make ops</code>.<br>Main error message:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/path/to/pytorch/caffe2/operators/accumulate_op.h: In constructor caffe2::AccumulateOp&lt;T, Context&gt;::AccumulateOp(const caffe2::OperatorDef&amp;, caffe2::Workspace*):</span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/accumulate_op.h:13:187: error: GetSingleArgument&lt;<span class=\"built_in\">float</span>&gt; is not a member of caffe2::AccumulateOp&lt;T, Context&gt;</span><br><span class=\"line\">   AccumulateOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class=\"line\">                                                                                                                                                                                           ^                        </span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/elementwise_ops.h: In constructor caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;::BinaryElementwiseWithArgsOp(const caffe2::OperatorDef&amp;, caffe2::Workspace*):</span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:189: error: GetSingleArgument&lt;bool&gt; is not a member of caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;</span><br><span class=\"line\">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class=\"line\">                                                                                                                                                                                             ^                       </span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:272: error: GetSingleArgument&lt;int&gt; is not a member of caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;</span><br><span class=\"line\">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class=\"line\">                                                                                                                                                                                                                                                                                ^                      </span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:350: error: GetSingleArgument&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; is not a member of caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;</span><br><span class=\"line\">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"Cause-5\"><a href=\"#Cause-5\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>I&#39;m not sure. Could be that <code>GetSingleArgument()</code> is defined elsewhere?</p>\n<h4 id=\"Solution-5\"><a href=\"#Solution-5\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Modify <code>/path/to/densepose/detectron/ops/pool_points_interp.h</code>. Change <code>OperatorBase::GetSingleArgument&lt;float&gt;</code> to <code>this-&gt;template GetSingleArgument&lt;float&gt;</code></p>\n<p>(Thanks to badpx@Github: <a href=\"https://github.com/facebookresearch/DensePose/pull/137/commits/51389c6a02173a25e9429825db452beb5e1cf3be\" target=\"_blank\" rel=\"noopener\">https://github.com/facebookresearch/DensePose/pull/137/commits/51389c6a02173a25e9429825db452beb5e1cf3be</a>) </p>\n<h3 id=\"2-7-Undefined-symbol-ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE\"><a href=\"#2-7-Undefined-symbol-ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE\" class=\"headerlink\" title=\"2.7 Undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE\"></a>2.7 Undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE</h3><h4 id=\"Details-6\"><a href=\"#Details-6\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>python detectron/tests/test_zero_even_op.py</code>.</p>\n<p>Main error message:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">OSError: /path/to/densepose/build/libcaffe2_detectron_custom_ops_gpu.so: undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"Cause-6\"><a href=\"#Cause-6\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>WTF is this!???<br>As can be seen, this symbol has something to do with Google, and protobuf.<br>I guess this is caused by a different protobuf version. Good news is that a proper version of protobuf was also built with Caffe2, so why not tell this to DensePose?</p>\n<h4 id=\"Solution-6\"><a href=\"#Solution-6\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>In <code>/path/to/densepose/CMakeLists.txt</code>, Add a few lines in the beginning:<br><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">add_library</span>(libprotobuf STATIC IMPORTED) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">set</span>(PROTOBUF_LIB <span class=\"string\">\"/path/to/pytorch/torch/lib/libprotobuf.a\"</span>) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">set_property</span>(TARGET libprotobuf PROPERTY IMPORTED_LOCATION <span class=\"string\">\"$&#123;PROTOBUF_LIB&#125;\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>You can find two <code>target_link_libraries</code> lines in this file(they are not adjacent):<br><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">target_link_libraries</span>(caffe2_detectron_custom_ops caffe2_library) </span><br><span class=\"line\"><span class=\"keyword\">target_link_libraries</span>(caffe2_detectron_custom_ops_gpu caffe2_gpu_library)</span><br></pre></td></tr></table></figure></p>\n<p>Edit the two lines, adding a &quot;libprotobuf&quot; at the end to each of them:<br><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">target_link_libraries</span>(caffe2_detectron_custom_ops caffe2_library libprotobuf) </span><br><span class=\"line\"><span class=\"keyword\">target_link_libraries</span>(caffe2_detectron_custom_ops_gpu caffe2_gpu_library libprotobuf)</span><br></pre></td></tr></table></figure></p>\n<p>Then run <code>make ops</code> again, and <code>python detectron/tests/test_zero_even_op.py</code> again.</p>\n<p>(Thanks to hyounsamk@Github: <a href=\"https://github.com/facebookresearch/DensePose/issues/119\" target=\"_blank\" rel=\"noopener\">https://github.com/facebookresearch/DensePose/issues/119</a>)</p>\n<p>After fixing this issue, my DensePose passed tests and was running flawlessly. If any more issues remain, don&#39;t hesitate to comment here~</p>\n<h2 id=\"0-Motivation\"><a href=\"#0-Motivation\" class=\"headerlink\" title=\"0 Motivation\"></a>0 Motivation</h2><p>Starting from this post, I decide to keep a record (tag: MineSweeping) of the issues I meet while working with environments and also their solutions. </p>\n<p>Doing configurations in order to run others&#39; code may be a difficult task, and is sometimes depressing, since various issues could arise, and the it&#39;s impossible for the authors to keep providing solutions for every user in the community. What&#39;s worse, after fixing some problems with a lot of struggle, one may have to waste the same amount of time on the same issue the next time he/she run it again. That&#39;s why I decide to keep this record: to avoid wasting time twice, while also helping others deal with problems if possible.</p>\n","site":{"data":{}},"excerpt":"<p><br><br>DensePose is a great work in real-time human pose estimation, which is based on Caffe2 and Detectron framework. It extracts dense human body 3D surface based on RGB images. The installation instructions are provided <a href=\"https://github.com/facebookresearch/DensePose/blob/master/INSTALL.md\" target=\"_blank\" rel=\"noopener\">here</a>.</p>\n<p>During my installation process, these are the problems that took me some time to tackle. I spent on week to finally figure out solutions to all the issues. So lucky of me not to give up too early...</p>\n<div align=\"center\"><br>    <img src=\"/images/densepose-ms/facebook.jpg\" width=\"15%\" height=\"15%\" alt=\"Greetings from Facebook AI Research\"><br></div>","more":"<h2 id=\"1-Environment\"><a href=\"#1-Environment\" class=\"headerlink\" title=\"1 Environment\"></a>1 Environment</h2><ul>\n<li>System: Ubuntu 18.04</li>\n<li>Linux kernel: 4.15.0-29-generic</li>\n<li>Graphics card: NVIDIA GeForce 1080Ti</li>\n<li>Graphics driver: 410.48</li>\n<li>CUDA: 10.0.130</li>\n<li>cuDNN: 7.3.1</li>\n<li>Caffe2: Built from source</li>\n<li>Python: 2.7.15, based on Anaconda 4.5.11</li>\n</ul>\n<h2 id=\"2-Problems-amp-Solutions\"><a href=\"#2-Problems-amp-Solutions\" class=\"headerlink\" title=\"2 Problems &amp; Solutions\"></a>2 Problems &amp; Solutions</h2><h3 id=\"2-1-Caffe2-module-not-found\"><a href=\"#2-1-Caffe2-module-not-found\" class=\"headerlink\" title=\"2.1 Caffe2 module not found\"></a>2.1 Caffe2 module not found</h3><h4 id=\"Details\"><a href=\"#Details\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make</code>.</p>\n<p>Main error message:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Could not find a package configuration file provided by <span class=\"string\">\"Caffe2\"</span> with any  </span><br><span class=\"line\">of the following names: </span><br><span class=\"line\">    Caffe2Config.cmake </span><br><span class=\"line\">    caffe2-config.cmake</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"Cause\"><a href=\"#Cause\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>Caffe2 build path isn&#39;t known by CMake.</p>\n<h4 id=\"Solution\"><a href=\"#Solution\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">set</span>(Caffe2_DIR <span class=\"string\">\"/path/to/pytorch/torch/share/cmake/Caffe2/\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>(Note: <code>set(Caffe2_DIR &quot;/path/to/pytorch/build/&quot;)</code> can also fix this issue but may cause other issues.) </p>\n<h3 id=\"2-2-Detectron-ops-lib-not-found\"><a href=\"#2-2-Detectron-ops-lib-not-found\" class=\"headerlink\" title=\"2.2 Detectron ops lib not found\"></a>2.2 Detectron ops lib not found</h3><h4 id=\"Details-1\"><a href=\"#Details-1\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>python2 $DENSEPOSE/detectron/tests/test_spatial_narrow_as_op.py</code> after <code>make</code>.</p>\n<p>Main error message:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Detectron ops lib not found; make sure that your Caffe2 version includes Detectron module.</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"Cause-1\"><a href=\"#Cause-1\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>Seems that the Python part of DensePose couldn&#39;t recognize Caffe2.</p>\n<h4 id=\"Solution-1\"><a href=\"#Solution-1\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Add <code>/path/to/pytorch</code> to <code>PYTHONPATH</code> environment variable. Could be added by directly <code>export PYTHONPATH=$PYTHONPATH:/path/to/pytorch</code> instruction or by adding this line to <code>~/.bashrc</code>. Remember to run <code>source ~/.bashrc</code> after the modification.</p>\n<h3 id=\"2-3-cmake-files-not-found-amp-Unknown-CMake-command-quot-caffe2-interface-library-quot\"><a href=\"#2-3-cmake-files-not-found-amp-Unknown-CMake-command-quot-caffe2-interface-library-quot\" class=\"headerlink\" title=\"2.3 *.cmake files not found &amp; Unknown CMake command &quot;caffe2_interface_library&quot;\"></a>2.3 *.cmake files not found &amp; Unknown CMake command &quot;caffe2_interface_library&quot;</h3><h4 id=\"Details-2\"><a href=\"#Details-2\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p>\n<p>Main error message:<br><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CMake Error at /path/to/pytorch/build/Caffe2Config.cmake:<span class=\"number\">14</span> (<span class=\"keyword\">include</span>):</span><br><span class=\"line\">  <span class=\"keyword\">include</span> could not find load file:</span><br><span class=\"line\"></span><br><span class=\"line\">    /path/to/pytorch/build/public/utils.cmake</span><br><span class=\"line\">    /path/to/pytorch/build/public/threads.cmake</span><br><span class=\"line\">    /path/to/pytorch/build/public/cuda.cmake</span><br><span class=\"line\">    /path/to/pytorch/build/public/mkl.cmake</span><br><span class=\"line\">    /path/to/pytorch/build/Caffe2Targets.cmake</span><br><span class=\"line\"></span><br><span class=\"line\">Call Stack (most recent call first):</span><br><span class=\"line\">  CMakeLists.txt:<span class=\"number\">8</span> (<span class=\"keyword\">find_package</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">CMake Error at /path/to/pytorch/build/Caffe2Config.cmake:<span class=\"number\">117</span> (caffe2_interface_library):</span><br><span class=\"line\">  Unknown CMake command <span class=\"string\">\"caffe2_interface_library\"</span>.</span><br><span class=\"line\">Call Stack (most recent call first):</span><br><span class=\"line\">  CMakeLists.txt:<span class=\"number\">8</span> (<span class=\"keyword\">find_package</span>)</span><br></pre></td></tr></table></figure></p>\n<p>(Several <code>*.cmake</code> files, I only showed a few.)</p>\n<h4 id=\"Cause-2\"><a href=\"#Cause-2\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>These files are not in the <code>pytorch/build</code> directory. By searching, I found that they are in the <code>pytorch/torch/share/cmake/Caffe2</code> directory.</p>\n<h4 id=\"Solution-2\"><a href=\"#Solution-2\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">set</span>(Caffe2_DIR <span class=\"string\">\"/path/to/pytorch/torch/share/cmake/Caffe2/\"</span>)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-4-quot-context-gpu-h-quot-not-found\"><a href=\"#2-4-quot-context-gpu-h-quot-not-found\" class=\"headerlink\" title=\"2.4 &quot;context_gpu.h&quot; not found.\"></a>2.4 &quot;context_gpu.h&quot; not found.</h3><h4 id=\"Details-3\"><a href=\"#Details-3\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p>\n<p>I forgot to record the error messages, but it should be obvious that some header files(not just <code>context_gpu.h</code>) are missing.</p>\n<h4 id=\"Cause-3\"><a href=\"#Cause-3\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>This time it&#39;s the include path not recognized...</p>\n<h4 id=\"Solution-3\"><a href=\"#Solution-3\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">include_directories</span>(<span class=\"string\">\"/path/to/pytorch/torch/lib/include\"</span>)</span><br></pre></td></tr></table></figure></p>\n<h3 id=\"2-5-quot-mkl-cblas-h-quot-not-found\"><a href=\"#2-5-quot-mkl-cblas-h-quot-not-found\" class=\"headerlink\" title=\"2.5 &quot;mkl_cblas.h&quot; not found.\"></a>2.5 &quot;mkl_cblas.h&quot; not found.</h3><h4 id=\"Details-4\"><a href=\"#Details-4\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p>\n<p>I forgot to record the error messages, but it should be obvious too.</p>\n<h4 id=\"Cause-4\"><a href=\"#Cause-4\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>Intel Math Kernel Library was turned on but not found. (Why is it enabled when I didn&#39;t even install it???)</p>\n<h4 id=\"Solution-4\"><a href=\"#Solution-4\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Install Intel Math Kernel Library <a href=\"https://software.intel.com/en-us/mkl/choose-download/linux\" target=\"_blank\" rel=\"noopener\">here</a> and add <code>/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/include</code> to <code>C_PATH</code> environment variable. The exact path may vary according to the MKL version and your configuration.<br>Maybe try <code>find / -name mkl_cblas.h</code> to make sure of its location after the installation.</p>\n<h3 id=\"2-6-GetSingleArgument-is-not-a-member-of-caffe2-PoolPointsInterpOp\"><a href=\"#2-6-GetSingleArgument-is-not-a-member-of-caffe2-PoolPointsInterpOp\" class=\"headerlink\" title=\"2.6 GetSingleArgument is not a member of caffe2::PoolPointsInterpOp\"></a>2.6 GetSingleArgument<float> is not a member of caffe2::PoolPointsInterpOp<t, context=\"\"></t,></float></h3><h4 id=\"Details-5\"><a href=\"#Details-5\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>make ops</code>.<br>Main error message:<br><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">/path/to/pytorch/caffe2/operators/accumulate_op.h: In constructor caffe2::AccumulateOp&lt;T, Context&gt;::AccumulateOp(const caffe2::OperatorDef&amp;, caffe2::Workspace*):</span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/accumulate_op.h:13:187: error: GetSingleArgument&lt;<span class=\"built_in\">float</span>&gt; is not a member of caffe2::AccumulateOp&lt;T, Context&gt;</span><br><span class=\"line\">   AccumulateOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class=\"line\">                                                                                                                                                                                           ^                        </span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/elementwise_ops.h: In constructor caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;::BinaryElementwiseWithArgsOp(const caffe2::OperatorDef&amp;, caffe2::Workspace*):</span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:189: error: GetSingleArgument&lt;bool&gt; is not a member of caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;</span><br><span class=\"line\">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class=\"line\">                                                                                                                                                                                             ^                       </span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:272: error: GetSingleArgument&lt;int&gt; is not a member of caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;</span><br><span class=\"line\">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class=\"line\">                                                                                                                                                                                                                                                                                ^                      </span><br><span class=\"line\">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:350: error: GetSingleArgument&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; is not a member of caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;</span><br><span class=\"line\">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"Cause-5\"><a href=\"#Cause-5\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>I&#39;m not sure. Could be that <code>GetSingleArgument()</code> is defined elsewhere?</p>\n<h4 id=\"Solution-5\"><a href=\"#Solution-5\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>Modify <code>/path/to/densepose/detectron/ops/pool_points_interp.h</code>. Change <code>OperatorBase::GetSingleArgument&lt;float&gt;</code> to <code>this-&gt;template GetSingleArgument&lt;float&gt;</code></p>\n<p>(Thanks to badpx@Github: <a href=\"https://github.com/facebookresearch/DensePose/pull/137/commits/51389c6a02173a25e9429825db452beb5e1cf3be\" target=\"_blank\" rel=\"noopener\">https://github.com/facebookresearch/DensePose/pull/137/commits/51389c6a02173a25e9429825db452beb5e1cf3be</a>) </p>\n<h3 id=\"2-7-Undefined-symbol-ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE\"><a href=\"#2-7-Undefined-symbol-ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE\" class=\"headerlink\" title=\"2.7 Undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE\"></a>2.7 Undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE</h3><h4 id=\"Details-6\"><a href=\"#Details-6\" class=\"headerlink\" title=\"Details\"></a>Details</h4><p>Occurred when running <code>python detectron/tests/test_zero_even_op.py</code>.</p>\n<p>Main error message:<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">OSError: /path/to/densepose/build/libcaffe2_detectron_custom_ops_gpu.so: undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE</span><br></pre></td></tr></table></figure></p>\n<h4 id=\"Cause-6\"><a href=\"#Cause-6\" class=\"headerlink\" title=\"Cause\"></a>Cause</h4><p>WTF is this!???<br>As can be seen, this symbol has something to do with Google, and protobuf.<br>I guess this is caused by a different protobuf version. Good news is that a proper version of protobuf was also built with Caffe2, so why not tell this to DensePose?</p>\n<h4 id=\"Solution-6\"><a href=\"#Solution-6\" class=\"headerlink\" title=\"Solution\"></a>Solution</h4><p>In <code>/path/to/densepose/CMakeLists.txt</code>, Add a few lines in the beginning:<br><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">add_library</span>(libprotobuf STATIC IMPORTED) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">set</span>(PROTOBUF_LIB <span class=\"string\">\"/path/to/pytorch/torch/lib/libprotobuf.a\"</span>) </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">set_property</span>(TARGET libprotobuf PROPERTY IMPORTED_LOCATION <span class=\"string\">\"$&#123;PROTOBUF_LIB&#125;\"</span>)</span><br></pre></td></tr></table></figure></p>\n<p>You can find two <code>target_link_libraries</code> lines in this file(they are not adjacent):<br><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">target_link_libraries</span>(caffe2_detectron_custom_ops caffe2_library) </span><br><span class=\"line\"><span class=\"keyword\">target_link_libraries</span>(caffe2_detectron_custom_ops_gpu caffe2_gpu_library)</span><br></pre></td></tr></table></figure></p>\n<p>Edit the two lines, adding a &quot;libprotobuf&quot; at the end to each of them:<br><figure class=\"highlight cmake\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">target_link_libraries</span>(caffe2_detectron_custom_ops caffe2_library libprotobuf) </span><br><span class=\"line\"><span class=\"keyword\">target_link_libraries</span>(caffe2_detectron_custom_ops_gpu caffe2_gpu_library libprotobuf)</span><br></pre></td></tr></table></figure></p>\n<p>Then run <code>make ops</code> again, and <code>python detectron/tests/test_zero_even_op.py</code> again.</p>\n<p>(Thanks to hyounsamk@Github: <a href=\"https://github.com/facebookresearch/DensePose/issues/119\" target=\"_blank\" rel=\"noopener\">https://github.com/facebookresearch/DensePose/issues/119</a>)</p>\n<p>After fixing this issue, my DensePose passed tests and was running flawlessly. If any more issues remain, don&#39;t hesitate to comment here~</p>\n<h2 id=\"0-Motivation\"><a href=\"#0-Motivation\" class=\"headerlink\" title=\"0 Motivation\"></a>0 Motivation</h2><p>Starting from this post, I decide to keep a record (tag: MineSweeping) of the issues I meet while working with environments and also their solutions. </p>\n<p>Doing configurations in order to run others&#39; code may be a difficult task, and is sometimes depressing, since various issues could arise, and the it&#39;s impossible for the authors to keep providing solutions for every user in the community. What&#39;s worse, after fixing some problems with a lot of struggle, one may have to waste the same amount of time on the same issue the next time he/she run it again. That&#39;s why I decide to keep this record: to avoid wasting time twice, while also helping others deal with problems if possible.</p>"},{"title":"A Review of ResNet - Residual Networks","date":"2018-04-22T05:55:35.000Z","_content":"\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n## 0 Introduction\nDeep learning researchers have been constructing skyscrapers in recent years. Especially, VGG nets and GoogLeNet have pushed the depths of convolutional networks to the extreme. But questions remain: if time and money aren't problems, are deeper networks always performing better? Not exactly.\n\nWhen residual networks were proposed, researchers around the world was stunned by its depth. \"Jesus Christ! Is this a neural network or the Dubai Tower?\" But **don't be afraid!** These networks are deep but the structures are simple. Interestingly, these networks not only defeated all opponents in the classification, detection, localization challenges in ImageNet 2015, but were also the main innovation in the best paper of CVPR2016.\n\n<div align=\"center\">\n    <img src=\"/images/resnet/network_growth.jpg\" width=\"40%\" height=\"40%\" alt=\"Network Growth\">\n</div>\n\n<!-- more -->\n\n## 1 The Crisis: Degradation of Deep Networks\n\nVGG nets proved the beneficial of representation depth of convolutional neural networks, at least within a certain range, to be exact. However, when Kaiming He et al. tried to deepen some plain networks, the training error and test error stopped decreasing after the network reached a certain depth(which is not surprising) and soon degraded. This is not an overfitting problem, because training errors also increased; nor is it a gradient vanishing problem, because there are some techniques(e.g. batch normalization[4]) that ease the pain.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/downgrade.png\" width=\"60%\" height=\"60%\" alt=\"The Downgrade Problem\">\n    Fig.1 The downgrade problem\n\n</div>\n\nWhat seems to be the cause of this degradation? Obviously, deeper neural networks are more difficult to train, but that doesn't mean deeper neural networks would yield worse results. To explain this problem, Balduzzi et al.[3] identified shattered gradient problem - as depth increases, gradients in standard feedforward networks increasingly resemble white noise. I will write about that later.\n\n## 2 A Closer Look at ResNet: The Residual Blocks\n\nAs the old saying goes, \"\". Although ResNets are as deep as a thousand layers, they are built with these basic residual blocks(the right part of the figure). \n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/residual_blocks.png\" width=\"50%\" height=\"50%\" alt=\"Comparison between normal weight layers and residual blocks\">\n    Fig.2 Parts of plain networks and a residual block(or residual unit)\n\n</div>\n\n### 2.1 Skip Connections\n\nIn comparison, basic units of plain network models would look like the one on the left: one ReLU function after a weight layer(usually also with biases), repeated several times. Let's denote the desired underlying mapping(the ideal mapping) of the two layers as \\\\(\\mathcal{H}(x)\\\\), and the real mapping as \\\\(\\mathcal{F}(x)\\\\). Clearly, the closer \\\\(\\mathcal{F}(x)\\\\) is to \\\\(\\mathcal{H}(x)\\\\), the better it fits.\n\nHowever, He et al. explicitly let these layers fit a residual mapping instead of the desired underlying mapping. This is implemented with \"shortcut connections\", which skip one or more layers, simply performing identity mappings and getting added to the outputs of the stacked weight layers. This way, \\\\(\\mathcal{F}(x)\\\\) would not try to fit \\\\(\\mathcal{H}(x)\\\\), but \\\\(\\mathcal{H}(x)-x\\\\). The whole structure(from the identity mapping branch, to merging the branches by the addition operation) are named \"residual blocks\"(or \"residual units\").\n\nWhat's the point in this? Let's do a simple analysis. The computation done by the original residual block is: $$y_l=h(x_l)+\\mathcal{F}(x_l,\\mathcal{W}_l),$$ $$x_{l+1}=f(y_l).$$\n\nHere are the definitions of symbols: \n\\\\(x_l\\\\): input features to the \\\\(l\\\\)-th residual block; \n\\\\(\\mathcal{W}_{l}={W_{l,k}|_{1\\leq k\\leq K}}\\\\): a set of weights(and biases) associated with the \\\\(l\\\\)-th residual unit. \\\\(K\\\\) is the number of layers in this block;\n\\\\(\\mathcal{F}(x,\\mathcal{W})\\\\): the residual function, which we talked about earlier. It's a stack of 2 conv. layers here;\n\\\\(f(x)\\\\): the activation function. We are using ReLU here;\n\\\\(h(x)\\\\): identity mapping.\n\nIf \\\\(f(x)\\\\) is also an identity mapping(as if we're not using any activation function), the first equation would become:\n$$x_{l+1}=x_l+\\mathcal{F}(x_l,\\mathcal{W}_l)$$\n\nTherefore, we can define \\\\(x_L\\\\) recursively of any layer:\n$$x_L=x_l+\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)$$\n\nThat's not the end yet! When it comes to the gradients, according to the chain rules of backpropagation, we have a beautiful definition:\n$$\\begin{split}\n\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} & = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\\\\\n& = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)\n\\end{split}$$\n\nWhat does it mean? It means that the information is directly backpropagated to ANY shallower block. This way, the gradients of a layer never vanish or explode even if the weights are too small or too big.\n\n### 2.2 Identity Mappings\nIt's important that we use identity mapping here! Just consider doing a simple modification here, for example, \\\\(h(x)=\\lambda_lx_l\\\\)(\\\\(\\lambda_l\\\\) is a modulating scalar). The definition of \\\\(x_L\\\\) and \\\\(\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}}\\\\) would become:\n$$x_L=(\\prod_{i=l}^{L-1}\\lambda_i)x_l+\\sum_{i=l}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i,\\mathcal{W}_i)$$\n$$\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}}=\\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big((\\prod_{i=l}^{L-1}\\lambda_i)+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)$$\n\nFor extremely deep neural networks where \\\\(L\\\\) is too large, \\\\(\\prod_{i=l}^{L-1}\\lambda_i\\\\) could be either too small or too large, causing gradient vanishing or gradient explosion. For \\\\(h(x)\\\\) with complex definitions, the gradient could be extremely complicated, thus losing the advantage of the skip connection. Skip connection works best under the condition where the grey channel in Fig. 3 cover no operations (except the addition) and is clean.\n\nInterestingly, this comfirmed the philosophy of \"\" once again.\n\n### 2.3 Post-activation or Pre-activation?\n\nWait a second... \"\\\\(f(x)\\\\) is also an identity mapping\" is just our assumption. The activation function is still there!\n\nRight. There IS an activation function, but it's moved to somewhere else.  In fact, the original residual block is still a little bit problematic - the output of one residual block is not always the input of the next, since there is a ReLU activation function after the addition(It did NOT REALLY keep the identity mapping to the next block!). Therefore, in[2], He et al. fixed the residual blocks by changing the order of operations.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/identity_mapping.png\" width=\"30%\" height=\"30%\" alt=\"New identity mapping\">\n    Fig.3 New identity mapping proposed by He et al.\n\n</div>\n\nBesides using a simple identity mapping, He et al. also discussed about the position of the activation function and the batch normalization operation. Assuming that we got a special(asymmetric) activation function \\\\(\\hat f(x)\\\\), which only affects the path to the next residual unit. Now our definition of \\\\(x_{x+1}\\\\) would become:\n$$x_{l+1}=x_l+\\mathcal{F}(\\hat f(x_l),\\mathcal{W}_l)$$\n\nWith \\\\(x_l\\\\) still multiplied by 1, information is still fully backpropagated to shallower residual blocks. And the good thing is that using this asymmetric activation function after the addition(partial post-activation) is equivalent to using it beforehand(pre-activation)! This is why He et al. chose to use pre-activation - otherwise it would be necessary to implement that magical activation function \\\\(\\hat f(x)\\\\).\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/pre-activation.png\" width=\"80%\" height=\"80%\" alt=\"Asymmetric after-addition activation\">\n    Fig.4 Using asymmetric after-addition activation is equivalent to constructing a pre-activation residual unit\n\n</div>\n\n## 3 ResNet Architectures\n\nHere are the ResNet architectures for ImageNet. Building blocks are shown in brackets, with the numbers of blocks stacked. With the first block of every stack(starting from conv3_x), a downsampling is performed. Each column represents one of the residual networks, and the deepest one has 152 weight layers! Since ResNets were proposed, VGG nets - which were officially called \"Very Deep Convolutional Networks\" - are not relatively deep anymore. Maybe call them \"A Little Bit Deep Convolutional Networks\".\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/architectures.png\" width=\"80%\" height=\"80%\" alt=\"ResNet architectures for ImageNet\">\n    Table. 1 ResNet architectures for ImageNet.\n\n</div>\n\n## 4 Experiments\n\n### 4.1 Performance on ImageNet\nHe et al. trained ResNet-18 and ResNet-34 on the ImageNet dataset, and also compared them to plain convolutional networks. In Fig. 5, the thin curves denote training error, and the bold ones denote validation error. The figure on the left shows the results of plain convolution networks(in which the 34-layered ones has higher error rates than the 18-layered one), and the figure on the right shows that residual networks perform better than plain ones, while deeper ones perform better than shallow ones.\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/training.png\" width=\"80%\" height=\"80%\" alt=\"Training ResNet on ImageNet\"/>\n    Fig. 5 Training ResNet on ImageNet\n\n</div>\n\n### 4.2 Effects of Different Shortcut Connections\n\nHe et al. also tried various types of shortcut connections to replace the identity mapping, and various positions of activation functions / batch normalization. Experiments show that the original identity mapping and full pre-activation yield the best results.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/shortcut-connections.png\" width=\"50%\" height=\"50%\" alt=\"Different shortcuts of residual units\"/>\n    Fig. 6 Various shortcuts in residual units\n\n    <img src=\"/images/resnet/shortcut-connections-experiment.png\" width=\"70%\" height=\"70%\" alt=\"Classification errors with different shortcuts\"/>\n    Table. 2 Classification error on CIFAR-10 test set with various shortcut connections in residual units\n\n    <img src=\"/images/resnet/activations.png\" width=\"70%\" height=\"50%\" alt=\"Different usages of activation in residual units\"/>\n    Fig. 7 Various usages of activation in residual units\n\n    <img src=\"/images/resnet/activations-experiment.png\" width=\"50%\" height=\"50%\" alt=\"Classification errors with different activations\"/>\n    Table. 3 Classification error on CIFAR-10 test set with various usages of activation in residual units\n\n</div>\n\n\n## 5 Conclusion\nResidual learning can be crowned as \"ONE OF THE GREATEST HITS IN DEEP LEARNING FIELDS\". With a simple identity mapping, it solved the degradation problem of deep neural networks. Now that you have learned about the concept of ResNet, why not give it a try and implement your first residual learning model today?\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/resnet-yooo.jpg\" width=\"40%\" height=\"40%\" alt=\"\">\n</div>\n\n## References\n\n[1] [He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.](http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n\n[2] [He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks[C]//European Conference on Computer Vision. Springer, Cham, 2016: 630-645.](https://arxiv.org/pdf/1603.05027.pdf)\n\n[3] [Balduzzi D, Frean M, Leary L, et al. The Shattered Gradients Problem: If resnets are the answer, then what is the question?[J]. arXiv preprint arXiv:1702.08591, 2017.](https://arxiv.org/pdf/1702.08591.pdf)\n\n[4] [Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.](https://arxiv.org/pdf/1502.03167.pdf)","source":"_posts/resnet.md","raw":"---\ntitle: A Review of ResNet - Residual Networks\ndate: 2018-04-22 14:55:35\ntags: [Deep Learning, Computer Vision]\n---\n\n<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n## 0 Introduction\nDeep learning researchers have been constructing skyscrapers in recent years. Especially, VGG nets and GoogLeNet have pushed the depths of convolutional networks to the extreme. But questions remain: if time and money aren't problems, are deeper networks always performing better? Not exactly.\n\nWhen residual networks were proposed, researchers around the world was stunned by its depth. \"Jesus Christ! Is this a neural network or the Dubai Tower?\" But **don't be afraid!** These networks are deep but the structures are simple. Interestingly, these networks not only defeated all opponents in the classification, detection, localization challenges in ImageNet 2015, but were also the main innovation in the best paper of CVPR2016.\n\n<div align=\"center\">\n    <img src=\"/images/resnet/network_growth.jpg\" width=\"40%\" height=\"40%\" alt=\"Network Growth\">\n</div>\n\n<!-- more -->\n\n## 1 The Crisis: Degradation of Deep Networks\n\nVGG nets proved the beneficial of representation depth of convolutional neural networks, at least within a certain range, to be exact. However, when Kaiming He et al. tried to deepen some plain networks, the training error and test error stopped decreasing after the network reached a certain depth(which is not surprising) and soon degraded. This is not an overfitting problem, because training errors also increased; nor is it a gradient vanishing problem, because there are some techniques(e.g. batch normalization[4]) that ease the pain.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/downgrade.png\" width=\"60%\" height=\"60%\" alt=\"The Downgrade Problem\">\n    Fig.1 The downgrade problem\n\n</div>\n\nWhat seems to be the cause of this degradation? Obviously, deeper neural networks are more difficult to train, but that doesn't mean deeper neural networks would yield worse results. To explain this problem, Balduzzi et al.[3] identified shattered gradient problem - as depth increases, gradients in standard feedforward networks increasingly resemble white noise. I will write about that later.\n\n## 2 A Closer Look at ResNet: The Residual Blocks\n\nAs the old saying goes, \"\". Although ResNets are as deep as a thousand layers, they are built with these basic residual blocks(the right part of the figure). \n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/residual_blocks.png\" width=\"50%\" height=\"50%\" alt=\"Comparison between normal weight layers and residual blocks\">\n    Fig.2 Parts of plain networks and a residual block(or residual unit)\n\n</div>\n\n### 2.1 Skip Connections\n\nIn comparison, basic units of plain network models would look like the one on the left: one ReLU function after a weight layer(usually also with biases), repeated several times. Let's denote the desired underlying mapping(the ideal mapping) of the two layers as \\\\(\\mathcal{H}(x)\\\\), and the real mapping as \\\\(\\mathcal{F}(x)\\\\). Clearly, the closer \\\\(\\mathcal{F}(x)\\\\) is to \\\\(\\mathcal{H}(x)\\\\), the better it fits.\n\nHowever, He et al. explicitly let these layers fit a residual mapping instead of the desired underlying mapping. This is implemented with \"shortcut connections\", which skip one or more layers, simply performing identity mappings and getting added to the outputs of the stacked weight layers. This way, \\\\(\\mathcal{F}(x)\\\\) would not try to fit \\\\(\\mathcal{H}(x)\\\\), but \\\\(\\mathcal{H}(x)-x\\\\). The whole structure(from the identity mapping branch, to merging the branches by the addition operation) are named \"residual blocks\"(or \"residual units\").\n\nWhat's the point in this? Let's do a simple analysis. The computation done by the original residual block is: $$y_l=h(x_l)+\\mathcal{F}(x_l,\\mathcal{W}_l),$$ $$x_{l+1}=f(y_l).$$\n\nHere are the definitions of symbols: \n\\\\(x_l\\\\): input features to the \\\\(l\\\\)-th residual block; \n\\\\(\\mathcal{W}_{l}={W_{l,k}|_{1\\leq k\\leq K}}\\\\): a set of weights(and biases) associated with the \\\\(l\\\\)-th residual unit. \\\\(K\\\\) is the number of layers in this block;\n\\\\(\\mathcal{F}(x,\\mathcal{W})\\\\): the residual function, which we talked about earlier. It's a stack of 2 conv. layers here;\n\\\\(f(x)\\\\): the activation function. We are using ReLU here;\n\\\\(h(x)\\\\): identity mapping.\n\nIf \\\\(f(x)\\\\) is also an identity mapping(as if we're not using any activation function), the first equation would become:\n$$x_{l+1}=x_l+\\mathcal{F}(x_l,\\mathcal{W}_l)$$\n\nTherefore, we can define \\\\(x_L\\\\) recursively of any layer:\n$$x_L=x_l+\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)$$\n\nThat's not the end yet! When it comes to the gradients, according to the chain rules of backpropagation, we have a beautiful definition:\n$$\\begin{split}\n\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} & = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\\\\\\n& = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)\n\\end{split}$$\n\nWhat does it mean? It means that the information is directly backpropagated to ANY shallower block. This way, the gradients of a layer never vanish or explode even if the weights are too small or too big.\n\n### 2.2 Identity Mappings\nIt's important that we use identity mapping here! Just consider doing a simple modification here, for example, \\\\(h(x)=\\lambda_lx_l\\\\)(\\\\(\\lambda_l\\\\) is a modulating scalar). The definition of \\\\(x_L\\\\) and \\\\(\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}}\\\\) would become:\n$$x_L=(\\prod_{i=l}^{L-1}\\lambda_i)x_l+\\sum_{i=l}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i,\\mathcal{W}_i)$$\n$$\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}}=\\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big((\\prod_{i=l}^{L-1}\\lambda_i)+\\frac{\\partial{}}{\\partial{x_l}}\\sum_{i=l}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)$$\n\nFor extremely deep neural networks where \\\\(L\\\\) is too large, \\\\(\\prod_{i=l}^{L-1}\\lambda_i\\\\) could be either too small or too large, causing gradient vanishing or gradient explosion. For \\\\(h(x)\\\\) with complex definitions, the gradient could be extremely complicated, thus losing the advantage of the skip connection. Skip connection works best under the condition where the grey channel in Fig. 3 cover no operations (except the addition) and is clean.\n\nInterestingly, this comfirmed the philosophy of \"\" once again.\n\n### 2.3 Post-activation or Pre-activation?\n\nWait a second... \"\\\\(f(x)\\\\) is also an identity mapping\" is just our assumption. The activation function is still there!\n\nRight. There IS an activation function, but it's moved to somewhere else.  In fact, the original residual block is still a little bit problematic - the output of one residual block is not always the input of the next, since there is a ReLU activation function after the addition(It did NOT REALLY keep the identity mapping to the next block!). Therefore, in[2], He et al. fixed the residual blocks by changing the order of operations.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/identity_mapping.png\" width=\"30%\" height=\"30%\" alt=\"New identity mapping\">\n    Fig.3 New identity mapping proposed by He et al.\n\n</div>\n\nBesides using a simple identity mapping, He et al. also discussed about the position of the activation function and the batch normalization operation. Assuming that we got a special(asymmetric) activation function \\\\(\\hat f(x)\\\\), which only affects the path to the next residual unit. Now our definition of \\\\(x_{x+1}\\\\) would become:\n$$x_{l+1}=x_l+\\mathcal{F}(\\hat f(x_l),\\mathcal{W}_l)$$\n\nWith \\\\(x_l\\\\) still multiplied by 1, information is still fully backpropagated to shallower residual blocks. And the good thing is that using this asymmetric activation function after the addition(partial post-activation) is equivalent to using it beforehand(pre-activation)! This is why He et al. chose to use pre-activation - otherwise it would be necessary to implement that magical activation function \\\\(\\hat f(x)\\\\).\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/pre-activation.png\" width=\"80%\" height=\"80%\" alt=\"Asymmetric after-addition activation\">\n    Fig.4 Using asymmetric after-addition activation is equivalent to constructing a pre-activation residual unit\n\n</div>\n\n## 3 ResNet Architectures\n\nHere are the ResNet architectures for ImageNet. Building blocks are shown in brackets, with the numbers of blocks stacked. With the first block of every stack(starting from conv3_x), a downsampling is performed. Each column represents one of the residual networks, and the deepest one has 152 weight layers! Since ResNets were proposed, VGG nets - which were officially called \"Very Deep Convolutional Networks\" - are not relatively deep anymore. Maybe call them \"A Little Bit Deep Convolutional Networks\".\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/architectures.png\" width=\"80%\" height=\"80%\" alt=\"ResNet architectures for ImageNet\">\n    Table. 1 ResNet architectures for ImageNet.\n\n</div>\n\n## 4 Experiments\n\n### 4.1 Performance on ImageNet\nHe et al. trained ResNet-18 and ResNet-34 on the ImageNet dataset, and also compared them to plain convolutional networks. In Fig. 5, the thin curves denote training error, and the bold ones denote validation error. The figure on the left shows the results of plain convolution networks(in which the 34-layered ones has higher error rates than the 18-layered one), and the figure on the right shows that residual networks perform better than plain ones, while deeper ones perform better than shallow ones.\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/training.png\" width=\"80%\" height=\"80%\" alt=\"Training ResNet on ImageNet\"/>\n    Fig. 5 Training ResNet on ImageNet\n\n</div>\n\n### 4.2 Effects of Different Shortcut Connections\n\nHe et al. also tried various types of shortcut connections to replace the identity mapping, and various positions of activation functions / batch normalization. Experiments show that the original identity mapping and full pre-activation yield the best results.\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/shortcut-connections.png\" width=\"50%\" height=\"50%\" alt=\"Different shortcuts of residual units\"/>\n    Fig. 6 Various shortcuts in residual units\n\n    <img src=\"/images/resnet/shortcut-connections-experiment.png\" width=\"70%\" height=\"70%\" alt=\"Classification errors with different shortcuts\"/>\n    Table. 2 Classification error on CIFAR-10 test set with various shortcut connections in residual units\n\n    <img src=\"/images/resnet/activations.png\" width=\"70%\" height=\"50%\" alt=\"Different usages of activation in residual units\"/>\n    Fig. 7 Various usages of activation in residual units\n\n    <img src=\"/images/resnet/activations-experiment.png\" width=\"50%\" height=\"50%\" alt=\"Classification errors with different activations\"/>\n    Table. 3 Classification error on CIFAR-10 test set with various usages of activation in residual units\n\n</div>\n\n\n## 5 Conclusion\nResidual learning can be crowned as \"ONE OF THE GREATEST HITS IN DEEP LEARNING FIELDS\". With a simple identity mapping, it solved the degradation problem of deep neural networks. Now that you have learned about the concept of ResNet, why not give it a try and implement your first residual learning model today?\n\n<div align=\"center\" class=\"figure\">\n    <img src=\"/images/resnet/resnet-yooo.jpg\" width=\"40%\" height=\"40%\" alt=\"\">\n</div>\n\n## References\n\n[1] [He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.](http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n\n[2] [He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks[C]//European Conference on Computer Vision. Springer, Cham, 2016: 630-645.](https://arxiv.org/pdf/1603.05027.pdf)\n\n[3] [Balduzzi D, Frean M, Leary L, et al. The Shattered Gradients Problem: If resnets are the answer, then what is the question?[J]. arXiv preprint arXiv:1702.08591, 2017.](https://arxiv.org/pdf/1702.08591.pdf)\n\n[4] [Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.](https://arxiv.org/pdf/1502.03167.pdf)","slug":"resnet","published":1,"updated":"2018-11-18T05:44:45.857Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomm0o16000lwgwn00b10ta4","content":"<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n<h2 id=\"0-Introduction\"><a href=\"#0-Introduction\" class=\"headerlink\" title=\"0 Introduction\"></a>0 Introduction</h2><p>Deep learning researchers have been constructing skyscrapers in recent years. Especially, VGG nets and GoogLeNet have pushed the depths of convolutional networks to the extreme. But questions remain: if time and money aren&#39;t problems, are deeper networks always performing better? Not exactly.</p>\n<p>When residual networks were proposed, researchers around the world was stunned by its depth. &quot;Jesus Christ! Is this a neural network or the Dubai Tower?&quot; But <strong>don&#39;t be afraid!</strong> These networks are deep but the structures are simple. Interestingly, these networks not only defeated all opponents in the classification, detection, localization challenges in ImageNet 2015, but were also the main innovation in the best paper of CVPR2016.</p>\n<div align=\"center\"><br>    <img src=\"/images/resnet/network_growth.jpg\" width=\"40%\" height=\"40%\" alt=\"Network Growth\"><br></div>\n\n<a id=\"more\"></a>\n<h2 id=\"1-The-Crisis-Degradation-of-Deep-Networks\"><a href=\"#1-The-Crisis-Degradation-of-Deep-Networks\" class=\"headerlink\" title=\"1 The Crisis: Degradation of Deep Networks\"></a>1 The Crisis: Degradation of Deep Networks</h2><p>VGG nets proved the beneficial of representation depth of convolutional neural networks, at least within a certain range, to be exact. However, when Kaiming He et al. tried to deepen some plain networks, the training error and test error stopped decreasing after the network reached a certain depth(which is not surprising) and soon degraded. This is not an overfitting problem, because training errors also increased; nor is it a gradient vanishing problem, because there are some techniques(e.g. batch normalization[4]) that ease the pain.</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/resnet/downgrade.png\" width=\"60%\" height=\"60%\" alt=\"The Downgrade Problem\"><br>    Fig.1 The downgrade problem<br><br></div>\n\n<p>What seems to be the cause of this degradation? Obviously, deeper neural networks are more difficult to train, but that doesn&#39;t mean deeper neural networks would yield worse results. To explain this problem, Balduzzi et al.[3] identified shattered gradient problem - as depth increases, gradients in standard feedforward networks increasingly resemble white noise. I will write about that later.</p>\n<h2 id=\"2-A-Closer-Look-at-ResNet-The-Residual-Blocks\"><a href=\"#2-A-Closer-Look-at-ResNet-The-Residual-Blocks\" class=\"headerlink\" title=\"2 A Closer Look at ResNet: The Residual Blocks\"></a>2 A Closer Look at ResNet: The Residual Blocks</h2><p>As the old saying goes, &quot;&quot;. Although ResNets are as deep as a thousand layers, they are built with these basic residual blocks(the right part of the figure). </p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/resnet/residual_blocks.png\" width=\"50%\" height=\"50%\" alt=\"Comparison between normal weight layers and residual blocks\"><br>    Fig.2 Parts of plain networks and a residual block(or residual unit)<br><br></div>\n\n<h3 id=\"2-1-Skip-Connections\"><a href=\"#2-1-Skip-Connections\" class=\"headerlink\" title=\"2.1 Skip Connections\"></a>2.1 Skip Connections</h3><p>In comparison, basic units of plain network models would look like the one on the left: one ReLU function after a weight layer(usually also with biases), repeated several times. Let&#39;s denote the desired underlying mapping(the ideal mapping) of the two layers as \\(\\mathcal{H}(x)\\), and the real mapping as \\(\\mathcal{F}(x)\\). Clearly, the closer \\(\\mathcal{F}(x)\\) is to \\(\\mathcal{H}(x)\\), the better it fits.</p>\n<p>However, He et al. explicitly let these layers fit a residual mapping instead of the desired underlying mapping. This is implemented with &quot;shortcut connections&quot;, which skip one or more layers, simply performing identity mappings and getting added to the outputs of the stacked weight layers. This way, \\(\\mathcal{F}(x)\\) would not try to fit \\(\\mathcal{H}(x)\\), but \\(\\mathcal{H}(x)-x\\). The whole structure(from the identity mapping branch, to merging the branches by the addition operation) are named &quot;residual blocks&quot;(or &quot;residual units&quot;).</p>\n<p>What&#39;s the point in this? Let&#39;s do a simple analysis. The computation done by the original residual block is: $$y_l=h(x_l)+\\mathcal{F}(x_l,\\mathcal{W}<em>l),$$ $$x</em>{l+1}=f(y_l).$$</p>\n<p>Here are the definitions of symbols:<br>\\(x<em>l\\): input features to the \\(l\\)-th residual block;<br>\\(\\mathcal{W}</em>{l}={W<em>{l,k}|</em>{1\\leq k\\leq K}}\\): a set of weights(and biases) associated with the \\(l\\)-th residual unit. \\(K\\) is the number of layers in this block;<br>\\(\\mathcal{F}(x,\\mathcal{W})\\): the residual function, which we talked about earlier. It&#39;s a stack of 2 conv. layers here;<br>\\(f(x)\\): the activation function. We are using ReLU here;<br>\\(h(x)\\): identity mapping.</p>\n<p>If \\(f(x)\\) is also an identity mapping(as if we&#39;re not using any activation function), the first equation would become:<br>$$x_{l+1}=x_l+\\mathcal{F}(x_l,\\mathcal{W}_l)$$</p>\n<p>Therefore, we can define \\(x_L\\) recursively of any layer:<br>$$x_L=x<em>l+\\sum</em>{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)$$</p>\n<p>That&#39;s not the end yet! When it comes to the gradients, according to the chain rules of backpropagation, we have a beautiful definition:<br>$$\\begin{split}<br>\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} &amp; = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\<br>&amp; = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x<em>l}}\\sum</em>{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)<br>\\end{split}$$</p>\n<p>What does it mean? It means that the information is directly backpropagated to ANY shallower block. This way, the gradients of a layer never vanish or explode even if the weights are too small or too big.</p>\n<h3 id=\"2-2-Identity-Mappings\"><a href=\"#2-2-Identity-Mappings\" class=\"headerlink\" title=\"2.2 Identity Mappings\"></a>2.2 Identity Mappings</h3><p>It&#39;s important that we use identity mapping here! Just consider doing a simple modification here, for example, \\(h(x)=\\lambda_lx_l\\)(\\(\\lambda_l\\) is a modulating scalar). The definition of \\(x_L\\) and \\(\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}}\\) would become:<br>$$x<em>L=(\\prod</em>{i=l}^{L-1}\\lambda_i)x<em>l+\\sum</em>{i=l}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i,\\mathcal{W}_i)$$<br>$$\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}}=\\frac{\\partial{\\mathcal{E}}}{\\partial{x<em>L}}\\Big((\\prod</em>{i=l}^{L-1}\\lambda_i)+\\frac{\\partial{}}{\\partial{x<em>l}}\\sum</em>{i=l}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)$$</p>\n<p>For extremely deep neural networks where \\(L\\) is too large, \\(\\prod_{i=l}^{L-1}\\lambda_i\\) could be either too small or too large, causing gradient vanishing or gradient explosion. For \\(h(x)\\) with complex definitions, the gradient could be extremely complicated, thus losing the advantage of the skip connection. Skip connection works best under the condition where the grey channel in Fig. 3 cover no operations (except the addition) and is clean.</p>\n<p>Interestingly, this comfirmed the philosophy of &quot;&quot; once again.</p>\n<h3 id=\"2-3-Post-activation-or-Pre-activation\"><a href=\"#2-3-Post-activation-or-Pre-activation\" class=\"headerlink\" title=\"2.3 Post-activation or Pre-activation?\"></a>2.3 Post-activation or Pre-activation?</h3><p>Wait a second... &quot;\\(f(x)\\) is also an identity mapping&quot; is just our assumption. The activation function is still there!</p>\n<p>Right. There IS an activation function, but it&#39;s moved to somewhere else.  In fact, the original residual block is still a little bit problematic - the output of one residual block is not always the input of the next, since there is a ReLU activation function after the addition(It did NOT REALLY keep the identity mapping to the next block!). Therefore, in[2], He et al. fixed the residual blocks by changing the order of operations.</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/resnet/identity_mapping.png\" width=\"30%\" height=\"30%\" alt=\"New identity mapping\"><br>    Fig.3 New identity mapping proposed by He et al.<br><br></div>\n\n<p>Besides using a simple identity mapping, He et al. also discussed about the position of the activation function and the batch normalization operation. Assuming that we got a special(asymmetric) activation function \\(\\hat f(x)\\), which only affects the path to the next residual unit. Now our definition of \\(x<em>{x+1}\\) would become:<br>$$x</em>{l+1}=x_l+\\mathcal{F}(\\hat f(x_l),\\mathcal{W}_l)$$</p>\n<p>With \\(x_l\\) still multiplied by 1, information is still fully backpropagated to shallower residual blocks. And the good thing is that using this asymmetric activation function after the addition(partial post-activation) is equivalent to using it beforehand(pre-activation)! This is why He et al. chose to use pre-activation - otherwise it would be necessary to implement that magical activation function \\(\\hat f(x)\\).</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/resnet/pre-activation.png\" width=\"80%\" height=\"80%\" alt=\"Asymmetric after-addition activation\"><br>    Fig.4 Using asymmetric after-addition activation is equivalent to constructing a pre-activation residual unit<br><br></div>\n\n<h2 id=\"3-ResNet-Architectures\"><a href=\"#3-ResNet-Architectures\" class=\"headerlink\" title=\"3 ResNet Architectures\"></a>3 ResNet Architectures</h2><p>Here are the ResNet architectures for ImageNet. Building blocks are shown in brackets, with the numbers of blocks stacked. With the first block of every stack(starting from conv3_x), a downsampling is performed. Each column represents one of the residual networks, and the deepest one has 152 weight layers! Since ResNets were proposed, VGG nets - which were officially called &quot;Very Deep Convolutional Networks&quot; - are not relatively deep anymore. Maybe call them &quot;A Little Bit Deep Convolutional Networks&quot;.</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/resnet/architectures.png\" width=\"80%\" height=\"80%\" alt=\"ResNet architectures for ImageNet\"><br>    Table. 1 ResNet architectures for ImageNet.<br><br></div>\n\n<h2 id=\"4-Experiments\"><a href=\"#4-Experiments\" class=\"headerlink\" title=\"4 Experiments\"></a>4 Experiments</h2><h3 id=\"4-1-Performance-on-ImageNet\"><a href=\"#4-1-Performance-on-ImageNet\" class=\"headerlink\" title=\"4.1 Performance on ImageNet\"></a>4.1 Performance on ImageNet</h3><p>He et al. trained ResNet-18 and ResNet-34 on the ImageNet dataset, and also compared them to plain convolutional networks. In Fig. 5, the thin curves denote training error, and the bold ones denote validation error. The figure on the left shows the results of plain convolution networks(in which the 34-layered ones has higher error rates than the 18-layered one), and the figure on the right shows that residual networks perform better than plain ones, while deeper ones perform better than shallow ones.</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/resnet/training.png\" width=\"80%\" height=\"80%\" alt=\"Training ResNet on ImageNet\"><br>    Fig. 5 Training ResNet on ImageNet<br><br></div>\n\n<h3 id=\"4-2-Effects-of-Different-Shortcut-Connections\"><a href=\"#4-2-Effects-of-Different-Shortcut-Connections\" class=\"headerlink\" title=\"4.2 Effects of Different Shortcut Connections\"></a>4.2 Effects of Different Shortcut Connections</h3><p>He et al. also tried various types of shortcut connections to replace the identity mapping, and various positions of activation functions / batch normalization. Experiments show that the original identity mapping and full pre-activation yield the best results.</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/resnet/shortcut-connections.png\" width=\"50%\" height=\"50%\" alt=\"Different shortcuts of residual units\"><br>    Fig. 6 Various shortcuts in residual units<br><br>    <img src=\"/images/resnet/shortcut-connections-experiment.png\" width=\"70%\" height=\"70%\" alt=\"Classification errors with different shortcuts\"><br>    Table. 2 Classification error on CIFAR-10 test set with various shortcut connections in residual units<br><br>    <img src=\"/images/resnet/activations.png\" width=\"70%\" height=\"50%\" alt=\"Different usages of activation in residual units\"><br>    Fig. 7 Various usages of activation in residual units<br><br>    <img src=\"/images/resnet/activations-experiment.png\" width=\"50%\" height=\"50%\" alt=\"Classification errors with different activations\"><br>    Table. 3 Classification error on CIFAR-10 test set with various usages of activation in residual units<br><br></div>\n\n\n<h2 id=\"5-Conclusion\"><a href=\"#5-Conclusion\" class=\"headerlink\" title=\"5 Conclusion\"></a>5 Conclusion</h2><p>Residual learning can be crowned as &quot;ONE OF THE GREATEST HITS IN DEEP LEARNING FIELDS&quot;. With a simple identity mapping, it solved the degradation problem of deep neural networks. Now that you have learned about the concept of ResNet, why not give it a try and implement your first residual learning model today?</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/resnet/resnet-yooo.jpg\" width=\"40%\" height=\"40%\" alt=\"\"><br></div>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><p>[1] <a href=\"http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</a></p>\n<p>[2] <a href=\"https://arxiv.org/pdf/1603.05027.pdf\" target=\"_blank\" rel=\"noopener\">He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks[C]//European Conference on Computer Vision. Springer, Cham, 2016: 630-645.</a></p>\n<p>[3] <a href=\"https://arxiv.org/pdf/1702.08591.pdf\" target=\"_blank\" rel=\"noopener\">Balduzzi D, Frean M, Leary L, et al. The Shattered Gradients Problem: If resnets are the answer, then what is the question?[J]. arXiv preprint arXiv:1702.08591, 2017.</a></p>\n<p>[4] <a href=\"https://arxiv.org/pdf/1502.03167.pdf\" target=\"_blank\" rel=\"noopener\">Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.</a></p>\n","site":{"data":{}},"excerpt":"<script type=\"text/javascript\" src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n<h2 id=\"0-Introduction\"><a href=\"#0-Introduction\" class=\"headerlink\" title=\"0 Introduction\"></a>0 Introduction</h2><p>Deep learning researchers have been constructing skyscrapers in recent years. Especially, VGG nets and GoogLeNet have pushed the depths of convolutional networks to the extreme. But questions remain: if time and money aren&#39;t problems, are deeper networks always performing better? Not exactly.</p>\n<p>When residual networks were proposed, researchers around the world was stunned by its depth. &quot;Jesus Christ! Is this a neural network or the Dubai Tower?&quot; But <strong>don&#39;t be afraid!</strong> These networks are deep but the structures are simple. Interestingly, these networks not only defeated all opponents in the classification, detection, localization challenges in ImageNet 2015, but were also the main innovation in the best paper of CVPR2016.</p>\n<div align=\"center\"><br>    <img src=\"/images/resnet/network_growth.jpg\" width=\"40%\" height=\"40%\" alt=\"Network Growth\"><br></div>","more":"<h2 id=\"1-The-Crisis-Degradation-of-Deep-Networks\"><a href=\"#1-The-Crisis-Degradation-of-Deep-Networks\" class=\"headerlink\" title=\"1 The Crisis: Degradation of Deep Networks\"></a>1 The Crisis: Degradation of Deep Networks</h2><p>VGG nets proved the beneficial of representation depth of convolutional neural networks, at least within a certain range, to be exact. However, when Kaiming He et al. tried to deepen some plain networks, the training error and test error stopped decreasing after the network reached a certain depth(which is not surprising) and soon degraded. This is not an overfitting problem, because training errors also increased; nor is it a gradient vanishing problem, because there are some techniques(e.g. batch normalization[4]) that ease the pain.</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/resnet/downgrade.png\" width=\"60%\" height=\"60%\" alt=\"The Downgrade Problem\"><br>    Fig.1 The downgrade problem<br><br></div>\n\n<p>What seems to be the cause of this degradation? Obviously, deeper neural networks are more difficult to train, but that doesn&#39;t mean deeper neural networks would yield worse results. To explain this problem, Balduzzi et al.[3] identified shattered gradient problem - as depth increases, gradients in standard feedforward networks increasingly resemble white noise. I will write about that later.</p>\n<h2 id=\"2-A-Closer-Look-at-ResNet-The-Residual-Blocks\"><a href=\"#2-A-Closer-Look-at-ResNet-The-Residual-Blocks\" class=\"headerlink\" title=\"2 A Closer Look at ResNet: The Residual Blocks\"></a>2 A Closer Look at ResNet: The Residual Blocks</h2><p>As the old saying goes, &quot;&quot;. Although ResNets are as deep as a thousand layers, they are built with these basic residual blocks(the right part of the figure). </p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/resnet/residual_blocks.png\" width=\"50%\" height=\"50%\" alt=\"Comparison between normal weight layers and residual blocks\"><br>    Fig.2 Parts of plain networks and a residual block(or residual unit)<br><br></div>\n\n<h3 id=\"2-1-Skip-Connections\"><a href=\"#2-1-Skip-Connections\" class=\"headerlink\" title=\"2.1 Skip Connections\"></a>2.1 Skip Connections</h3><p>In comparison, basic units of plain network models would look like the one on the left: one ReLU function after a weight layer(usually also with biases), repeated several times. Let&#39;s denote the desired underlying mapping(the ideal mapping) of the two layers as \\(\\mathcal{H}(x)\\), and the real mapping as \\(\\mathcal{F}(x)\\). Clearly, the closer \\(\\mathcal{F}(x)\\) is to \\(\\mathcal{H}(x)\\), the better it fits.</p>\n<p>However, He et al. explicitly let these layers fit a residual mapping instead of the desired underlying mapping. This is implemented with &quot;shortcut connections&quot;, which skip one or more layers, simply performing identity mappings and getting added to the outputs of the stacked weight layers. This way, \\(\\mathcal{F}(x)\\) would not try to fit \\(\\mathcal{H}(x)\\), but \\(\\mathcal{H}(x)-x\\). The whole structure(from the identity mapping branch, to merging the branches by the addition operation) are named &quot;residual blocks&quot;(or &quot;residual units&quot;).</p>\n<p>What&#39;s the point in this? Let&#39;s do a simple analysis. The computation done by the original residual block is: $$y_l=h(x_l)+\\mathcal{F}(x_l,\\mathcal{W}<em>l),$$ $$x</em>{l+1}=f(y_l).$$</p>\n<p>Here are the definitions of symbols:<br>\\(x<em>l\\): input features to the \\(l\\)-th residual block;<br>\\(\\mathcal{W}</em>{l}={W<em>{l,k}|</em>{1\\leq k\\leq K}}\\): a set of weights(and biases) associated with the \\(l\\)-th residual unit. \\(K\\) is the number of layers in this block;<br>\\(\\mathcal{F}(x,\\mathcal{W})\\): the residual function, which we talked about earlier. It&#39;s a stack of 2 conv. layers here;<br>\\(f(x)\\): the activation function. We are using ReLU here;<br>\\(h(x)\\): identity mapping.</p>\n<p>If \\(f(x)\\) is also an identity mapping(as if we&#39;re not using any activation function), the first equation would become:<br>$$x_{l+1}=x_l+\\mathcal{F}(x_l,\\mathcal{W}_l)$$</p>\n<p>Therefore, we can define \\(x_L\\) recursively of any layer:<br>$$x_L=x<em>l+\\sum</em>{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)$$</p>\n<p>That&#39;s not the end yet! When it comes to the gradients, according to the chain rules of backpropagation, we have a beautiful definition:<br>$$\\begin{split}<br>\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}} &amp; = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\frac{\\partial{x_L}}{\\partial{x_l}}\\\\<br>&amp; = \\frac{\\partial{\\mathcal{E}}}{\\partial{x_L}}\\Big(1+\\frac{\\partial{}}{\\partial{x<em>l}}\\sum</em>{i=l}^{L-1}\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)<br>\\end{split}$$</p>\n<p>What does it mean? It means that the information is directly backpropagated to ANY shallower block. This way, the gradients of a layer never vanish or explode even if the weights are too small or too big.</p>\n<h3 id=\"2-2-Identity-Mappings\"><a href=\"#2-2-Identity-Mappings\" class=\"headerlink\" title=\"2.2 Identity Mappings\"></a>2.2 Identity Mappings</h3><p>It&#39;s important that we use identity mapping here! Just consider doing a simple modification here, for example, \\(h(x)=\\lambda_lx_l\\)(\\(\\lambda_l\\) is a modulating scalar). The definition of \\(x_L\\) and \\(\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}}\\) would become:<br>$$x<em>L=(\\prod</em>{i=l}^{L-1}\\lambda_i)x<em>l+\\sum</em>{i=l}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i,\\mathcal{W}_i)$$<br>$$\\frac{\\partial{\\mathcal{E}}}{\\partial{x_l}}=\\frac{\\partial{\\mathcal{E}}}{\\partial{x<em>L}}\\Big((\\prod</em>{i=l}^{L-1}\\lambda_i)+\\frac{\\partial{}}{\\partial{x<em>l}}\\sum</em>{i=l}^{L-1}(\\prod_{j=i+1}^{L-1}\\lambda_j)\\mathcal{F}(x_i,\\mathcal{W}_i)\\Big)$$</p>\n<p>For extremely deep neural networks where \\(L\\) is too large, \\(\\prod_{i=l}^{L-1}\\lambda_i\\) could be either too small or too large, causing gradient vanishing or gradient explosion. For \\(h(x)\\) with complex definitions, the gradient could be extremely complicated, thus losing the advantage of the skip connection. Skip connection works best under the condition where the grey channel in Fig. 3 cover no operations (except the addition) and is clean.</p>\n<p>Interestingly, this comfirmed the philosophy of &quot;&quot; once again.</p>\n<h3 id=\"2-3-Post-activation-or-Pre-activation\"><a href=\"#2-3-Post-activation-or-Pre-activation\" class=\"headerlink\" title=\"2.3 Post-activation or Pre-activation?\"></a>2.3 Post-activation or Pre-activation?</h3><p>Wait a second... &quot;\\(f(x)\\) is also an identity mapping&quot; is just our assumption. The activation function is still there!</p>\n<p>Right. There IS an activation function, but it&#39;s moved to somewhere else.  In fact, the original residual block is still a little bit problematic - the output of one residual block is not always the input of the next, since there is a ReLU activation function after the addition(It did NOT REALLY keep the identity mapping to the next block!). Therefore, in[2], He et al. fixed the residual blocks by changing the order of operations.</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/resnet/identity_mapping.png\" width=\"30%\" height=\"30%\" alt=\"New identity mapping\"><br>    Fig.3 New identity mapping proposed by He et al.<br><br></div>\n\n<p>Besides using a simple identity mapping, He et al. also discussed about the position of the activation function and the batch normalization operation. Assuming that we got a special(asymmetric) activation function \\(\\hat f(x)\\), which only affects the path to the next residual unit. Now our definition of \\(x<em>{x+1}\\) would become:<br>$$x</em>{l+1}=x_l+\\mathcal{F}(\\hat f(x_l),\\mathcal{W}_l)$$</p>\n<p>With \\(x_l\\) still multiplied by 1, information is still fully backpropagated to shallower residual blocks. And the good thing is that using this asymmetric activation function after the addition(partial post-activation) is equivalent to using it beforehand(pre-activation)! This is why He et al. chose to use pre-activation - otherwise it would be necessary to implement that magical activation function \\(\\hat f(x)\\).</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/resnet/pre-activation.png\" width=\"80%\" height=\"80%\" alt=\"Asymmetric after-addition activation\"><br>    Fig.4 Using asymmetric after-addition activation is equivalent to constructing a pre-activation residual unit<br><br></div>\n\n<h2 id=\"3-ResNet-Architectures\"><a href=\"#3-ResNet-Architectures\" class=\"headerlink\" title=\"3 ResNet Architectures\"></a>3 ResNet Architectures</h2><p>Here are the ResNet architectures for ImageNet. Building blocks are shown in brackets, with the numbers of blocks stacked. With the first block of every stack(starting from conv3_x), a downsampling is performed. Each column represents one of the residual networks, and the deepest one has 152 weight layers! Since ResNets were proposed, VGG nets - which were officially called &quot;Very Deep Convolutional Networks&quot; - are not relatively deep anymore. Maybe call them &quot;A Little Bit Deep Convolutional Networks&quot;.</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/resnet/architectures.png\" width=\"80%\" height=\"80%\" alt=\"ResNet architectures for ImageNet\"><br>    Table. 1 ResNet architectures for ImageNet.<br><br></div>\n\n<h2 id=\"4-Experiments\"><a href=\"#4-Experiments\" class=\"headerlink\" title=\"4 Experiments\"></a>4 Experiments</h2><h3 id=\"4-1-Performance-on-ImageNet\"><a href=\"#4-1-Performance-on-ImageNet\" class=\"headerlink\" title=\"4.1 Performance on ImageNet\"></a>4.1 Performance on ImageNet</h3><p>He et al. trained ResNet-18 and ResNet-34 on the ImageNet dataset, and also compared them to plain convolutional networks. In Fig. 5, the thin curves denote training error, and the bold ones denote validation error. The figure on the left shows the results of plain convolution networks(in which the 34-layered ones has higher error rates than the 18-layered one), and the figure on the right shows that residual networks perform better than plain ones, while deeper ones perform better than shallow ones.</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/resnet/training.png\" width=\"80%\" height=\"80%\" alt=\"Training ResNet on ImageNet\"><br>    Fig. 5 Training ResNet on ImageNet<br><br></div>\n\n<h3 id=\"4-2-Effects-of-Different-Shortcut-Connections\"><a href=\"#4-2-Effects-of-Different-Shortcut-Connections\" class=\"headerlink\" title=\"4.2 Effects of Different Shortcut Connections\"></a>4.2 Effects of Different Shortcut Connections</h3><p>He et al. also tried various types of shortcut connections to replace the identity mapping, and various positions of activation functions / batch normalization. Experiments show that the original identity mapping and full pre-activation yield the best results.</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/resnet/shortcut-connections.png\" width=\"50%\" height=\"50%\" alt=\"Different shortcuts of residual units\"><br>    Fig. 6 Various shortcuts in residual units<br><br>    <img src=\"/images/resnet/shortcut-connections-experiment.png\" width=\"70%\" height=\"70%\" alt=\"Classification errors with different shortcuts\"><br>    Table. 2 Classification error on CIFAR-10 test set with various shortcut connections in residual units<br><br>    <img src=\"/images/resnet/activations.png\" width=\"70%\" height=\"50%\" alt=\"Different usages of activation in residual units\"><br>    Fig. 7 Various usages of activation in residual units<br><br>    <img src=\"/images/resnet/activations-experiment.png\" width=\"50%\" height=\"50%\" alt=\"Classification errors with different activations\"><br>    Table. 3 Classification error on CIFAR-10 test set with various usages of activation in residual units<br><br></div>\n\n\n<h2 id=\"5-Conclusion\"><a href=\"#5-Conclusion\" class=\"headerlink\" title=\"5 Conclusion\"></a>5 Conclusion</h2><p>Residual learning can be crowned as &quot;ONE OF THE GREATEST HITS IN DEEP LEARNING FIELDS&quot;. With a simple identity mapping, it solved the degradation problem of deep neural networks. Now that you have learned about the concept of ResNet, why not give it a try and implement your first residual learning model today?</p>\n<div align=\"center\" class=\"figure\"><br>    <img src=\"/images/resnet/resnet-yooo.jpg\" width=\"40%\" height=\"40%\" alt=\"\"><br></div>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><p>[1] <a href=\"http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf\" target=\"_blank\" rel=\"noopener\">He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</a></p>\n<p>[2] <a href=\"https://arxiv.org/pdf/1603.05027.pdf\" target=\"_blank\" rel=\"noopener\">He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks[C]//European Conference on Computer Vision. Springer, Cham, 2016: 630-645.</a></p>\n<p>[3] <a href=\"https://arxiv.org/pdf/1702.08591.pdf\" target=\"_blank\" rel=\"noopener\">Balduzzi D, Frean M, Leary L, et al. The Shattered Gradients Problem: If resnets are the answer, then what is the question?[J]. arXiv preprint arXiv:1702.08591, 2017.</a></p>\n<p>[4] <a href=\"https://arxiv.org/pdf/1502.03167.pdf\" target=\"_blank\" rel=\"noopener\">Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.</a></p>"},{"title":"A Painless Tensorflow Basic Tutorial - Take ResNet-56 as an Example","langs":["zh-cn","en-us"],"date":"2018-04-29T13:05:43.000Z","_content":"\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n<br>\n<p lang=\"en-us\">\nTensorFlow is a powerful open-source deep learning framework, supporting various languages including Python. However, its APIs are far too complicated for a beginner in deep learning(especially those who are new to Python). In order to ease the pain of having to understand the mess of various elements in TensorFlow computation graphs, I made this tutorial to help beginners take the first bite of the cake.\n\nResNets are one of the greatest works in the deep learning field. Although they look scary with extreme depths, it's not a hard job to implement one. Now let's build one of the simplest ResNets - ResNet-56, and train it on the CIFAR-10 dataset.\n\n</p>\n<!-- more -->\n<p lang=\"zh-cn\">\nTensorFlowPythonAPIPythonTensorFlow213\nResNetResNetResNetResNetResNet-56CIFAR-10\n</p>\n<div align=\"center\" class=\"figure\">\n![Let's Rock!](/images/tftutorial/oyo.gif)\n</div>\n<p lang=\"en-us\">\nFirst let's take a look at ResNet-56. It's proposed by Kaiming He et al., and is designed to confirm the effect of residual networks. It has 56 weighted layers, deep but simple. The structure is shown in the figure below:\n</p>\n<p lang=\"zh-cn\">\nResNet-56ResNet56\n</p>\n<div align=\"center\" class=\"figure\">\n<img src=\"/images/tftutorial/resnet56.png\" alt=\"ResNet-56\" width=\"80%\"/>\n\nFig. 1 The structure of ResNet-56\n</div>\n<br>\n<p lang=\"en-us\">\nSeems a little bit long? Don't worry, let's do this step by step.\n</p>\n<p lang=\"zh-cn\">\n\n</p>\n\n## 1 Ingredients\nPython 3.6\n\nTensorFlow 1.4.0\n\nNumpy 1.13.3\n\nOpenCV 3.2.0\n\n[CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)\n<p lang=\"en-us\">\nAlso prepare some basic knowledge on Python programming, digital image processing and convolutional neural networks. If you are already capable of building, training and validating your own neural networks with TensorFlow, you don't have to read this post.\n</p>\n<p lang=\"zh-cn\">\nPythonTensorFlow\n</p>\n\n## 2 Recipe\n### 2.0 Prepare the tools\n<p lang=\"en-us\">\nPrepare(import) the tools for our project, including all that I mentioned above. Like this :P\n</p>\n<p lang=\"zh-cn\">\n(i)(m)(p)(o)(r)(t)\n</p>\n```python\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nimport pickle\nfrom tensor_chain import TensorChain\n```\n<p lang=\"en-us\">\nWait... What's this? TensorChain? Another deep learning framework like TensorFlow?\n\nUh, nope. This is my own encapsulation of some TensorFlow APIs, for the sake of easing your pain. You'll only have to focus on \"what's what\" in the beginning. We'll look into my implementation of this encapsulation later, when you are clear how everything goes. Please [download this file](/files/tensor_chain.py) and put it where your code file is, and import it.\n</p>\n<p lang=\"zh-cn\">\n... TensorChain\n\n...TensorFlow APITensorFlow[](/files/tensor_chain.py)import\n</p>\n\n### 2.1 Decide the input\n<p lang=\"en-us\">\nEvery neural network requires an input - you always have to identify the details of a question, before asking the computer to solve it. All of the variable, constant in TensorFlow are objects of type <em>tf.Tensors</em>. And the <em>tf.placeholder</em> of our input(s) is a special one. Images in CIFAR-10 dataset are RGB images(3 channels) of 32x32(really small), so our input should shaped like [32, 32, 3]. Also, we want to input a little <em>batch</em> of multiple images. Therefore, our input data should be an array of shape <em>[?, 32, 32, 3]</em>. Unknown dimension size can be marked as None, and it will be clear when we feed the model with the actual images. It's coded like this:\n</p>\n<p lang=\"zh-cn\">\nTensorFlow<em>tf.Tensor</em><em>tf.placeholder</em>CIFAR-1032x32RGBRGB[32, 32, 3]<em>batch</em><em>[?, 32, 32, 3]</em>numpyNonebatch\n</p>\n```python\ninput_tensor = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n```\n<p lang=\"en-us\">\n*Ground truth* data also need to be known in supervised learning, so we also have to define a placeholder for the ground truth data: \n</p>\n<p lang=\"zh-cn\">\n*ground truth*ground truthplaceholder\n</p>\n```python\nground_truth = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n```\n<p lang=\"en-us\">\nWe want the label data to be in the one-hot encoding format, which means an array of length 10, denoting 10 classes. Only on one position is a '1', and on other positions are '0's.\n</p>\n<p lang=\"zh-cn\">\nOne-Hot101010\n</p>\n\n### 2.2 Do some operations\n<p lang=\"en-us\">\nFor now, let's use our TensorChain to build it fast. Under most circumstances that we may face, the computations are based on the input data or the result of the former computation, so our network(or say, the most of it) look more like a chain than a web. Every time we add some new operation(layer), we add it to our <em>TensorChain</em> object. Just remember to get the <em>output_tensor</em> of this object(denoting the output tensor of the last operation on the chain) when you need to ue native TensorFlow API.\nThe construction function of TensorChain class requires a Tensor object as the parameter, which is also the input tensor of this chain. As we mentioned earlier, all we have to do is add operations. See my ResNet-56 code:\n</p>\n<p lang=\"zh-cn\">\nTensorChainTensorChainTensorFlow API<em>output_tensor</em>Tensor\nTensorChainTensorResNet-56\n</p>\n```python\nchain = TensorChain(input_tensor) \\\n        .convolution_layer_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 32, stride=2) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 64, stride=2) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .flatten() \\\n        .fully_connected_layer(10)\n```\n<p lang=\"en-us\">\nThis is it? Right, this is it! Isn't it cool? Didn't seem that high, huh? That's because I encapsulated that huge mess of weights and biases, only leaving a few parameters that decide the structure of the network. Later in this pose we'll talk about the actual work that these functions do.\n</p>\n<p lang=\"zh-cn\">\n56\n</p>\n\n### 2.3 Define the loss\n<p lang=\"en-us\">\nIn supervised learning, you always have to tell the learning target to the model. To tell the model how to optimize, you have to let it know how, how much, on which direction should it change its parameters. This is done by using a loss function. Therefore, we need to define a loss function for our ResNet-56 model(which we designed for this classification problem) so that it will learn and optimize.\nA commonly used loss function in classification problems is cross entropy. It's defined below:\n</p>\n<p lang=\"zh-cn\">\n*loss*ResNet-56\n\n</p>\n$$C=-\\frac{1}{n}\\sum_x{y\\ln a+(1-y)\\ln(1-a)}$$\n<p lang=\"en-us\">\nin which \\\\(y\\\\) is the expected(or say correct) output and \\\\(a\\\\) is the actual output.\nThis seems a little bit complicated. But it's not a hard job to implement, since TensorFlow implemented it already! You can also try and implement it yourself within one line if you want. For now we use the pre-defined cross entropy loss function:\n</p>\n<p lang=\"zh-cn\">\n\\\\(y\\\\)\\\\(a\\\\)\n...TensorFlow\n</p>\n```python\nloss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))\n```\n<p lang=\"en-us\">\nand it returns a tf.Tensor that denotes an average of cross entropies(don't forget that this is a batch). As for the 'softmax' before the 'cross_entropy', it's a function that project the data in an array to range 0~1, which allows us to do a comparison between our prediction and the ground truth(in one-hot code). The definition is simple too:\\\n</p>\n<p lang=\"zh-cn\">\nbatchTensorcross_entropy*softmax*0~1exp()\\\n</p>\n$$S_i=\\frac{e^{V_i}}{\\sum_j{e^{V_j}}}$$\n<p></p>\n\n### 2.4 Define the train op\n<p lang=\"en-us\">\nNow we have the loss function. We'll have to tell its value to an *optimizer*, which make our model learn and optimize in order to minimize the loss value. Gradient Descent Optimizer, Adagrad Optimizer, Adam Optimizer and Momentum Optimizer are commonly used optimizers. Here we use an Adam Optimizer for instance. You're free to try any other one here. When\n</p>\n<p lang=\"zh-cn\">\n*optimizer*Gradient Descent OptimizerAdagrad OptimizerAdam OptimizerMomentum Optimizer\\\\(10^-3\\\\)\n</p>\n```python\noptimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n```\n<p lang=\"en-us\">\nAlso, tell the optimizer that what the loss tensor is. The returned object is a train operation.\n</p>\n<p lang=\"zh-cn\">\nTensor*train op*\n</p>\n```python\ntrain = optimizer.minimize(loss)\n```\n<p lang=\"en-us\">\nThe neural network is finished. It's time to grab some data and train it.\n</p>\n<p lang=\"zh-cn\">\n\n</p>\n\n### 2.5 Feed the model with data, and train it!\n<p lang=\"en-us\">\nRemember how we defined the placeholders? It's time to fetch some data that fits the placeholders and train it. See how CIFAR-10 dataset can be fetched on its [website](https://www.cs.toronto.edu/~kriz/cifar.html).\n</p>\n<p lang=\"zh-cn\">\nplaceholderCIFAR-10[](https://www.cs.toronto.edu/~kriz/cifar.html)\n</p>\n```python\ndef unpickle(file):\n    import pickle\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict\n```\n<p lang=\"en-us\">\nThe returned value *dict* is a Python dictionary. Every time we unpickle a file, a dictionary would be returned. Its 'data' key leads to 10000 RGB images of size 32x32, which is stored in a [10000, 3072] array(3072=32*32*3, I guess you know how it's stored now). The 'label' key leads to 10000 values in range 0~9. Obviously we have to reshape the data so as to fit it into the network model:\n</p>\n<p lang=\"zh-cn\">\n*dict*Pythondictbatchdata_batch_1data1000032x32RGB[10000, 3072]3072=10000x32x32x3label100000-9\n</p>\n```python\nbatch = unpickle(DATA_PATH + 'data_batch_{}'.format(i))  # 'i' is the loop variable\n\n# Read the image data\nimage_data = np.reshape(batch[b'data'], (10000, 32, 32, 3), 'F').astype(np.float32)   \nimage_data = image_data / 255                            # Cast range(0, 255) to range(0, 1)\nimage_data = np.transpose(image_data, (0, 2, 1, 3))      # Exchange row and column\n\n# Read the label data and convert into one-hot code\nlabel_data = batch[b'labels']\nnew_label_data = np.zeros((10000, 10))                   \nfor j in range(10000):\n    new_label_data[j][label_data[j]] = 1\n```\n<p lang=\"en-us\">\nThe details for data processing are not covered here. Try doing step-by-step to see the results.\nThe *image_data* and *new_label_data* are contain 10000 pieces of data each. Let's divide them into 100 small batches(100 elements each, including image and label) and feed it into the model. Do this on all the 5 batch files:\n</p>\n<p lang=\"zh-cn\">\n\n*image_data**new_label_data*10000batch100100+5batch\n</p>\n```python\nwith tf.Session() as session:\n    session.run(tf.global_variables_initializer())\n    for j in range(100): # 10000 / BATCH_SIZE\n        # Divide them and get one part\n        image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]\n        label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]\n        \n        # Feed the model\n        session.run(train, feed_dict={\n            input_tensor: image_batch,\n            ground_truth: label_batch\n        })\n```\n<p lang=\"en-us\">\nA *session* - created with *tf.Session()* - is required every time we run a TensorFlow model, no matter when we're training it or evaluating it. The first time you run a model, you'll need to run *session.run(tf.global_variables_initializer())* to initialize the values of the TensorFlow variables defined previously.\nWhen running *session.run()*, you must first decide a TensorFlow operation(or a list of operations) that you need. If its result is dependent on some actual data(which means that some data in one or more placeholders flow to this operation), it's also required that you feed it the actual data by adding a *feed_dict* parameter. For example, I'm training this ResNet-56 model, in which a loss will be calculated with my *ground_truth* and the prediction result that comes from the *input_tensor*. Therefore, I'll have to give a value for each placeholder given above(format: \"placeholder name: corresponding data\"), and fold them in one Python dictionary.\n</p>\n<p lang=\"zh-cn\">\nTensorFlowtf.Session()*session**session.run(tf.global_variables_initializer())*TensorFlow\n*session.run()*operationplaceholderoperation*feed_dict**ground_truth**input_tensor*Pythonfeed_dict\n</p>\n<p lang=\"en-us\">\nI'm also interested in the loss function value in each iteration(which means feeding a batch of data and executing one forward-propagation and one back-propagation) in the training process. Therefore, what I'll fill in the parameter is not just the train op, but also the loss tensor. And the session.run() above should be modified to:\n</p>\n<p lang=\"zh-cn\">\nbatchsession.run()trainlosssession.run()\n</p>\n```python\n        [train_, loss_value] = session.run([train, loss],\n            feed_dict={\n                input_tensor: image_batch,\n                ground_truth: label_batch\n            })\n        print(\"Loss = {}\".format(loss_value)\n```\n<p lang=\"en-us\">\nThis is when the return value of session.run() becomes useful. Its value(s) - corresponding to the first parameter of run() - is/are the actual value(s) of the tensor(s) in the first parameter. In our example, *loss_value* is the actual output of the loss tensor. As for train_, we don't care what it is. Just add it to match the dimensions.\n</p>\n<p lang=\"zh-cn\">\nsession.run()operation*loss_value*losstraintrain_None\n</p>\n<p lang=\"en-us\">\nActually, one epoch(train the model once with the whole dataset) is not enough for the model to fully optimize. I trained this model for 40 epochs and added some loop variables to display the result. You can see my code and my output below. It's highly recommended that you train this with a high-performance GPU, or it would be a century before you train your model to a satisfactory degree.\n</p>\n<p lang=\"zh-cn\">\nepoch40epochGPUGPU\n</p>\n```python\nimport tensorflow as tf\nimport numpy as np\nimport pickle\nfrom tensor_chain import TensorChain\n\ndef unpickle(file):\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict\n\nif __name__ == '__main__':\n    input_tensor = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n    ground_truth = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n\n    chain = TensorChain(input_tensor) \\\n            .convolution_layer_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 32, stride=2) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 64, stride=2) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .flatten() \\\n            .fully_connected_layer(10)\n\n    prediction = chain.output_tensor\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train = optimizer.minimize(loss)\n\n    with tf.Session() as session:\n        session.run(tf.global_variables_initializer())\n        iteration = 1\n        BATCH_SIZE = 100\n        DATA_PATH = '../data/cifar-10-batches-py/'\n        for epoch in range(1, 41):\n            for i in range(1, 6):\n                data = unpickle(DATA_PATH + 'data_batch_{}'.format(i))\n                image_data = np.reshape(data[b'data'], (10000, 32, 32, 3), 'F').astype(np.float32)\n                image_data = image_data / 255\n                image_data = np.transpose(image_data, (0, 2, 1, 3))\n                label_data = data[b'labels']\n                new_label_data = np.zeros((10000, 10))\n                for j in range(10000):\n                    new_label_data[j][label_data[j]] = 1\n                for j in range(int(10000 / BATCH_SIZE)):\n                    image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]\n                    label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]\n                    [train_, loss_] = session.run(\n                        [train, loss],\n                        feed_dict={\n                            input_tensor: image_batch,\n                            ground_truth: label_batch\n                        })\n                    iteration += 1\n                    print(\"Epoch {}, Iteration {}, Loss = {}\".format(epoch, iteration, loss_))\n```\n\n<div align=\"center\" class=\"figure\">\n<img src=\"/images/tftutorial/train.png\" alt=\"Training result\" width=\"40%\">\n\nFig. 2 Training result: cross entropy has dropped below 0.5\n</div>\n<p></p>\n\n### 2.6 Conclusion\n<p lang=\"en-us\">\nIn a word, building & training neural network models with TensorFlow involves the following steps:\n\n1. Decide the *input tensor*\n\n2. Add operations(*op*s) based on existing tensors\n\n3. Define the *loss* tensor, just like other tensors\n\n4. Select an *optimizer* and define the *train* op\n\n5. Process *data* and feed the model with them\n</p>\n<p lang=\"zh-cn\">\nTensorFlow\n\n1. **Tensor\n\n2. Tensor*op*\n\n3. **Tensor\n\n4. ****\n\n5. **shape\n</p>\n\n## 3 A Closer Look\n<p lang=\"en-us\">\nWait, it's too late to leave now!\nTensorChain saved you from having to deal with a mess of TensorFlow classes and functions. Now it's time that we take a closer look at how TensorChain is implemented, thus understanding the native TensorFlow APIs.\n</p>\n<p lang=\"zh-cn\">\n\nTensorChainTensorFlowTensorChainTensorFlowAPI\n</p>\n\n### 3.1 TensorFlow variables\n<p lang=\"en-us\">\nLet's begin with TensorFlow variables. Variables in TensorFlow are similar to variables in C, Java or any other strong typed programming languages - they have a type, though not necessarily explicitly decided upon definition. Usually them will change as the training process goes on, getting close to a best value.\nThe most commonly used variables in TensorFlow are weights and biases. I guess that you have seen formulae like:\n</p>\n<p lang=\"zh-cn\">\nTensorFlowTensorFlowCJava\nTensorFlowweightsbiases\n</p>\n$$y=Wx+b$$\n<p lang=\"en-us\">\nThe \\\\(W\\\\) here is the weight, and the \\\\(b\\\\) here is the bias. When implementing some common network layers, they two are always used as the parameters in the layers. For instance, at the very beginning of our ResNet-56, we had a 3x3 sized convolution layer with 16 channels. Its implementation in TensorChain is:\n</p>\n<p lang=\"zh-cn\">\n\\\\(W\\\\)\\\\(b\\\\)ResNet-563x316TensorChain\n</p>\n```python\n    def convolution_layer_2d(self, filter_size: int, num_channels: int, stride: int = 1, name: str = None,\n                             disable_log: bool = False):\n        \"\"\"\n        Add a 2D convolution layer\n        :param filter_size: Filter size(width and height) for this operation\n        :param num_channels: Channel number of this filter\n        :param stride: Stride for this convolution operation\n        :param name: The name of the tensor\n        :param disable_log: Set it True if you don't want this layer to be recorded\n        :return: This object itself\n        \"\"\"\n        filter = self._weights([filter_size, filter_size, self.num_channels, num_channels], layer_name=name,\n                               suffix='filter')\n        bias = self._bias([num_channels], layer_name=name)\n        self.num_channels = num_channels\n        self.output_tensor = tf.nn.conv2d(self.output_tensor, filter,\n                                          [1, stride, stride, 1], 'SAME', name=name)\n        self.output_tensor = tf.add(self.output_tensor, bias)\n        self._log_layer(\n            '2D Convolution layer, filter size = {}x{}, stride = {}, {} channels'.format(filter_size, filter_size,\n                                                                                         stride,\n                                                                                         num_channels),\n            disable=disable_log)\n        return self\n```\n<p lang=\"en-us\">\nSee? On line 16, we used a *tf.nn.conv2d()* function, the parameters of which are *input*, *filter*, *strides*, *padding*, etc. As can be guessed from the names, this function does a convolution operation with out input and the weights(the convolution *filter* here). A *bias* is added to the result as the final output. There are also many people who argue that the bias here is meaningless and should removed. One line of code is sufficient for defining a variable:\n</p>\n<p lang=\"zh-cn\">\n16*tf.nn.conv2d()**input**filter**strides**padding*Tensor*filter*inputTensor*bias*biasbias\n</p>\n```python\ntf.Variable(tf.truncated_normal(shape, stddev=sigma), dtype=tf.float32, name=suffix)\n```\n<p lang=\"en-us\">\nTo define weight or bias variables, create a *tf.Variable* object. Usually you'll need to give the *initial_value* which also decides the shape of this tensor. *tf.truncated_normal()* and *tf.constant()* are usually used as the initial values. Also, other APIs - function *tf.get_variable()* and package *tf.initializers* are frequently used when using some more methods for initialization. I strongly recommend that you try using these APIs yourself.\n</p>\n<p lang=\"zh-cn\">\n*tf.Variable**initial_value*TFshapeshapeAPI*tf.get_variable()**tf.initializers*API\n</p>\n\n### 3.2 Tensors and operations\n<p lang=\"en-us\">\nGoing on with the parameters of the *tf.nn.conv2d()* function. The required parameters also include *strides* and *padding*. You should have already learned about what strides mean in convolution, and I'll only talk about their formats. *strides* require a 1-D vector with a length of 4, like [1, 2, 2, 1]. The 1st and the 4th number is always 1(in order to match dimensions with the input), while the 2nd and the 3rd means the vertical stride and the horizonal stride. \nThe 4th parameter *padding* is a little bit different from its definition in convolution operation. It requires 'SAME' of 'VALID', denoting 'with' or 'without' zero paddings. When it's 'SAME', zero padding is introduced to make the shapes match as needed, equally on every side of the input map.\n</p>\n<p lang=\"zh-cn\">\n*tf.nn.conv2d()**strides**padding*stride*strides*141API\n*padding*padding'SAME''VALID''SAME'stride>1/stride\n</p>\n<p lang=\"en-us\">\ntf.nn.conv2d() is just an example of TensorFlow *operations*. Other functions like *tf.matmul()*, *tf.reduce_mean()*, *tf.global_variables_initializer()*, *tf.losses.softmax_cross_entropy()*, *tf.truncated_normal()* are all operations. Operation functions return tensors(*tf.truncated_normal* also return a tensor, a tensor with initializers).\n</p>\n<p lang=\"zh-cn\">\ntf.nn.conv2d()TensorFlow*operation**tf.matmul()**tf.reduce_mean()**tf.nn.relu()**tf.batch_normalization()**tf.global_variables_initializer()**tf.losses.softmax_cross_entropy()**tf.truncated_normal()*TensorFlowTensorFlowTensor*tf.truncated_normal()*Tensor\n</p>\n<p lang=\"en-us\">\nAll the functions in the TensorChain class are based on the most basic TensorFlow operations and variables. After learning about these basic TensorFlow concepts, actually you can already abandon TensorChain, go and try implementing your own neural networks yourself!\n</p>\n<p lang=\"zh-cn\">\nTensorChainTensorFlowTensorChain\n</p>\n\n## 4 Spices\n<p lang=\"en-us\">\nI'm not joking just now! But I know that there are a lot of things that you still don't understand about using TensorFlow - like \"how do I visualize my computation graph\", \"how do I save/load my model to/from files\", \"how do I record some tensors' values while training\" or \"how do I view the loss curves\" - after all TensorFlow APIs are far more complicated than just building those nets. Those are also important techniques in your research. If you'd rather ask me than spending some time experimenting, please go on with reading.\n</p>\n<p lang=\"zh-cn\">\nTensorFlow/TensorTensorFlowAPITensorFlow\n</p>\n\n### 4.1 Saving and loading your model\n<p lang=\"en-us\">\nThe very first thing that you may want to do - after training a network model with nice outcomes - would be saving it. Saving a model is fairly easy - just use a *tf.train.Saver* object. See my code below:\n</p>\n<p lang=\"zh-cn\">\n*tf.train.Saver*\n</p>\n```python\nwith tf.Session() as session:\n    # Train it for some iterations\n    # Train it for some iterations\n    # Train it for some iterations\n    saver = tf.train.Saver()\n    saver.save(session, 'models/model.ckpt')\n```\n<p lang=\"en-us\">\nI saved my model and variable values to 'models/model.ckpt'. But actually, you'll find 3 files in the 'models' directory - *model.ckpt.data-00000-of-00001*, *model.ckpt.meta* and *model.ckpt.index* - none of which is 'model.ckpt'! That's because TensorFlow stores the graph structure separately from variables values. The *.meta* file describes the saved graph structure; the *.index* file records the mappings between tensor names and tensor metadata; and the *.data-00000-of-00001* file - which is always the biggest one - saves all the variable values. If you need the graph data together with the variable values to be loaded, use a Saver to load after creating a session:\n</p>\n<p lang=\"zh-cn\">\n'models/model.ckpt'models*model.ckpt.data-00000-of-00001**model.ckpt.meta**model.ckpt.index*model.ckptTensorFlow*.meta**.index*TensornameTensor*.data-00000-of-00001*TensorFlowsessionSaver\n</p>\n```python\nwith tf.Session() as session:\n    saver = tf.train.Saver()\n    saver.restore(session, 'models/model.ckpt')\n    # Then continue doing everything just like the model is just trained\n```\n<p lang=\"en-us\">\nRemember that session.run(tf.global_variables_initializer()) shouldn't be executed, since variables are already initialized with your saved *.data-0000-of-00001* file.\nIf you only need the graph to be loaded, only use the *.meta* file:\n</p>\n<p lang=\"zh-cn\">\nsession.run(tf.global_variables_initializer())checkpoint\n*.meta*\n</p>\n```python\nwith tf.Session() as session:\n    tf.train.import_meta_graph('models/model.ckpt.meta')\n    # Then continue doing everything just like the model is just built\n```\n<p lang=\"en-us\">\nFunction *tf.train.import_meta_graph()* loads(appends) the graph to your current computation graph. The values of tensors are still uninitialized so you'll have to execute session.run(tf.global_variables_initializer()) again. The tensors that you defined in the model can be retrieved by their names(property of the Tensor objects, instead of Python variable names). For example:\n</p>\n<p lang=\"zh-cn\">\n*tf.train.import_meta_graph()*Tensorsession.run(tf.global_variables_initializer())\n</p>\n```python\nwith tf.Session() as session:\n    # Recover the model here\n\n    graph = tf.get_default_graph()\n    image_tensor = graph.get_tensor_by_name('input_image:0')\n    loss = graph.get_tensor_by_name('loss:0')\n    train = graph.get_operation_by_name('train)\n```\n<p lang=\"en-us\">\nTo retrieve normal tensors, you'll have to append a *':0'* to the name of the op. This means getting the associated tensor of the op. *train* is a little special - we only need the op, so the function is *get_operation_by_name()* so the ':0' is not necessary.\n</p>\n<p lang=\"zh-cn\">\nTensorTensorname*':0'*Tensor*train*op*get_operation_by_name()*Tensor':0'\n</p>\n\n<p lang=\"en-us\" align=\"center\">\n[THIS SECTION IS UNDER CONSTRUCTION]\n</p>\n<p lang=\"zh-cn\" align=\"center\">\n[]\n</p>","source":"_posts/diy-resnet.md","raw":"---\ntitle: A Painless Tensorflow Basic Tutorial - Take ResNet-56 as an Example\ntags:\n  - Deep Learning\n  - DIY\nlangs:\n  - zh-cn\n  - en-us\ndate: 2018-04-29 22:05:43\n---\n\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n<br>\n<p lang=\"en-us\">\nTensorFlow is a powerful open-source deep learning framework, supporting various languages including Python. However, its APIs are far too complicated for a beginner in deep learning(especially those who are new to Python). In order to ease the pain of having to understand the mess of various elements in TensorFlow computation graphs, I made this tutorial to help beginners take the first bite of the cake.\n\nResNets are one of the greatest works in the deep learning field. Although they look scary with extreme depths, it's not a hard job to implement one. Now let's build one of the simplest ResNets - ResNet-56, and train it on the CIFAR-10 dataset.\n\n</p>\n<!-- more -->\n<p lang=\"zh-cn\">\nTensorFlowPythonAPIPythonTensorFlow213\nResNetResNetResNetResNetResNet-56CIFAR-10\n</p>\n<div align=\"center\" class=\"figure\">\n![Let's Rock!](/images/tftutorial/oyo.gif)\n</div>\n<p lang=\"en-us\">\nFirst let's take a look at ResNet-56. It's proposed by Kaiming He et al., and is designed to confirm the effect of residual networks. It has 56 weighted layers, deep but simple. The structure is shown in the figure below:\n</p>\n<p lang=\"zh-cn\">\nResNet-56ResNet56\n</p>\n<div align=\"center\" class=\"figure\">\n<img src=\"/images/tftutorial/resnet56.png\" alt=\"ResNet-56\" width=\"80%\"/>\n\nFig. 1 The structure of ResNet-56\n</div>\n<br>\n<p lang=\"en-us\">\nSeems a little bit long? Don't worry, let's do this step by step.\n</p>\n<p lang=\"zh-cn\">\n\n</p>\n\n## 1 Ingredients\nPython 3.6\n\nTensorFlow 1.4.0\n\nNumpy 1.13.3\n\nOpenCV 3.2.0\n\n[CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)\n<p lang=\"en-us\">\nAlso prepare some basic knowledge on Python programming, digital image processing and convolutional neural networks. If you are already capable of building, training and validating your own neural networks with TensorFlow, you don't have to read this post.\n</p>\n<p lang=\"zh-cn\">\nPythonTensorFlow\n</p>\n\n## 2 Recipe\n### 2.0 Prepare the tools\n<p lang=\"en-us\">\nPrepare(import) the tools for our project, including all that I mentioned above. Like this :P\n</p>\n<p lang=\"zh-cn\">\n(i)(m)(p)(o)(r)(t)\n</p>\n```python\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nimport pickle\nfrom tensor_chain import TensorChain\n```\n<p lang=\"en-us\">\nWait... What's this? TensorChain? Another deep learning framework like TensorFlow?\n\nUh, nope. This is my own encapsulation of some TensorFlow APIs, for the sake of easing your pain. You'll only have to focus on \"what's what\" in the beginning. We'll look into my implementation of this encapsulation later, when you are clear how everything goes. Please [download this file](/files/tensor_chain.py) and put it where your code file is, and import it.\n</p>\n<p lang=\"zh-cn\">\n... TensorChain\n\n...TensorFlow APITensorFlow[](/files/tensor_chain.py)import\n</p>\n\n### 2.1 Decide the input\n<p lang=\"en-us\">\nEvery neural network requires an input - you always have to identify the details of a question, before asking the computer to solve it. All of the variable, constant in TensorFlow are objects of type <em>tf.Tensors</em>. And the <em>tf.placeholder</em> of our input(s) is a special one. Images in CIFAR-10 dataset are RGB images(3 channels) of 32x32(really small), so our input should shaped like [32, 32, 3]. Also, we want to input a little <em>batch</em> of multiple images. Therefore, our input data should be an array of shape <em>[?, 32, 32, 3]</em>. Unknown dimension size can be marked as None, and it will be clear when we feed the model with the actual images. It's coded like this:\n</p>\n<p lang=\"zh-cn\">\nTensorFlow<em>tf.Tensor</em><em>tf.placeholder</em>CIFAR-1032x32RGBRGB[32, 32, 3]<em>batch</em><em>[?, 32, 32, 3]</em>numpyNonebatch\n</p>\n```python\ninput_tensor = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n```\n<p lang=\"en-us\">\n*Ground truth* data also need to be known in supervised learning, so we also have to define a placeholder for the ground truth data: \n</p>\n<p lang=\"zh-cn\">\n*ground truth*ground truthplaceholder\n</p>\n```python\nground_truth = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n```\n<p lang=\"en-us\">\nWe want the label data to be in the one-hot encoding format, which means an array of length 10, denoting 10 classes. Only on one position is a '1', and on other positions are '0's.\n</p>\n<p lang=\"zh-cn\">\nOne-Hot101010\n</p>\n\n### 2.2 Do some operations\n<p lang=\"en-us\">\nFor now, let's use our TensorChain to build it fast. Under most circumstances that we may face, the computations are based on the input data or the result of the former computation, so our network(or say, the most of it) look more like a chain than a web. Every time we add some new operation(layer), we add it to our <em>TensorChain</em> object. Just remember to get the <em>output_tensor</em> of this object(denoting the output tensor of the last operation on the chain) when you need to ue native TensorFlow API.\nThe construction function of TensorChain class requires a Tensor object as the parameter, which is also the input tensor of this chain. As we mentioned earlier, all we have to do is add operations. See my ResNet-56 code:\n</p>\n<p lang=\"zh-cn\">\nTensorChainTensorChainTensorFlow API<em>output_tensor</em>Tensor\nTensorChainTensorResNet-56\n</p>\n```python\nchain = TensorChain(input_tensor) \\\n        .convolution_layer_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 16) \\\n        .residual_block_2d(3, 32, stride=2) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 32) \\\n        .residual_block_2d(3, 64, stride=2) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .residual_block_2d(3, 64) \\\n        .flatten() \\\n        .fully_connected_layer(10)\n```\n<p lang=\"en-us\">\nThis is it? Right, this is it! Isn't it cool? Didn't seem that high, huh? That's because I encapsulated that huge mess of weights and biases, only leaving a few parameters that decide the structure of the network. Later in this pose we'll talk about the actual work that these functions do.\n</p>\n<p lang=\"zh-cn\">\n56\n</p>\n\n### 2.3 Define the loss\n<p lang=\"en-us\">\nIn supervised learning, you always have to tell the learning target to the model. To tell the model how to optimize, you have to let it know how, how much, on which direction should it change its parameters. This is done by using a loss function. Therefore, we need to define a loss function for our ResNet-56 model(which we designed for this classification problem) so that it will learn and optimize.\nA commonly used loss function in classification problems is cross entropy. It's defined below:\n</p>\n<p lang=\"zh-cn\">\n*loss*ResNet-56\n\n</p>\n$$C=-\\frac{1}{n}\\sum_x{y\\ln a+(1-y)\\ln(1-a)}$$\n<p lang=\"en-us\">\nin which \\\\(y\\\\) is the expected(or say correct) output and \\\\(a\\\\) is the actual output.\nThis seems a little bit complicated. But it's not a hard job to implement, since TensorFlow implemented it already! You can also try and implement it yourself within one line if you want. For now we use the pre-defined cross entropy loss function:\n</p>\n<p lang=\"zh-cn\">\n\\\\(y\\\\)\\\\(a\\\\)\n...TensorFlow\n</p>\n```python\nloss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))\n```\n<p lang=\"en-us\">\nand it returns a tf.Tensor that denotes an average of cross entropies(don't forget that this is a batch). As for the 'softmax' before the 'cross_entropy', it's a function that project the data in an array to range 0~1, which allows us to do a comparison between our prediction and the ground truth(in one-hot code). The definition is simple too:\\\n</p>\n<p lang=\"zh-cn\">\nbatchTensorcross_entropy*softmax*0~1exp()\\\n</p>\n$$S_i=\\frac{e^{V_i}}{\\sum_j{e^{V_j}}}$$\n<p></p>\n\n### 2.4 Define the train op\n<p lang=\"en-us\">\nNow we have the loss function. We'll have to tell its value to an *optimizer*, which make our model learn and optimize in order to minimize the loss value. Gradient Descent Optimizer, Adagrad Optimizer, Adam Optimizer and Momentum Optimizer are commonly used optimizers. Here we use an Adam Optimizer for instance. You're free to try any other one here. When\n</p>\n<p lang=\"zh-cn\">\n*optimizer*Gradient Descent OptimizerAdagrad OptimizerAdam OptimizerMomentum Optimizer\\\\(10^-3\\\\)\n</p>\n```python\noptimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n```\n<p lang=\"en-us\">\nAlso, tell the optimizer that what the loss tensor is. The returned object is a train operation.\n</p>\n<p lang=\"zh-cn\">\nTensor*train op*\n</p>\n```python\ntrain = optimizer.minimize(loss)\n```\n<p lang=\"en-us\">\nThe neural network is finished. It's time to grab some data and train it.\n</p>\n<p lang=\"zh-cn\">\n\n</p>\n\n### 2.5 Feed the model with data, and train it!\n<p lang=\"en-us\">\nRemember how we defined the placeholders? It's time to fetch some data that fits the placeholders and train it. See how CIFAR-10 dataset can be fetched on its [website](https://www.cs.toronto.edu/~kriz/cifar.html).\n</p>\n<p lang=\"zh-cn\">\nplaceholderCIFAR-10[](https://www.cs.toronto.edu/~kriz/cifar.html)\n</p>\n```python\ndef unpickle(file):\n    import pickle\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict\n```\n<p lang=\"en-us\">\nThe returned value *dict* is a Python dictionary. Every time we unpickle a file, a dictionary would be returned. Its 'data' key leads to 10000 RGB images of size 32x32, which is stored in a [10000, 3072] array(3072=32*32*3, I guess you know how it's stored now). The 'label' key leads to 10000 values in range 0~9. Obviously we have to reshape the data so as to fit it into the network model:\n</p>\n<p lang=\"zh-cn\">\n*dict*Pythondictbatchdata_batch_1data1000032x32RGB[10000, 3072]3072=10000x32x32x3label100000-9\n</p>\n```python\nbatch = unpickle(DATA_PATH + 'data_batch_{}'.format(i))  # 'i' is the loop variable\n\n# Read the image data\nimage_data = np.reshape(batch[b'data'], (10000, 32, 32, 3), 'F').astype(np.float32)   \nimage_data = image_data / 255                            # Cast range(0, 255) to range(0, 1)\nimage_data = np.transpose(image_data, (0, 2, 1, 3))      # Exchange row and column\n\n# Read the label data and convert into one-hot code\nlabel_data = batch[b'labels']\nnew_label_data = np.zeros((10000, 10))                   \nfor j in range(10000):\n    new_label_data[j][label_data[j]] = 1\n```\n<p lang=\"en-us\">\nThe details for data processing are not covered here. Try doing step-by-step to see the results.\nThe *image_data* and *new_label_data* are contain 10000 pieces of data each. Let's divide them into 100 small batches(100 elements each, including image and label) and feed it into the model. Do this on all the 5 batch files:\n</p>\n<p lang=\"zh-cn\">\n\n*image_data**new_label_data*10000batch100100+5batch\n</p>\n```python\nwith tf.Session() as session:\n    session.run(tf.global_variables_initializer())\n    for j in range(100): # 10000 / BATCH_SIZE\n        # Divide them and get one part\n        image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]\n        label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]\n        \n        # Feed the model\n        session.run(train, feed_dict={\n            input_tensor: image_batch,\n            ground_truth: label_batch\n        })\n```\n<p lang=\"en-us\">\nA *session* - created with *tf.Session()* - is required every time we run a TensorFlow model, no matter when we're training it or evaluating it. The first time you run a model, you'll need to run *session.run(tf.global_variables_initializer())* to initialize the values of the TensorFlow variables defined previously.\nWhen running *session.run()*, you must first decide a TensorFlow operation(or a list of operations) that you need. If its result is dependent on some actual data(which means that some data in one or more placeholders flow to this operation), it's also required that you feed it the actual data by adding a *feed_dict* parameter. For example, I'm training this ResNet-56 model, in which a loss will be calculated with my *ground_truth* and the prediction result that comes from the *input_tensor*. Therefore, I'll have to give a value for each placeholder given above(format: \"placeholder name: corresponding data\"), and fold them in one Python dictionary.\n</p>\n<p lang=\"zh-cn\">\nTensorFlowtf.Session()*session**session.run(tf.global_variables_initializer())*TensorFlow\n*session.run()*operationplaceholderoperation*feed_dict**ground_truth**input_tensor*Pythonfeed_dict\n</p>\n<p lang=\"en-us\">\nI'm also interested in the loss function value in each iteration(which means feeding a batch of data and executing one forward-propagation and one back-propagation) in the training process. Therefore, what I'll fill in the parameter is not just the train op, but also the loss tensor. And the session.run() above should be modified to:\n</p>\n<p lang=\"zh-cn\">\nbatchsession.run()trainlosssession.run()\n</p>\n```python\n        [train_, loss_value] = session.run([train, loss],\n            feed_dict={\n                input_tensor: image_batch,\n                ground_truth: label_batch\n            })\n        print(\"Loss = {}\".format(loss_value)\n```\n<p lang=\"en-us\">\nThis is when the return value of session.run() becomes useful. Its value(s) - corresponding to the first parameter of run() - is/are the actual value(s) of the tensor(s) in the first parameter. In our example, *loss_value* is the actual output of the loss tensor. As for train_, we don't care what it is. Just add it to match the dimensions.\n</p>\n<p lang=\"zh-cn\">\nsession.run()operation*loss_value*losstraintrain_None\n</p>\n<p lang=\"en-us\">\nActually, one epoch(train the model once with the whole dataset) is not enough for the model to fully optimize. I trained this model for 40 epochs and added some loop variables to display the result. You can see my code and my output below. It's highly recommended that you train this with a high-performance GPU, or it would be a century before you train your model to a satisfactory degree.\n</p>\n<p lang=\"zh-cn\">\nepoch40epochGPUGPU\n</p>\n```python\nimport tensorflow as tf\nimport numpy as np\nimport pickle\nfrom tensor_chain import TensorChain\n\ndef unpickle(file):\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict\n\nif __name__ == '__main__':\n    input_tensor = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n    ground_truth = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n\n    chain = TensorChain(input_tensor) \\\n            .convolution_layer_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 16) \\\n            .residual_block_2d(3, 32, stride=2) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 32) \\\n            .residual_block_2d(3, 64, stride=2) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .residual_block_2d(3, 64) \\\n            .flatten() \\\n            .fully_connected_layer(10)\n\n    prediction = chain.output_tensor\n    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))\n\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    train = optimizer.minimize(loss)\n\n    with tf.Session() as session:\n        session.run(tf.global_variables_initializer())\n        iteration = 1\n        BATCH_SIZE = 100\n        DATA_PATH = '../data/cifar-10-batches-py/'\n        for epoch in range(1, 41):\n            for i in range(1, 6):\n                data = unpickle(DATA_PATH + 'data_batch_{}'.format(i))\n                image_data = np.reshape(data[b'data'], (10000, 32, 32, 3), 'F').astype(np.float32)\n                image_data = image_data / 255\n                image_data = np.transpose(image_data, (0, 2, 1, 3))\n                label_data = data[b'labels']\n                new_label_data = np.zeros((10000, 10))\n                for j in range(10000):\n                    new_label_data[j][label_data[j]] = 1\n                for j in range(int(10000 / BATCH_SIZE)):\n                    image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]\n                    label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]\n                    [train_, loss_] = session.run(\n                        [train, loss],\n                        feed_dict={\n                            input_tensor: image_batch,\n                            ground_truth: label_batch\n                        })\n                    iteration += 1\n                    print(\"Epoch {}, Iteration {}, Loss = {}\".format(epoch, iteration, loss_))\n```\n\n<div align=\"center\" class=\"figure\">\n<img src=\"/images/tftutorial/train.png\" alt=\"Training result\" width=\"40%\">\n\nFig. 2 Training result: cross entropy has dropped below 0.5\n</div>\n<p></p>\n\n### 2.6 Conclusion\n<p lang=\"en-us\">\nIn a word, building & training neural network models with TensorFlow involves the following steps:\n\n1. Decide the *input tensor*\n\n2. Add operations(*op*s) based on existing tensors\n\n3. Define the *loss* tensor, just like other tensors\n\n4. Select an *optimizer* and define the *train* op\n\n5. Process *data* and feed the model with them\n</p>\n<p lang=\"zh-cn\">\nTensorFlow\n\n1. **Tensor\n\n2. Tensor*op*\n\n3. **Tensor\n\n4. ****\n\n5. **shape\n</p>\n\n## 3 A Closer Look\n<p lang=\"en-us\">\nWait, it's too late to leave now!\nTensorChain saved you from having to deal with a mess of TensorFlow classes and functions. Now it's time that we take a closer look at how TensorChain is implemented, thus understanding the native TensorFlow APIs.\n</p>\n<p lang=\"zh-cn\">\n\nTensorChainTensorFlowTensorChainTensorFlowAPI\n</p>\n\n### 3.1 TensorFlow variables\n<p lang=\"en-us\">\nLet's begin with TensorFlow variables. Variables in TensorFlow are similar to variables in C, Java or any other strong typed programming languages - they have a type, though not necessarily explicitly decided upon definition. Usually them will change as the training process goes on, getting close to a best value.\nThe most commonly used variables in TensorFlow are weights and biases. I guess that you have seen formulae like:\n</p>\n<p lang=\"zh-cn\">\nTensorFlowTensorFlowCJava\nTensorFlowweightsbiases\n</p>\n$$y=Wx+b$$\n<p lang=\"en-us\">\nThe \\\\(W\\\\) here is the weight, and the \\\\(b\\\\) here is the bias. When implementing some common network layers, they two are always used as the parameters in the layers. For instance, at the very beginning of our ResNet-56, we had a 3x3 sized convolution layer with 16 channels. Its implementation in TensorChain is:\n</p>\n<p lang=\"zh-cn\">\n\\\\(W\\\\)\\\\(b\\\\)ResNet-563x316TensorChain\n</p>\n```python\n    def convolution_layer_2d(self, filter_size: int, num_channels: int, stride: int = 1, name: str = None,\n                             disable_log: bool = False):\n        \"\"\"\n        Add a 2D convolution layer\n        :param filter_size: Filter size(width and height) for this operation\n        :param num_channels: Channel number of this filter\n        :param stride: Stride for this convolution operation\n        :param name: The name of the tensor\n        :param disable_log: Set it True if you don't want this layer to be recorded\n        :return: This object itself\n        \"\"\"\n        filter = self._weights([filter_size, filter_size, self.num_channels, num_channels], layer_name=name,\n                               suffix='filter')\n        bias = self._bias([num_channels], layer_name=name)\n        self.num_channels = num_channels\n        self.output_tensor = tf.nn.conv2d(self.output_tensor, filter,\n                                          [1, stride, stride, 1], 'SAME', name=name)\n        self.output_tensor = tf.add(self.output_tensor, bias)\n        self._log_layer(\n            '2D Convolution layer, filter size = {}x{}, stride = {}, {} channels'.format(filter_size, filter_size,\n                                                                                         stride,\n                                                                                         num_channels),\n            disable=disable_log)\n        return self\n```\n<p lang=\"en-us\">\nSee? On line 16, we used a *tf.nn.conv2d()* function, the parameters of which are *input*, *filter*, *strides*, *padding*, etc. As can be guessed from the names, this function does a convolution operation with out input and the weights(the convolution *filter* here). A *bias* is added to the result as the final output. There are also many people who argue that the bias here is meaningless and should removed. One line of code is sufficient for defining a variable:\n</p>\n<p lang=\"zh-cn\">\n16*tf.nn.conv2d()**input**filter**strides**padding*Tensor*filter*inputTensor*bias*biasbias\n</p>\n```python\ntf.Variable(tf.truncated_normal(shape, stddev=sigma), dtype=tf.float32, name=suffix)\n```\n<p lang=\"en-us\">\nTo define weight or bias variables, create a *tf.Variable* object. Usually you'll need to give the *initial_value* which also decides the shape of this tensor. *tf.truncated_normal()* and *tf.constant()* are usually used as the initial values. Also, other APIs - function *tf.get_variable()* and package *tf.initializers* are frequently used when using some more methods for initialization. I strongly recommend that you try using these APIs yourself.\n</p>\n<p lang=\"zh-cn\">\n*tf.Variable**initial_value*TFshapeshapeAPI*tf.get_variable()**tf.initializers*API\n</p>\n\n### 3.2 Tensors and operations\n<p lang=\"en-us\">\nGoing on with the parameters of the *tf.nn.conv2d()* function. The required parameters also include *strides* and *padding*. You should have already learned about what strides mean in convolution, and I'll only talk about their formats. *strides* require a 1-D vector with a length of 4, like [1, 2, 2, 1]. The 1st and the 4th number is always 1(in order to match dimensions with the input), while the 2nd and the 3rd means the vertical stride and the horizonal stride. \nThe 4th parameter *padding* is a little bit different from its definition in convolution operation. It requires 'SAME' of 'VALID', denoting 'with' or 'without' zero paddings. When it's 'SAME', zero padding is introduced to make the shapes match as needed, equally on every side of the input map.\n</p>\n<p lang=\"zh-cn\">\n*tf.nn.conv2d()**strides**padding*stride*strides*141API\n*padding*padding'SAME''VALID''SAME'stride>1/stride\n</p>\n<p lang=\"en-us\">\ntf.nn.conv2d() is just an example of TensorFlow *operations*. Other functions like *tf.matmul()*, *tf.reduce_mean()*, *tf.global_variables_initializer()*, *tf.losses.softmax_cross_entropy()*, *tf.truncated_normal()* are all operations. Operation functions return tensors(*tf.truncated_normal* also return a tensor, a tensor with initializers).\n</p>\n<p lang=\"zh-cn\">\ntf.nn.conv2d()TensorFlow*operation**tf.matmul()**tf.reduce_mean()**tf.nn.relu()**tf.batch_normalization()**tf.global_variables_initializer()**tf.losses.softmax_cross_entropy()**tf.truncated_normal()*TensorFlowTensorFlowTensor*tf.truncated_normal()*Tensor\n</p>\n<p lang=\"en-us\">\nAll the functions in the TensorChain class are based on the most basic TensorFlow operations and variables. After learning about these basic TensorFlow concepts, actually you can already abandon TensorChain, go and try implementing your own neural networks yourself!\n</p>\n<p lang=\"zh-cn\">\nTensorChainTensorFlowTensorChain\n</p>\n\n## 4 Spices\n<p lang=\"en-us\">\nI'm not joking just now! But I know that there are a lot of things that you still don't understand about using TensorFlow - like \"how do I visualize my computation graph\", \"how do I save/load my model to/from files\", \"how do I record some tensors' values while training\" or \"how do I view the loss curves\" - after all TensorFlow APIs are far more complicated than just building those nets. Those are also important techniques in your research. If you'd rather ask me than spending some time experimenting, please go on with reading.\n</p>\n<p lang=\"zh-cn\">\nTensorFlow/TensorTensorFlowAPITensorFlow\n</p>\n\n### 4.1 Saving and loading your model\n<p lang=\"en-us\">\nThe very first thing that you may want to do - after training a network model with nice outcomes - would be saving it. Saving a model is fairly easy - just use a *tf.train.Saver* object. See my code below:\n</p>\n<p lang=\"zh-cn\">\n*tf.train.Saver*\n</p>\n```python\nwith tf.Session() as session:\n    # Train it for some iterations\n    # Train it for some iterations\n    # Train it for some iterations\n    saver = tf.train.Saver()\n    saver.save(session, 'models/model.ckpt')\n```\n<p lang=\"en-us\">\nI saved my model and variable values to 'models/model.ckpt'. But actually, you'll find 3 files in the 'models' directory - *model.ckpt.data-00000-of-00001*, *model.ckpt.meta* and *model.ckpt.index* - none of which is 'model.ckpt'! That's because TensorFlow stores the graph structure separately from variables values. The *.meta* file describes the saved graph structure; the *.index* file records the mappings between tensor names and tensor metadata; and the *.data-00000-of-00001* file - which is always the biggest one - saves all the variable values. If you need the graph data together with the variable values to be loaded, use a Saver to load after creating a session:\n</p>\n<p lang=\"zh-cn\">\n'models/model.ckpt'models*model.ckpt.data-00000-of-00001**model.ckpt.meta**model.ckpt.index*model.ckptTensorFlow*.meta**.index*TensornameTensor*.data-00000-of-00001*TensorFlowsessionSaver\n</p>\n```python\nwith tf.Session() as session:\n    saver = tf.train.Saver()\n    saver.restore(session, 'models/model.ckpt')\n    # Then continue doing everything just like the model is just trained\n```\n<p lang=\"en-us\">\nRemember that session.run(tf.global_variables_initializer()) shouldn't be executed, since variables are already initialized with your saved *.data-0000-of-00001* file.\nIf you only need the graph to be loaded, only use the *.meta* file:\n</p>\n<p lang=\"zh-cn\">\nsession.run(tf.global_variables_initializer())checkpoint\n*.meta*\n</p>\n```python\nwith tf.Session() as session:\n    tf.train.import_meta_graph('models/model.ckpt.meta')\n    # Then continue doing everything just like the model is just built\n```\n<p lang=\"en-us\">\nFunction *tf.train.import_meta_graph()* loads(appends) the graph to your current computation graph. The values of tensors are still uninitialized so you'll have to execute session.run(tf.global_variables_initializer()) again. The tensors that you defined in the model can be retrieved by their names(property of the Tensor objects, instead of Python variable names). For example:\n</p>\n<p lang=\"zh-cn\">\n*tf.train.import_meta_graph()*Tensorsession.run(tf.global_variables_initializer())\n</p>\n```python\nwith tf.Session() as session:\n    # Recover the model here\n\n    graph = tf.get_default_graph()\n    image_tensor = graph.get_tensor_by_name('input_image:0')\n    loss = graph.get_tensor_by_name('loss:0')\n    train = graph.get_operation_by_name('train)\n```\n<p lang=\"en-us\">\nTo retrieve normal tensors, you'll have to append a *':0'* to the name of the op. This means getting the associated tensor of the op. *train* is a little special - we only need the op, so the function is *get_operation_by_name()* so the ':0' is not necessary.\n</p>\n<p lang=\"zh-cn\">\nTensorTensorname*':0'*Tensor*train*op*get_operation_by_name()*Tensor':0'\n</p>\n\n<p lang=\"en-us\" align=\"center\">\n[THIS SECTION IS UNDER CONSTRUCTION]\n</p>\n<p lang=\"zh-cn\" align=\"center\">\n[]\n</p>","slug":"diy-resnet","published":1,"updated":"2018-11-18T05:44:45.857Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cjomm0o22000twgwn4e414y3j","content":"<p><script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML\" async></script><br><br></p>\n<p lang=\"en-us\"><br>TensorFlow is a powerful open-source deep learning framework, supporting various languages including Python. However, its APIs are far too complicated for a beginner in deep learning(especially those who are new to Python). In order to ease the pain of having to understand the mess of various elements in TensorFlow computation graphs, I made this tutorial to help beginners take the first bite of the cake.<br><br>ResNets are one of the greatest works in the deep learning field. Although they look scary with extreme depths, it&#39;s not a hard job to implement one. Now let&#39;s build one of the simplest ResNets - ResNet-56, and train it on the CIFAR-10 dataset.<br><br></p><br><a id=\"more\"></a><br><p lang=\"zh-cn\"><br>TensorFlowPythonAPIPythonTensorFlow213<br>ResNetResNetResNetResNetResNet-56CIFAR-10<br></p><br><div align=\"center\" class=\"figure\"><br><img src=\"/images/tftutorial/oyo.gif\" alt=\"Let&#39;s Rock!\"><br></div><br><p lang=\"en-us\"><br>First let&#39;s take a look at ResNet-56. It&#39;s proposed by Kaiming He et al., and is designed to confirm the effect of residual networks. It has 56 weighted layers, deep but simple. The structure is shown in the figure below:<br></p><br><p lang=\"zh-cn\"><br>ResNet-56ResNet56<br></p><br><div align=\"center\" class=\"figure\"><br><img src=\"/images/tftutorial/resnet56.png\" alt=\"ResNet-56\" width=\"80%\"><br><br>Fig. 1 The structure of ResNet-56<br></div><br><br><br><p lang=\"en-us\"><br>Seems a little bit long? Don&#39;t worry, let&#39;s do this step by step.<br></p><br><p lang=\"zh-cn\"><br><br></p>\n\n<h2 id=\"1-Ingredients\"><a href=\"#1-Ingredients\" class=\"headerlink\" title=\"1 Ingredients\"></a>1 Ingredients</h2><p>Python 3.6</p>\n<p>TensorFlow 1.4.0</p>\n<p>Numpy 1.13.3</p>\n<p>OpenCV 3.2.0</p>\n<p><a href=\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\" target=\"_blank\" rel=\"noopener\">CIFAR-10 Dataset</a></p>\n<p lang=\"en-us\"><br>Also prepare some basic knowledge on Python programming, digital image processing and convolutional neural networks. If you are already capable of building, training and validating your own neural networks with TensorFlow, you don&#39;t have to read this post.<br></p><br><p lang=\"zh-cn\"><br>PythonTensorFlow<br></p>\n\n<h2 id=\"2-Recipe\"><a href=\"#2-Recipe\" class=\"headerlink\" title=\"2 Recipe\"></a>2 Recipe</h2><h3 id=\"2-0-Prepare-the-tools\"><a href=\"#2-0-Prepare-the-tools\" class=\"headerlink\" title=\"2.0 Prepare the tools\"></a>2.0 Prepare the tools</h3><p lang=\"en-us\"><br>Prepare(import) the tools for our project, including all that I mentioned above. Like this :P<br></p><br><p lang=\"zh-cn\"><br>(i)(m)(p)(o)(r)(t)<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> cv2</span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensor_chain <span class=\"keyword\">import</span> TensorChain</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>Wait... What&#39;s this? TensorChain? Another deep learning framework like TensorFlow?<br><br>Uh, nope. This is my own encapsulation of some TensorFlow APIs, for the sake of easing your pain. You&#39;ll only have to focus on &quot;what&#39;s what&quot; in the beginning. We&#39;ll look into my implementation of this encapsulation later, when you are clear how everything goes. Please <a href=\"/files/tensor_chain.py\">download this file</a> and put it where your code file is, and import it.<br></p><br><p lang=\"zh-cn\"><br>... TensorChain<br><br>...TensorFlow APITensorFlow<a href=\"/files/tensor_chain.py\"></a>import<br></p>\n\n<h3 id=\"2-1-Decide-the-input\"><a href=\"#2-1-Decide-the-input\" class=\"headerlink\" title=\"2.1 Decide the input\"></a>2.1 Decide the input</h3><p lang=\"en-us\"><br>Every neural network requires an input - you always have to identify the details of a question, before asking the computer to solve it. All of the variable, constant in TensorFlow are objects of type <em>tf.Tensors</em>. And the <em>tf.placeholder</em> of our input(s) is a special one. Images in CIFAR-10 dataset are RGB images(3 channels) of 32x32(really small), so our input should shaped like [32, 32, 3]. Also, we want to input a little <em>batch</em> of multiple images. Therefore, our input data should be an array of shape <em>[?, 32, 32, 3]</em>. Unknown dimension size can be marked as None, and it will be clear when we feed the model with the actual images. It&#39;s coded like this:<br></p><br><p lang=\"zh-cn\"><br>TensorFlow<em>tf.Tensor</em><em>tf.placeholder</em>CIFAR-1032x32RGBRGB[32, 32, 3]<em>batch</em><em>[?, 32, 32, 3]</em>numpyNonebatch<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input_tensor = tf.placeholder(dtype=tf.float32, shape=[<span class=\"keyword\">None</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>])</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br><em>Ground truth</em> data also need to be known in supervised learning, so we also have to define a placeholder for the ground truth data:<br></p><br><p lang=\"zh-cn\"><br><em>ground truth</em>ground truthplaceholder<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ground_truth = tf.placeholder(dtype=tf.float32, shape=[<span class=\"keyword\">None</span>, <span class=\"number\">10</span>])</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>We want the label data to be in the one-hot encoding format, which means an array of length 10, denoting 10 classes. Only on one position is a &#39;1&#39;, and on other positions are &#39;0&#39;s.<br></p><br><p lang=\"zh-cn\"><br>One-Hot101010<br></p>\n\n<h3 id=\"2-2-Do-some-operations\"><a href=\"#2-2-Do-some-operations\" class=\"headerlink\" title=\"2.2 Do some operations\"></a>2.2 Do some operations</h3><p lang=\"en-us\"><br>For now, let&#39;s use our TensorChain to build it fast. Under most circumstances that we may face, the computations are based on the input data or the result of the former computation, so our network(or say, the most of it) look more like a chain than a web. Every time we add some new operation(layer), we add it to our <em>TensorChain</em> object. Just remember to get the <em>output_tensor</em> of this object(denoting the output tensor of the last operation on the chain) when you need to ue native TensorFlow API.<br>The construction function of TensorChain class requires a Tensor object as the parameter, which is also the input tensor of this chain. As we mentioned earlier, all we have to do is add operations. See my ResNet-56 code:<br></p><br><p lang=\"zh-cn\"><br>TensorChainTensorChainTensorFlow API<em>output_tensor</em>Tensor<br>TensorChainTensorResNet-56<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chain = TensorChain(input_tensor) \\</span><br><span class=\"line\">        .convolution_layer_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>, stride=<span class=\"number\">2</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>, stride=<span class=\"number\">2</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .flatten() \\</span><br><span class=\"line\">        .fully_connected_layer(<span class=\"number\">10</span>)</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>This is it? Right, this is it! Isn&#39;t it cool? Didn&#39;t seem that high, huh? That&#39;s because I encapsulated that huge mess of weights and biases, only leaving a few parameters that decide the structure of the network. Later in this pose we&#39;ll talk about the actual work that these functions do.<br></p><br><p lang=\"zh-cn\"><br>56<br></p>\n\n<h3 id=\"2-3-Define-the-loss\"><a href=\"#2-3-Define-the-loss\" class=\"headerlink\" title=\"2.3 Define the loss\"></a>2.3 Define the loss</h3><p lang=\"en-us\"><br>In supervised learning, you always have to tell the learning target to the model. To tell the model how to optimize, you have to let it know how, how much, on which direction should it change its parameters. This is done by using a loss function. Therefore, we need to define a loss function for our ResNet-56 model(which we designed for this classification problem) so that it will learn and optimize.<br>A commonly used loss function in classification problems is cross entropy. It&#39;s defined below:<br></p><br><p lang=\"zh-cn\"><br><em>loss</em>ResNet-56<br><br></p><br>$$C=-\\frac{1}{n}\\sum_x{y\\ln a+(1-y)\\ln(1-a)}$$<br><p lang=\"en-us\"><br>in which \\(y\\) is the expected(or say correct) output and \\(a\\) is the actual output.<br>This seems a little bit complicated. But it&#39;s not a hard job to implement, since TensorFlow implemented it already! You can also try and implement it yourself within one line if you want. For now we use the pre-defined cross entropy loss function:<br></p><br><p lang=\"zh-cn\"><br>\\(y\\)\\(a\\)<br>...TensorFlow<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>and it returns a tf.Tensor that denotes an average of cross entropies(don&#39;t forget that this is a batch). As for the &#39;softmax&#39; before the &#39;cross_entropy&#39;, it&#39;s a function that project the data in an array to range 0~1, which allows us to do a comparison between our prediction and the ground truth(in one-hot code). The definition is simple too:\\<br></p><br><p lang=\"zh-cn\"><br>batchTensorcross_entropy<em>softmax</em>0~1exp()\\<br></p><br>$$S_i=\\frac{e^{V_i}}{\\sum_j{e^{V_j}}}$$<br><p></p>\n\n<h3 id=\"2-4-Define-the-train-op\"><a href=\"#2-4-Define-the-train-op\" class=\"headerlink\" title=\"2.4 Define the train op\"></a>2.4 Define the train op</h3><p lang=\"en-us\"><br>Now we have the loss function. We&#39;ll have to tell its value to an <em>optimizer</em>, which make our model learn and optimize in order to minimize the loss value. Gradient Descent Optimizer, Adagrad Optimizer, Adam Optimizer and Momentum Optimizer are commonly used optimizers. Here we use an Adam Optimizer for instance. You&#39;re free to try any other one here. When<br></p><br><p lang=\"zh-cn\"><br><em>optimizer</em>Gradient Descent OptimizerAdagrad OptimizerAdam OptimizerMomentum Optimizer\\(10^-3\\)<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">optimizer = tf.train.AdamOptimizer(learning_rate=<span class=\"number\">0.001</span>)</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>Also, tell the optimizer that what the loss tensor is. The returned object is a train operation.<br></p><br><p lang=\"zh-cn\"><br>Tensor<em>train op</em><br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train = optimizer.minimize(loss)</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>The neural network is finished. It&#39;s time to grab some data and train it.<br></p><br><p lang=\"zh-cn\"><br><br></p>\n\n<h3 id=\"2-5-Feed-the-model-with-data-and-train-it\"><a href=\"#2-5-Feed-the-model-with-data-and-train-it\" class=\"headerlink\" title=\"2.5 Feed the model with data, and train it!\"></a>2.5 Feed the model with data, and train it!</h3><p lang=\"en-us\"><br>Remember how we defined the placeholders? It&#39;s time to fetch some data that fits the placeholders and train it. See how CIFAR-10 dataset can be fetched on its <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\" target=\"_blank\" rel=\"noopener\">website</a>.<br></p><br><p lang=\"zh-cn\"><br>placeholderCIFAR-10<a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\" target=\"_blank\" rel=\"noopener\"></a><br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">unpickle</span><span class=\"params\">(file)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">import</span> pickle</span><br><span class=\"line\">    <span class=\"keyword\">with</span> open(file, <span class=\"string\">'rb'</span>) <span class=\"keyword\">as</span> fo:</span><br><span class=\"line\">        dict = pickle.load(fo, encoding=<span class=\"string\">'bytes'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dict</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>The returned value <em>dict</em> is a Python dictionary. Every time we unpickle a file, a dictionary would be returned. Its &#39;data&#39; key leads to 10000 RGB images of size 32x32, which is stored in a [10000, 3072] array(3072=32<em>32</em>3, I guess you know how it&#39;s stored now). The &#39;label&#39; key leads to 10000 values in range 0~9. Obviously we have to reshape the data so as to fit it into the network model:<br></p><br><p lang=\"zh-cn\"><br><em>dict</em>Pythondictbatchdata_batch_1data1000032x32RGB[10000, 3072]3072=10000x32x32x3label100000-9<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch = unpickle(DATA_PATH + <span class=\"string\">'data_batch_&#123;&#125;'</span>.format(i))  <span class=\"comment\"># 'i' is the loop variable</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Read the image data</span></span><br><span class=\"line\">image_data = np.reshape(batch[<span class=\"string\">b'data'</span>], (<span class=\"number\">10000</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>), <span class=\"string\">'F'</span>).astype(np.float32)   </span><br><span class=\"line\">image_data = image_data / <span class=\"number\">255</span>                            <span class=\"comment\"># Cast range(0, 255) to range(0, 1)</span></span><br><span class=\"line\">image_data = np.transpose(image_data, (<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">3</span>))      <span class=\"comment\"># Exchange row and column</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Read the label data and convert into one-hot code</span></span><br><span class=\"line\">label_data = batch[<span class=\"string\">b'labels'</span>]</span><br><span class=\"line\">new_label_data = np.zeros((<span class=\"number\">10000</span>, <span class=\"number\">10</span>))                   </span><br><span class=\"line\"><span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">10000</span>):</span><br><span class=\"line\">    new_label_data[j][label_data[j]] = <span class=\"number\">1</span></span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>The details for data processing are not covered here. Try doing step-by-step to see the results.<br>The <em>image_data</em> and <em>new_label_data</em> are contain 10000 pieces of data each. Let&#39;s divide them into 100 small batches(100 elements each, including image and label) and feed it into the model. Do this on all the 5 batch files:<br></p><br><p lang=\"zh-cn\"><br><br><em>image_data</em><em>new_label_data</em>10000batch100100+5batch<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    session.run(tf.global_variables_initializer())</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">100</span>): <span class=\"comment\"># 10000 / BATCH_SIZE</span></span><br><span class=\"line\">        <span class=\"comment\"># Divide them and get one part</span></span><br><span class=\"line\">        image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class=\"line\">        label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># Feed the model</span></span><br><span class=\"line\">        session.run(train, feed_dict=&#123;</span><br><span class=\"line\">            input_tensor: image_batch,</span><br><span class=\"line\">            ground_truth: label_batch</span><br><span class=\"line\">        &#125;)</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>A <em>session</em> - created with <em>tf.Session()</em> - is required every time we run a TensorFlow model, no matter when we&#39;re training it or evaluating it. The first time you run a model, you&#39;ll need to run <em>session.run(tf.global_variables_initializer())</em> to initialize the values of the TensorFlow variables defined previously.<br>When running <em>session.run()</em>, you must first decide a TensorFlow operation(or a list of operations) that you need. If its result is dependent on some actual data(which means that some data in one or more placeholders flow to this operation), it&#39;s also required that you feed it the actual data by adding a <em>feed_dict</em> parameter. For example, I&#39;m training this ResNet-56 model, in which a loss will be calculated with my <em>ground_truth</em> and the prediction result that comes from the <em>input_tensor</em>. Therefore, I&#39;ll have to give a value for each placeholder given above(format: &quot;placeholder name: corresponding data&quot;), and fold them in one Python dictionary.<br></p><br><p lang=\"zh-cn\"><br>TensorFlowtf.Session()<em>session</em><em>session.run(tf.global_variables_initializer())</em>TensorFlow<br><em>session.run()</em>operationplaceholderoperation<em>feed_dict</em><em>ground_truth</em><em>input_tensor</em>Pythonfeed_dict<br></p><br><p lang=\"en-us\"><br>I&#39;m also interested in the loss function value in each iteration(which means feeding a batch of data and executing one forward-propagation and one back-propagation) in the training process. Therefore, what I&#39;ll fill in the parameter is not just the train op, but also the loss tensor. And the session.run() above should be modified to:<br></p><br><p lang=\"zh-cn\"><br>batchsession.run()trainlosssession.run()<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[train_, loss_value] = session.run([train, loss],</span><br><span class=\"line\">    feed_dict=&#123;</span><br><span class=\"line\">        input_tensor: image_batch,</span><br><span class=\"line\">        ground_truth: label_batch</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">print(<span class=\"string\">\"Loss = &#123;&#125;\"</span>.format(loss_value)</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>This is when the return value of session.run() becomes useful. Its value(s) - corresponding to the first parameter of run() - is/are the actual value(s) of the tensor(s) in the first parameter. In our example, <em>loss_value</em> is the actual output of the loss tensor. As for train_, we don&#39;t care what it is. Just add it to match the dimensions.<br></p><br><p lang=\"zh-cn\"><br>session.run()operation<em>loss_value</em>losstraintrain_None<br></p><br><p lang=\"en-us\"><br>Actually, one epoch(train the model once with the whole dataset) is not enough for the model to fully optimize. I trained this model for 40 epochs and added some loop variables to display the result. You can see my code and my output below. It&#39;s highly recommended that you train this with a high-performance GPU, or it would be a century before you train your model to a satisfactory degree.<br></p><br><p lang=\"zh-cn\"><br>epoch40epochGPUGPU<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensor_chain <span class=\"keyword\">import</span> TensorChain</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">unpickle</span><span class=\"params\">(file)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> open(file, <span class=\"string\">'rb'</span>) <span class=\"keyword\">as</span> fo:</span><br><span class=\"line\">        dict = pickle.load(fo, encoding=<span class=\"string\">'bytes'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dict</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    input_tensor = tf.placeholder(dtype=tf.float32, shape=[<span class=\"keyword\">None</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>])</span><br><span class=\"line\">    ground_truth = tf.placeholder(dtype=tf.float32, shape=[<span class=\"keyword\">None</span>, <span class=\"number\">10</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    chain = TensorChain(input_tensor) \\</span><br><span class=\"line\">            .convolution_layer_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>, stride=<span class=\"number\">2</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>, stride=<span class=\"number\">2</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .flatten() \\</span><br><span class=\"line\">            .fully_connected_layer(<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    prediction = chain.output_tensor</span><br><span class=\"line\">    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))</span><br><span class=\"line\"></span><br><span class=\"line\">    optimizer = tf.train.AdamOptimizer(learning_rate=<span class=\"number\">0.001</span>)</span><br><span class=\"line\">    train = optimizer.minimize(loss)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">        session.run(tf.global_variables_initializer())</span><br><span class=\"line\">        iteration = <span class=\"number\">1</span></span><br><span class=\"line\">        BATCH_SIZE = <span class=\"number\">100</span></span><br><span class=\"line\">        DATA_PATH = <span class=\"string\">'../data/cifar-10-batches-py/'</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, <span class=\"number\">41</span>):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, <span class=\"number\">6</span>):</span><br><span class=\"line\">                data = unpickle(DATA_PATH + <span class=\"string\">'data_batch_&#123;&#125;'</span>.format(i))</span><br><span class=\"line\">                image_data = np.reshape(data[<span class=\"string\">b'data'</span>], (<span class=\"number\">10000</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>), <span class=\"string\">'F'</span>).astype(np.float32)</span><br><span class=\"line\">                image_data = image_data / <span class=\"number\">255</span></span><br><span class=\"line\">                image_data = np.transpose(image_data, (<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">3</span>))</span><br><span class=\"line\">                label_data = data[<span class=\"string\">b'labels'</span>]</span><br><span class=\"line\">                new_label_data = np.zeros((<span class=\"number\">10000</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\">                <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">10000</span>):</span><br><span class=\"line\">                    new_label_data[j][label_data[j]] = <span class=\"number\">1</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(int(<span class=\"number\">10000</span> / BATCH_SIZE)):</span><br><span class=\"line\">                    image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class=\"line\">                    label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class=\"line\">                    [train_, loss_] = session.run(</span><br><span class=\"line\">                        [train, loss],</span><br><span class=\"line\">                        feed_dict=&#123;</span><br><span class=\"line\">                            input_tensor: image_batch,</span><br><span class=\"line\">                            ground_truth: label_batch</span><br><span class=\"line\">                        &#125;)</span><br><span class=\"line\">                    iteration += <span class=\"number\">1</span></span><br><span class=\"line\">                    print(<span class=\"string\">\"Epoch &#123;&#125;, Iteration &#123;&#125;, Loss = &#123;&#125;\"</span>.format(epoch, iteration, loss_))</span><br></pre></td></tr></table></figure><br><br><div align=\"center\" class=\"figure\"><br><img src=\"/images/tftutorial/train.png\" alt=\"Training result\" width=\"40%\"><br><br>Fig. 2 Training result: cross entropy has dropped below 0.5<br></div><br><p></p>\n\n<h3 id=\"2-6-Conclusion\"><a href=\"#2-6-Conclusion\" class=\"headerlink\" title=\"2.6 Conclusion\"></a>2.6 Conclusion</h3><p lang=\"en-us\"><br>In a word, building &amp; training neural network models with TensorFlow involves the following steps:<br><br>1. Decide the <em>input tensor</em><br><br>2. Add operations(<em>op</em>s) based on existing tensors<br><br>3. Define the <em>loss</em> tensor, just like other tensors<br><br>4. Select an <em>optimizer</em> and define the <em>train</em> op<br><br>5. Process <em>data</em> and feed the model with them<br></p><br><p lang=\"zh-cn\"><br>TensorFlow<br><br>1. <em></em>Tensor<br><br>2. Tensor<em>op</em><br><br>3. <em></em>Tensor<br><br>4. <em></em><em></em><br><br>5. <em></em>shape<br></p>\n\n<h2 id=\"3-A-Closer-Look\"><a href=\"#3-A-Closer-Look\" class=\"headerlink\" title=\"3 A Closer Look\"></a>3 A Closer Look</h2><p lang=\"en-us\"><br>Wait, it&#39;s too late to leave now!<br>TensorChain saved you from having to deal with a mess of TensorFlow classes and functions. Now it&#39;s time that we take a closer look at how TensorChain is implemented, thus understanding the native TensorFlow APIs.<br></p><br><p lang=\"zh-cn\"><br><br>TensorChainTensorFlowTensorChainTensorFlowAPI<br></p>\n\n<h3 id=\"3-1-TensorFlow-variables\"><a href=\"#3-1-TensorFlow-variables\" class=\"headerlink\" title=\"3.1 TensorFlow variables\"></a>3.1 TensorFlow variables</h3><p lang=\"en-us\"><br>Let&#39;s begin with TensorFlow variables. Variables in TensorFlow are similar to variables in C, Java or any other strong typed programming languages - they have a type, though not necessarily explicitly decided upon definition. Usually them will change as the training process goes on, getting close to a best value.<br>The most commonly used variables in TensorFlow are weights and biases. I guess that you have seen formulae like:<br></p><br><p lang=\"zh-cn\"><br>TensorFlowTensorFlowCJava<br>TensorFlowweightsbiases<br></p><br>$$y=Wx+b$$<br><p lang=\"en-us\"><br>The \\(W\\) here is the weight, and the \\(b\\) here is the bias. When implementing some common network layers, they two are always used as the parameters in the layers. For instance, at the very beginning of our ResNet-56, we had a 3x3 sized convolution layer with 16 channels. Its implementation in TensorChain is:<br></p><br><p lang=\"zh-cn\"><br>\\(W\\)\\(b\\)ResNet-563x316TensorChain<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">convolution_layer_2d</span><span class=\"params\">(self, filter_size: int, num_channels: int, stride: int = <span class=\"number\">1</span>, name: str = None,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                         disable_log: bool = False)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Add a 2D convolution layer</span></span><br><span class=\"line\"><span class=\"string\">    :param filter_size: Filter size(width and height) for this operation</span></span><br><span class=\"line\"><span class=\"string\">    :param num_channels: Channel number of this filter</span></span><br><span class=\"line\"><span class=\"string\">    :param stride: Stride for this convolution operation</span></span><br><span class=\"line\"><span class=\"string\">    :param name: The name of the tensor</span></span><br><span class=\"line\"><span class=\"string\">    :param disable_log: Set it True if you don't want this layer to be recorded</span></span><br><span class=\"line\"><span class=\"string\">    :return: This object itself</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    filter = self._weights([filter_size, filter_size, self.num_channels, num_channels], layer_name=name,</span><br><span class=\"line\">                           suffix=<span class=\"string\">'filter'</span>)</span><br><span class=\"line\">    bias = self._bias([num_channels], layer_name=name)</span><br><span class=\"line\">    self.num_channels = num_channels</span><br><span class=\"line\">    self.output_tensor = tf.nn.conv2d(self.output_tensor, filter,</span><br><span class=\"line\">                                      [<span class=\"number\">1</span>, stride, stride, <span class=\"number\">1</span>], <span class=\"string\">'SAME'</span>, name=name)</span><br><span class=\"line\">    self.output_tensor = tf.add(self.output_tensor, bias)</span><br><span class=\"line\">    self._log_layer(</span><br><span class=\"line\">        <span class=\"string\">'2D Convolution layer, filter size = &#123;&#125;x&#123;&#125;, stride = &#123;&#125;, &#123;&#125; channels'</span>.format(filter_size, filter_size,</span><br><span class=\"line\">                                                                                     stride,</span><br><span class=\"line\">                                                                                     num_channels),</span><br><span class=\"line\">        disable=disable_log)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> self</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>See? On line 16, we used a <em>tf.nn.conv2d()</em> function, the parameters of which are <em>input</em>, <em>filter</em>, <em>strides</em>, <em>padding</em>, etc. As can be guessed from the names, this function does a convolution operation with out input and the weights(the convolution <em>filter</em> here). A <em>bias</em> is added to the result as the final output. There are also many people who argue that the bias here is meaningless and should removed. One line of code is sufficient for defining a variable:<br></p><br><p lang=\"zh-cn\"><br>16<em>tf.nn.conv2d()</em><em>input</em><em>filter</em><em>strides</em><em>padding</em>Tensor<em>filter</em>inputTensor<em>bias</em>biasbias<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tf.Variable(tf.truncated_normal(shape, stddev=sigma), dtype=tf.float32, name=suffix)</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>To define weight or bias variables, create a <em>tf.Variable</em> object. Usually you&#39;ll need to give the <em>initial_value</em> which also decides the shape of this tensor. <em>tf.truncated_normal()</em> and <em>tf.constant()</em> are usually used as the initial values. Also, other APIs - function <em>tf.get_variable()</em> and package <em>tf.initializers</em> are frequently used when using some more methods for initialization. I strongly recommend that you try using these APIs yourself.<br></p><br><p lang=\"zh-cn\"><br><em>tf.Variable</em><em>initial_value</em>TFshapeshapeAPI<em>tf.get_variable()</em><em>tf.initializers</em>API<br></p>\n\n<h3 id=\"3-2-Tensors-and-operations\"><a href=\"#3-2-Tensors-and-operations\" class=\"headerlink\" title=\"3.2 Tensors and operations\"></a>3.2 Tensors and operations</h3><p lang=\"en-us\"><br>Going on with the parameters of the <em>tf.nn.conv2d()</em> function. The required parameters also include <em>strides</em> and <em>padding</em>. You should have already learned about what strides mean in convolution, and I&#39;ll only talk about their formats. <em>strides</em> require a 1-D vector with a length of 4, like [1, 2, 2, 1]. The 1st and the 4th number is always 1(in order to match dimensions with the input), while the 2nd and the 3rd means the vertical stride and the horizonal stride.<br>The 4th parameter <em>padding</em> is a little bit different from its definition in convolution operation. It requires &#39;SAME&#39; of &#39;VALID&#39;, denoting &#39;with&#39; or &#39;without&#39; zero paddings. When it&#39;s &#39;SAME&#39;, zero padding is introduced to make the shapes match as needed, equally on every side of the input map.<br></p><br><p lang=\"zh-cn\"><br><em>tf.nn.conv2d()</em><em>strides</em><em>padding</em>stride<em>strides</em>141API<br><em>padding</em>padding&#39;SAME&#39;&#39;VALID&#39;&#39;SAME&#39;stride&gt;1/stride<br></p><br><p lang=\"en-us\"><br>tf.nn.conv2d() is just an example of TensorFlow <em>operations</em>. Other functions like <em>tf.matmul()</em>, <em>tf.reduce_mean()</em>, <em>tf.global_variables_initializer()</em>, <em>tf.losses.softmax_cross_entropy()</em>, <em>tf.truncated_normal()</em> are all operations. Operation functions return tensors(<em>tf.truncated_normal</em> also return a tensor, a tensor with initializers).<br></p><br><p lang=\"zh-cn\"><br>tf.nn.conv2d()TensorFlow<em>operation</em><em>tf.matmul()</em><em>tf.reduce_mean()</em><em>tf.nn.relu()</em><em>tf.batch_normalization()</em><em>tf.global_variables_initializer()</em><em>tf.losses.softmax_cross_entropy()</em><em>tf.truncated_normal()</em>TensorFlowTensorFlowTensor<em>tf.truncated_normal()</em>Tensor<br></p><br><p lang=\"en-us\"><br>All the functions in the TensorChain class are based on the most basic TensorFlow operations and variables. After learning about these basic TensorFlow concepts, actually you can already abandon TensorChain, go and try implementing your own neural networks yourself!<br></p><br><p lang=\"zh-cn\"><br>TensorChainTensorFlowTensorChain<br></p>\n\n<h2 id=\"4-Spices\"><a href=\"#4-Spices\" class=\"headerlink\" title=\"4 Spices\"></a>4 Spices</h2><p lang=\"en-us\"><br>I&#39;m not joking just now! But I know that there are a lot of things that you still don&#39;t understand about using TensorFlow - like &quot;how do I visualize my computation graph&quot;, &quot;how do I save/load my model to/from files&quot;, &quot;how do I record some tensors&#39; values while training&quot; or &quot;how do I view the loss curves&quot; - after all TensorFlow APIs are far more complicated than just building those nets. Those are also important techniques in your research. If you&#39;d rather ask me than spending some time experimenting, please go on with reading.<br></p><br><p lang=\"zh-cn\"><br>TensorFlow/TensorTensorFlowAPITensorFlow<br></p>\n\n<h3 id=\"4-1-Saving-and-loading-your-model\"><a href=\"#4-1-Saving-and-loading-your-model\" class=\"headerlink\" title=\"4.1 Saving and loading your model\"></a>4.1 Saving and loading your model</h3><p lang=\"en-us\"><br>The very first thing that you may want to do - after training a network model with nice outcomes - would be saving it. Saving a model is fairly easy - just use a <em>tf.train.Saver</em> object. See my code below:<br></p><br><p lang=\"zh-cn\"><br><em>tf.train.Saver</em><br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    <span class=\"comment\"># Train it for some iterations</span></span><br><span class=\"line\">    <span class=\"comment\"># Train it for some iterations</span></span><br><span class=\"line\">    <span class=\"comment\"># Train it for some iterations</span></span><br><span class=\"line\">    saver = tf.train.Saver()</span><br><span class=\"line\">    saver.save(session, <span class=\"string\">'models/model.ckpt'</span>)</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>I saved my model and variable values to &#39;models/model.ckpt&#39;. But actually, you&#39;ll find 3 files in the &#39;models&#39; directory - <em>model.ckpt.data-00000-of-00001</em>, <em>model.ckpt.meta</em> and <em>model.ckpt.index</em> - none of which is &#39;model.ckpt&#39;! That&#39;s because TensorFlow stores the graph structure separately from variables values. The <em>.meta</em> file describes the saved graph structure; the <em>.index</em> file records the mappings between tensor names and tensor metadata; and the <em>.data-00000-of-00001</em> file - which is always the biggest one - saves all the variable values. If you need the graph data together with the variable values to be loaded, use a Saver to load after creating a session:<br></p><br><p lang=\"zh-cn\"><br>&#39;models/model.ckpt&#39;models<em>model.ckpt.data-00000-of-00001</em><em>model.ckpt.meta</em><em>model.ckpt.index</em>model.ckptTensorFlow<em>.meta</em><em>.index</em>TensornameTensor<em>.data-00000-of-00001</em>TensorFlowsessionSaver<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    saver = tf.train.Saver()</span><br><span class=\"line\">    saver.restore(session, <span class=\"string\">'models/model.ckpt'</span>)</span><br><span class=\"line\">    <span class=\"comment\"># Then continue doing everything just like the model is just trained</span></span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>Remember that session.run(tf.global_variables_initializer()) shouldn&#39;t be executed, since variables are already initialized with your saved <em>.data-0000-of-00001</em> file.<br>If you only need the graph to be loaded, only use the <em>.meta</em> file:<br></p><br><p lang=\"zh-cn\"><br>session.run(tf.global_variables_initializer())checkpoint<br><em>.meta</em><br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    tf.train.import_meta_graph(<span class=\"string\">'models/model.ckpt.meta'</span>)</span><br><span class=\"line\">    <span class=\"comment\"># Then continue doing everything just like the model is just built</span></span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>Function <em>tf.train.import_meta_graph()</em> loads(appends) the graph to your current computation graph. The values of tensors are still uninitialized so you&#39;ll have to execute session.run(tf.global_variables_initializer()) again. The tensors that you defined in the model can be retrieved by their names(property of the Tensor objects, instead of Python variable names). For example:<br></p><br><p lang=\"zh-cn\"><br><em>tf.train.import_meta_graph()</em>Tensorsession.run(tf.global_variables_initializer())<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    <span class=\"comment\"># Recover the model here</span></span><br><span class=\"line\"></span><br><span class=\"line\">    graph = tf.get_default_graph()</span><br><span class=\"line\">    image_tensor = graph.get_tensor_by_name(<span class=\"string\">'input_image:0'</span>)</span><br><span class=\"line\">    loss = graph.get_tensor_by_name(<span class=\"string\">'loss:0'</span>)</span><br><span class=\"line\">    train = graph.get_operation_by_name(<span class=\"string\">'train)</span></span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>To retrieve normal tensors, you&#39;ll have to append a <em>&#39;:0&#39;</em> to the name of the op. This means getting the associated tensor of the op. <em>train</em> is a little special - we only need the op, so the function is <em>get_operation_by_name()</em> so the &#39;:0&#39; is not necessary.<br></p><br><p lang=\"zh-cn\"><br>TensorTensorname<em>&#39;:0&#39;</em>Tensor<em>train</em>op<em>get_operation_by_name()</em>Tensor&#39;:0&#39;<br></p>\n\n<p lang=\"en-us\" align=\"center\"><br>[THIS SECTION IS UNDER CONSTRUCTION]<br></p><br><p lang=\"zh-cn\" align=\"center\"><br>[]<br></p>","site":{"data":{}},"excerpt":"<p><script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML\" async></script><br><br></p>\n<p lang=\"en-us\"><br>TensorFlow is a powerful open-source deep learning framework, supporting various languages including Python. However, its APIs are far too complicated for a beginner in deep learning(especially those who are new to Python). In order to ease the pain of having to understand the mess of various elements in TensorFlow computation graphs, I made this tutorial to help beginners take the first bite of the cake.<br><br>ResNets are one of the greatest works in the deep learning field. Although they look scary with extreme depths, it&#39;s not a hard job to implement one. Now let&#39;s build one of the simplest ResNets - ResNet-56, and train it on the CIFAR-10 dataset.<br><br></p><br>","more":"<br><p lang=\"zh-cn\"><br>TensorFlowPythonAPIPythonTensorFlow213<br>ResNetResNetResNetResNetResNet-56CIFAR-10<br></p><br><div align=\"center\" class=\"figure\"><br><img src=\"/images/tftutorial/oyo.gif\" alt=\"Let&#39;s Rock!\"><br></div><br><p lang=\"en-us\"><br>First let&#39;s take a look at ResNet-56. It&#39;s proposed by Kaiming He et al., and is designed to confirm the effect of residual networks. It has 56 weighted layers, deep but simple. The structure is shown in the figure below:<br></p><br><p lang=\"zh-cn\"><br>ResNet-56ResNet56<br></p><br><div align=\"center\" class=\"figure\"><br><img src=\"/images/tftutorial/resnet56.png\" alt=\"ResNet-56\" width=\"80%\"><br><br>Fig. 1 The structure of ResNet-56<br></div><br><br><br><p lang=\"en-us\"><br>Seems a little bit long? Don&#39;t worry, let&#39;s do this step by step.<br></p><br><p lang=\"zh-cn\"><br><br></p>\n\n<h2 id=\"1-Ingredients\"><a href=\"#1-Ingredients\" class=\"headerlink\" title=\"1 Ingredients\"></a>1 Ingredients</h2><p>Python 3.6</p>\n<p>TensorFlow 1.4.0</p>\n<p>Numpy 1.13.3</p>\n<p>OpenCV 3.2.0</p>\n<p><a href=\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\" target=\"_blank\" rel=\"noopener\">CIFAR-10 Dataset</a></p>\n<p lang=\"en-us\"><br>Also prepare some basic knowledge on Python programming, digital image processing and convolutional neural networks. If you are already capable of building, training and validating your own neural networks with TensorFlow, you don&#39;t have to read this post.<br></p><br><p lang=\"zh-cn\"><br>PythonTensorFlow<br></p>\n\n<h2 id=\"2-Recipe\"><a href=\"#2-Recipe\" class=\"headerlink\" title=\"2 Recipe\"></a>2 Recipe</h2><h3 id=\"2-0-Prepare-the-tools\"><a href=\"#2-0-Prepare-the-tools\" class=\"headerlink\" title=\"2.0 Prepare the tools\"></a>2.0 Prepare the tools</h3><p lang=\"en-us\"><br>Prepare(import) the tools for our project, including all that I mentioned above. Like this :P<br></p><br><p lang=\"zh-cn\"><br>(i)(m)(p)(o)(r)(t)<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> cv2</span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensor_chain <span class=\"keyword\">import</span> TensorChain</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>Wait... What&#39;s this? TensorChain? Another deep learning framework like TensorFlow?<br><br>Uh, nope. This is my own encapsulation of some TensorFlow APIs, for the sake of easing your pain. You&#39;ll only have to focus on &quot;what&#39;s what&quot; in the beginning. We&#39;ll look into my implementation of this encapsulation later, when you are clear how everything goes. Please <a href=\"/files/tensor_chain.py\">download this file</a> and put it where your code file is, and import it.<br></p><br><p lang=\"zh-cn\"><br>... TensorChain<br><br>...TensorFlow APITensorFlow<a href=\"/files/tensor_chain.py\"></a>import<br></p>\n\n<h3 id=\"2-1-Decide-the-input\"><a href=\"#2-1-Decide-the-input\" class=\"headerlink\" title=\"2.1 Decide the input\"></a>2.1 Decide the input</h3><p lang=\"en-us\"><br>Every neural network requires an input - you always have to identify the details of a question, before asking the computer to solve it. All of the variable, constant in TensorFlow are objects of type <em>tf.Tensors</em>. And the <em>tf.placeholder</em> of our input(s) is a special one. Images in CIFAR-10 dataset are RGB images(3 channels) of 32x32(really small), so our input should shaped like [32, 32, 3]. Also, we want to input a little <em>batch</em> of multiple images. Therefore, our input data should be an array of shape <em>[?, 32, 32, 3]</em>. Unknown dimension size can be marked as None, and it will be clear when we feed the model with the actual images. It&#39;s coded like this:<br></p><br><p lang=\"zh-cn\"><br>TensorFlow<em>tf.Tensor</em><em>tf.placeholder</em>CIFAR-1032x32RGBRGB[32, 32, 3]<em>batch</em><em>[?, 32, 32, 3]</em>numpyNonebatch<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input_tensor = tf.placeholder(dtype=tf.float32, shape=[<span class=\"keyword\">None</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>])</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br><em>Ground truth</em> data also need to be known in supervised learning, so we also have to define a placeholder for the ground truth data:<br></p><br><p lang=\"zh-cn\"><br><em>ground truth</em>ground truthplaceholder<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ground_truth = tf.placeholder(dtype=tf.float32, shape=[<span class=\"keyword\">None</span>, <span class=\"number\">10</span>])</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>We want the label data to be in the one-hot encoding format, which means an array of length 10, denoting 10 classes. Only on one position is a &#39;1&#39;, and on other positions are &#39;0&#39;s.<br></p><br><p lang=\"zh-cn\"><br>One-Hot101010<br></p>\n\n<h3 id=\"2-2-Do-some-operations\"><a href=\"#2-2-Do-some-operations\" class=\"headerlink\" title=\"2.2 Do some operations\"></a>2.2 Do some operations</h3><p lang=\"en-us\"><br>For now, let&#39;s use our TensorChain to build it fast. Under most circumstances that we may face, the computations are based on the input data or the result of the former computation, so our network(or say, the most of it) look more like a chain than a web. Every time we add some new operation(layer), we add it to our <em>TensorChain</em> object. Just remember to get the <em>output_tensor</em> of this object(denoting the output tensor of the last operation on the chain) when you need to ue native TensorFlow API.<br>The construction function of TensorChain class requires a Tensor object as the parameter, which is also the input tensor of this chain. As we mentioned earlier, all we have to do is add operations. See my ResNet-56 code:<br></p><br><p lang=\"zh-cn\"><br>TensorChainTensorChainTensorFlow API<em>output_tensor</em>Tensor<br>TensorChainTensorResNet-56<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chain = TensorChain(input_tensor) \\</span><br><span class=\"line\">        .convolution_layer_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>, stride=<span class=\"number\">2</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>, stride=<span class=\"number\">2</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">        .flatten() \\</span><br><span class=\"line\">        .fully_connected_layer(<span class=\"number\">10</span>)</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>This is it? Right, this is it! Isn&#39;t it cool? Didn&#39;t seem that high, huh? That&#39;s because I encapsulated that huge mess of weights and biases, only leaving a few parameters that decide the structure of the network. Later in this pose we&#39;ll talk about the actual work that these functions do.<br></p><br><p lang=\"zh-cn\"><br>56<br></p>\n\n<h3 id=\"2-3-Define-the-loss\"><a href=\"#2-3-Define-the-loss\" class=\"headerlink\" title=\"2.3 Define the loss\"></a>2.3 Define the loss</h3><p lang=\"en-us\"><br>In supervised learning, you always have to tell the learning target to the model. To tell the model how to optimize, you have to let it know how, how much, on which direction should it change its parameters. This is done by using a loss function. Therefore, we need to define a loss function for our ResNet-56 model(which we designed for this classification problem) so that it will learn and optimize.<br>A commonly used loss function in classification problems is cross entropy. It&#39;s defined below:<br></p><br><p lang=\"zh-cn\"><br><em>loss</em>ResNet-56<br><br></p><br>$$C=-\\frac{1}{n}\\sum_x{y\\ln a+(1-y)\\ln(1-a)}$$<br><p lang=\"en-us\"><br>in which \\(y\\) is the expected(or say correct) output and \\(a\\) is the actual output.<br>This seems a little bit complicated. But it&#39;s not a hard job to implement, since TensorFlow implemented it already! You can also try and implement it yourself within one line if you want. For now we use the pre-defined cross entropy loss function:<br></p><br><p lang=\"zh-cn\"><br>\\(y\\)\\(a\\)<br>...TensorFlow<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>and it returns a tf.Tensor that denotes an average of cross entropies(don&#39;t forget that this is a batch). As for the &#39;softmax&#39; before the &#39;cross_entropy&#39;, it&#39;s a function that project the data in an array to range 0~1, which allows us to do a comparison between our prediction and the ground truth(in one-hot code). The definition is simple too:\\<br></p><br><p lang=\"zh-cn\"><br>batchTensorcross_entropy<em>softmax</em>0~1exp()\\<br></p><br>$$S_i=\\frac{e^{V_i}}{\\sum_j{e^{V_j}}}$$<br><p></p>\n\n<h3 id=\"2-4-Define-the-train-op\"><a href=\"#2-4-Define-the-train-op\" class=\"headerlink\" title=\"2.4 Define the train op\"></a>2.4 Define the train op</h3><p lang=\"en-us\"><br>Now we have the loss function. We&#39;ll have to tell its value to an <em>optimizer</em>, which make our model learn and optimize in order to minimize the loss value. Gradient Descent Optimizer, Adagrad Optimizer, Adam Optimizer and Momentum Optimizer are commonly used optimizers. Here we use an Adam Optimizer for instance. You&#39;re free to try any other one here. When<br></p><br><p lang=\"zh-cn\"><br><em>optimizer</em>Gradient Descent OptimizerAdagrad OptimizerAdam OptimizerMomentum Optimizer\\(10^-3\\)<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">optimizer = tf.train.AdamOptimizer(learning_rate=<span class=\"number\">0.001</span>)</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>Also, tell the optimizer that what the loss tensor is. The returned object is a train operation.<br></p><br><p lang=\"zh-cn\"><br>Tensor<em>train op</em><br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">train = optimizer.minimize(loss)</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>The neural network is finished. It&#39;s time to grab some data and train it.<br></p><br><p lang=\"zh-cn\"><br><br></p>\n\n<h3 id=\"2-5-Feed-the-model-with-data-and-train-it\"><a href=\"#2-5-Feed-the-model-with-data-and-train-it\" class=\"headerlink\" title=\"2.5 Feed the model with data, and train it!\"></a>2.5 Feed the model with data, and train it!</h3><p lang=\"en-us\"><br>Remember how we defined the placeholders? It&#39;s time to fetch some data that fits the placeholders and train it. See how CIFAR-10 dataset can be fetched on its <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\" target=\"_blank\" rel=\"noopener\">website</a>.<br></p><br><p lang=\"zh-cn\"><br>placeholderCIFAR-10<a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\" target=\"_blank\" rel=\"noopener\"></a><br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">unpickle</span><span class=\"params\">(file)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">import</span> pickle</span><br><span class=\"line\">    <span class=\"keyword\">with</span> open(file, <span class=\"string\">'rb'</span>) <span class=\"keyword\">as</span> fo:</span><br><span class=\"line\">        dict = pickle.load(fo, encoding=<span class=\"string\">'bytes'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dict</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>The returned value <em>dict</em> is a Python dictionary. Every time we unpickle a file, a dictionary would be returned. Its &#39;data&#39; key leads to 10000 RGB images of size 32x32, which is stored in a [10000, 3072] array(3072=32<em>32</em>3, I guess you know how it&#39;s stored now). The &#39;label&#39; key leads to 10000 values in range 0~9. Obviously we have to reshape the data so as to fit it into the network model:<br></p><br><p lang=\"zh-cn\"><br><em>dict</em>Pythondictbatchdata_batch_1data1000032x32RGB[10000, 3072]3072=10000x32x32x3label100000-9<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch = unpickle(DATA_PATH + <span class=\"string\">'data_batch_&#123;&#125;'</span>.format(i))  <span class=\"comment\"># 'i' is the loop variable</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Read the image data</span></span><br><span class=\"line\">image_data = np.reshape(batch[<span class=\"string\">b'data'</span>], (<span class=\"number\">10000</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>), <span class=\"string\">'F'</span>).astype(np.float32)   </span><br><span class=\"line\">image_data = image_data / <span class=\"number\">255</span>                            <span class=\"comment\"># Cast range(0, 255) to range(0, 1)</span></span><br><span class=\"line\">image_data = np.transpose(image_data, (<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">3</span>))      <span class=\"comment\"># Exchange row and column</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Read the label data and convert into one-hot code</span></span><br><span class=\"line\">label_data = batch[<span class=\"string\">b'labels'</span>]</span><br><span class=\"line\">new_label_data = np.zeros((<span class=\"number\">10000</span>, <span class=\"number\">10</span>))                   </span><br><span class=\"line\"><span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">10000</span>):</span><br><span class=\"line\">    new_label_data[j][label_data[j]] = <span class=\"number\">1</span></span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>The details for data processing are not covered here. Try doing step-by-step to see the results.<br>The <em>image_data</em> and <em>new_label_data</em> are contain 10000 pieces of data each. Let&#39;s divide them into 100 small batches(100 elements each, including image and label) and feed it into the model. Do this on all the 5 batch files:<br></p><br><p lang=\"zh-cn\"><br><br><em>image_data</em><em>new_label_data</em>10000batch100100+5batch<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    session.run(tf.global_variables_initializer())</span><br><span class=\"line\">    <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">100</span>): <span class=\"comment\"># 10000 / BATCH_SIZE</span></span><br><span class=\"line\">        <span class=\"comment\"># Divide them and get one part</span></span><br><span class=\"line\">        image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class=\"line\">        label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># Feed the model</span></span><br><span class=\"line\">        session.run(train, feed_dict=&#123;</span><br><span class=\"line\">            input_tensor: image_batch,</span><br><span class=\"line\">            ground_truth: label_batch</span><br><span class=\"line\">        &#125;)</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>A <em>session</em> - created with <em>tf.Session()</em> - is required every time we run a TensorFlow model, no matter when we&#39;re training it or evaluating it. The first time you run a model, you&#39;ll need to run <em>session.run(tf.global_variables_initializer())</em> to initialize the values of the TensorFlow variables defined previously.<br>When running <em>session.run()</em>, you must first decide a TensorFlow operation(or a list of operations) that you need. If its result is dependent on some actual data(which means that some data in one or more placeholders flow to this operation), it&#39;s also required that you feed it the actual data by adding a <em>feed_dict</em> parameter. For example, I&#39;m training this ResNet-56 model, in which a loss will be calculated with my <em>ground_truth</em> and the prediction result that comes from the <em>input_tensor</em>. Therefore, I&#39;ll have to give a value for each placeholder given above(format: &quot;placeholder name: corresponding data&quot;), and fold them in one Python dictionary.<br></p><br><p lang=\"zh-cn\"><br>TensorFlowtf.Session()<em>session</em><em>session.run(tf.global_variables_initializer())</em>TensorFlow<br><em>session.run()</em>operationplaceholderoperation<em>feed_dict</em><em>ground_truth</em><em>input_tensor</em>Pythonfeed_dict<br></p><br><p lang=\"en-us\"><br>I&#39;m also interested in the loss function value in each iteration(which means feeding a batch of data and executing one forward-propagation and one back-propagation) in the training process. Therefore, what I&#39;ll fill in the parameter is not just the train op, but also the loss tensor. And the session.run() above should be modified to:<br></p><br><p lang=\"zh-cn\"><br>batchsession.run()trainlosssession.run()<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[train_, loss_value] = session.run([train, loss],</span><br><span class=\"line\">    feed_dict=&#123;</span><br><span class=\"line\">        input_tensor: image_batch,</span><br><span class=\"line\">        ground_truth: label_batch</span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">print(<span class=\"string\">\"Loss = &#123;&#125;\"</span>.format(loss_value)</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>This is when the return value of session.run() becomes useful. Its value(s) - corresponding to the first parameter of run() - is/are the actual value(s) of the tensor(s) in the first parameter. In our example, <em>loss_value</em> is the actual output of the loss tensor. As for train_, we don&#39;t care what it is. Just add it to match the dimensions.<br></p><br><p lang=\"zh-cn\"><br>session.run()operation<em>loss_value</em>losstraintrain_None<br></p><br><p lang=\"en-us\"><br>Actually, one epoch(train the model once with the whole dataset) is not enough for the model to fully optimize. I trained this model for 40 epochs and added some loop variables to display the result. You can see my code and my output below. It&#39;s highly recommended that you train this with a high-performance GPU, or it would be a century before you train your model to a satisfactory degree.<br></p><br><p lang=\"zh-cn\"><br>epoch40epochGPUGPU<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tensorflow <span class=\"keyword\">as</span> tf</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pickle</span><br><span class=\"line\"><span class=\"keyword\">from</span> tensor_chain <span class=\"keyword\">import</span> TensorChain</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">unpickle</span><span class=\"params\">(file)</span>:</span></span><br><span class=\"line\">    <span class=\"keyword\">with</span> open(file, <span class=\"string\">'rb'</span>) <span class=\"keyword\">as</span> fo:</span><br><span class=\"line\">        dict = pickle.load(fo, encoding=<span class=\"string\">'bytes'</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dict</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">'__main__'</span>:</span><br><span class=\"line\">    input_tensor = tf.placeholder(dtype=tf.float32, shape=[<span class=\"keyword\">None</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>])</span><br><span class=\"line\">    ground_truth = tf.placeholder(dtype=tf.float32, shape=[<span class=\"keyword\">None</span>, <span class=\"number\">10</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">    chain = TensorChain(input_tensor) \\</span><br><span class=\"line\">            .convolution_layer_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">16</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>, stride=<span class=\"number\">2</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">32</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>, stride=<span class=\"number\">2</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .residual_block_2d(<span class=\"number\">3</span>, <span class=\"number\">64</span>) \\</span><br><span class=\"line\">            .flatten() \\</span><br><span class=\"line\">            .fully_connected_layer(<span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    prediction = chain.output_tensor</span><br><span class=\"line\">    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))</span><br><span class=\"line\"></span><br><span class=\"line\">    optimizer = tf.train.AdamOptimizer(learning_rate=<span class=\"number\">0.001</span>)</span><br><span class=\"line\">    train = optimizer.minimize(loss)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">        session.run(tf.global_variables_initializer())</span><br><span class=\"line\">        iteration = <span class=\"number\">1</span></span><br><span class=\"line\">        BATCH_SIZE = <span class=\"number\">100</span></span><br><span class=\"line\">        DATA_PATH = <span class=\"string\">'../data/cifar-10-batches-py/'</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, <span class=\"number\">41</span>):</span><br><span class=\"line\">            <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>, <span class=\"number\">6</span>):</span><br><span class=\"line\">                data = unpickle(DATA_PATH + <span class=\"string\">'data_batch_&#123;&#125;'</span>.format(i))</span><br><span class=\"line\">                image_data = np.reshape(data[<span class=\"string\">b'data'</span>], (<span class=\"number\">10000</span>, <span class=\"number\">32</span>, <span class=\"number\">32</span>, <span class=\"number\">3</span>), <span class=\"string\">'F'</span>).astype(np.float32)</span><br><span class=\"line\">                image_data = image_data / <span class=\"number\">255</span></span><br><span class=\"line\">                image_data = np.transpose(image_data, (<span class=\"number\">0</span>, <span class=\"number\">2</span>, <span class=\"number\">1</span>, <span class=\"number\">3</span>))</span><br><span class=\"line\">                label_data = data[<span class=\"string\">b'labels'</span>]</span><br><span class=\"line\">                new_label_data = np.zeros((<span class=\"number\">10000</span>, <span class=\"number\">10</span>))</span><br><span class=\"line\">                <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">10000</span>):</span><br><span class=\"line\">                    new_label_data[j][label_data[j]] = <span class=\"number\">1</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(int(<span class=\"number\">10000</span> / BATCH_SIZE)):</span><br><span class=\"line\">                    image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class=\"line\">                    label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class=\"line\">                    [train_, loss_] = session.run(</span><br><span class=\"line\">                        [train, loss],</span><br><span class=\"line\">                        feed_dict=&#123;</span><br><span class=\"line\">                            input_tensor: image_batch,</span><br><span class=\"line\">                            ground_truth: label_batch</span><br><span class=\"line\">                        &#125;)</span><br><span class=\"line\">                    iteration += <span class=\"number\">1</span></span><br><span class=\"line\">                    print(<span class=\"string\">\"Epoch &#123;&#125;, Iteration &#123;&#125;, Loss = &#123;&#125;\"</span>.format(epoch, iteration, loss_))</span><br></pre></td></tr></table></figure><br><br><div align=\"center\" class=\"figure\"><br><img src=\"/images/tftutorial/train.png\" alt=\"Training result\" width=\"40%\"><br><br>Fig. 2 Training result: cross entropy has dropped below 0.5<br></div><br><p></p>\n\n<h3 id=\"2-6-Conclusion\"><a href=\"#2-6-Conclusion\" class=\"headerlink\" title=\"2.6 Conclusion\"></a>2.6 Conclusion</h3><p lang=\"en-us\"><br>In a word, building &amp; training neural network models with TensorFlow involves the following steps:<br><br>1. Decide the <em>input tensor</em><br><br>2. Add operations(<em>op</em>s) based on existing tensors<br><br>3. Define the <em>loss</em> tensor, just like other tensors<br><br>4. Select an <em>optimizer</em> and define the <em>train</em> op<br><br>5. Process <em>data</em> and feed the model with them<br></p><br><p lang=\"zh-cn\"><br>TensorFlow<br><br>1. <em></em>Tensor<br><br>2. Tensor<em>op</em><br><br>3. <em></em>Tensor<br><br>4. <em></em><em></em><br><br>5. <em></em>shape<br></p>\n\n<h2 id=\"3-A-Closer-Look\"><a href=\"#3-A-Closer-Look\" class=\"headerlink\" title=\"3 A Closer Look\"></a>3 A Closer Look</h2><p lang=\"en-us\"><br>Wait, it&#39;s too late to leave now!<br>TensorChain saved you from having to deal with a mess of TensorFlow classes and functions. Now it&#39;s time that we take a closer look at how TensorChain is implemented, thus understanding the native TensorFlow APIs.<br></p><br><p lang=\"zh-cn\"><br><br>TensorChainTensorFlowTensorChainTensorFlowAPI<br></p>\n\n<h3 id=\"3-1-TensorFlow-variables\"><a href=\"#3-1-TensorFlow-variables\" class=\"headerlink\" title=\"3.1 TensorFlow variables\"></a>3.1 TensorFlow variables</h3><p lang=\"en-us\"><br>Let&#39;s begin with TensorFlow variables. Variables in TensorFlow are similar to variables in C, Java or any other strong typed programming languages - they have a type, though not necessarily explicitly decided upon definition. Usually them will change as the training process goes on, getting close to a best value.<br>The most commonly used variables in TensorFlow are weights and biases. I guess that you have seen formulae like:<br></p><br><p lang=\"zh-cn\"><br>TensorFlowTensorFlowCJava<br>TensorFlowweightsbiases<br></p><br>$$y=Wx+b$$<br><p lang=\"en-us\"><br>The \\(W\\) here is the weight, and the \\(b\\) here is the bias. When implementing some common network layers, they two are always used as the parameters in the layers. For instance, at the very beginning of our ResNet-56, we had a 3x3 sized convolution layer with 16 channels. Its implementation in TensorChain is:<br></p><br><p lang=\"zh-cn\"><br>\\(W\\)\\(b\\)ResNet-563x316TensorChain<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">convolution_layer_2d</span><span class=\"params\">(self, filter_size: int, num_channels: int, stride: int = <span class=\"number\">1</span>, name: str = None,</span></span></span><br><span class=\"line\"><span class=\"function\"><span class=\"params\">                         disable_log: bool = False)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">\"\"\"</span></span><br><span class=\"line\"><span class=\"string\">    Add a 2D convolution layer</span></span><br><span class=\"line\"><span class=\"string\">    :param filter_size: Filter size(width and height) for this operation</span></span><br><span class=\"line\"><span class=\"string\">    :param num_channels: Channel number of this filter</span></span><br><span class=\"line\"><span class=\"string\">    :param stride: Stride for this convolution operation</span></span><br><span class=\"line\"><span class=\"string\">    :param name: The name of the tensor</span></span><br><span class=\"line\"><span class=\"string\">    :param disable_log: Set it True if you don't want this layer to be recorded</span></span><br><span class=\"line\"><span class=\"string\">    :return: This object itself</span></span><br><span class=\"line\"><span class=\"string\">    \"\"\"</span></span><br><span class=\"line\">    filter = self._weights([filter_size, filter_size, self.num_channels, num_channels], layer_name=name,</span><br><span class=\"line\">                           suffix=<span class=\"string\">'filter'</span>)</span><br><span class=\"line\">    bias = self._bias([num_channels], layer_name=name)</span><br><span class=\"line\">    self.num_channels = num_channels</span><br><span class=\"line\">    self.output_tensor = tf.nn.conv2d(self.output_tensor, filter,</span><br><span class=\"line\">                                      [<span class=\"number\">1</span>, stride, stride, <span class=\"number\">1</span>], <span class=\"string\">'SAME'</span>, name=name)</span><br><span class=\"line\">    self.output_tensor = tf.add(self.output_tensor, bias)</span><br><span class=\"line\">    self._log_layer(</span><br><span class=\"line\">        <span class=\"string\">'2D Convolution layer, filter size = &#123;&#125;x&#123;&#125;, stride = &#123;&#125;, &#123;&#125; channels'</span>.format(filter_size, filter_size,</span><br><span class=\"line\">                                                                                     stride,</span><br><span class=\"line\">                                                                                     num_channels),</span><br><span class=\"line\">        disable=disable_log)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> self</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>See? On line 16, we used a <em>tf.nn.conv2d()</em> function, the parameters of which are <em>input</em>, <em>filter</em>, <em>strides</em>, <em>padding</em>, etc. As can be guessed from the names, this function does a convolution operation with out input and the weights(the convolution <em>filter</em> here). A <em>bias</em> is added to the result as the final output. There are also many people who argue that the bias here is meaningless and should removed. One line of code is sufficient for defining a variable:<br></p><br><p lang=\"zh-cn\"><br>16<em>tf.nn.conv2d()</em><em>input</em><em>filter</em><em>strides</em><em>padding</em>Tensor<em>filter</em>inputTensor<em>bias</em>biasbias<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tf.Variable(tf.truncated_normal(shape, stddev=sigma), dtype=tf.float32, name=suffix)</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>To define weight or bias variables, create a <em>tf.Variable</em> object. Usually you&#39;ll need to give the <em>initial_value</em> which also decides the shape of this tensor. <em>tf.truncated_normal()</em> and <em>tf.constant()</em> are usually used as the initial values. Also, other APIs - function <em>tf.get_variable()</em> and package <em>tf.initializers</em> are frequently used when using some more methods for initialization. I strongly recommend that you try using these APIs yourself.<br></p><br><p lang=\"zh-cn\"><br><em>tf.Variable</em><em>initial_value</em>TFshapeshapeAPI<em>tf.get_variable()</em><em>tf.initializers</em>API<br></p>\n\n<h3 id=\"3-2-Tensors-and-operations\"><a href=\"#3-2-Tensors-and-operations\" class=\"headerlink\" title=\"3.2 Tensors and operations\"></a>3.2 Tensors and operations</h3><p lang=\"en-us\"><br>Going on with the parameters of the <em>tf.nn.conv2d()</em> function. The required parameters also include <em>strides</em> and <em>padding</em>. You should have already learned about what strides mean in convolution, and I&#39;ll only talk about their formats. <em>strides</em> require a 1-D vector with a length of 4, like [1, 2, 2, 1]. The 1st and the 4th number is always 1(in order to match dimensions with the input), while the 2nd and the 3rd means the vertical stride and the horizonal stride.<br>The 4th parameter <em>padding</em> is a little bit different from its definition in convolution operation. It requires &#39;SAME&#39; of &#39;VALID&#39;, denoting &#39;with&#39; or &#39;without&#39; zero paddings. When it&#39;s &#39;SAME&#39;, zero padding is introduced to make the shapes match as needed, equally on every side of the input map.<br></p><br><p lang=\"zh-cn\"><br><em>tf.nn.conv2d()</em><em>strides</em><em>padding</em>stride<em>strides</em>141API<br><em>padding</em>padding&#39;SAME&#39;&#39;VALID&#39;&#39;SAME&#39;stride&gt;1/stride<br></p><br><p lang=\"en-us\"><br>tf.nn.conv2d() is just an example of TensorFlow <em>operations</em>. Other functions like <em>tf.matmul()</em>, <em>tf.reduce_mean()</em>, <em>tf.global_variables_initializer()</em>, <em>tf.losses.softmax_cross_entropy()</em>, <em>tf.truncated_normal()</em> are all operations. Operation functions return tensors(<em>tf.truncated_normal</em> also return a tensor, a tensor with initializers).<br></p><br><p lang=\"zh-cn\"><br>tf.nn.conv2d()TensorFlow<em>operation</em><em>tf.matmul()</em><em>tf.reduce_mean()</em><em>tf.nn.relu()</em><em>tf.batch_normalization()</em><em>tf.global_variables_initializer()</em><em>tf.losses.softmax_cross_entropy()</em><em>tf.truncated_normal()</em>TensorFlowTensorFlowTensor<em>tf.truncated_normal()</em>Tensor<br></p><br><p lang=\"en-us\"><br>All the functions in the TensorChain class are based on the most basic TensorFlow operations and variables. After learning about these basic TensorFlow concepts, actually you can already abandon TensorChain, go and try implementing your own neural networks yourself!<br></p><br><p lang=\"zh-cn\"><br>TensorChainTensorFlowTensorChain<br></p>\n\n<h2 id=\"4-Spices\"><a href=\"#4-Spices\" class=\"headerlink\" title=\"4 Spices\"></a>4 Spices</h2><p lang=\"en-us\"><br>I&#39;m not joking just now! But I know that there are a lot of things that you still don&#39;t understand about using TensorFlow - like &quot;how do I visualize my computation graph&quot;, &quot;how do I save/load my model to/from files&quot;, &quot;how do I record some tensors&#39; values while training&quot; or &quot;how do I view the loss curves&quot; - after all TensorFlow APIs are far more complicated than just building those nets. Those are also important techniques in your research. If you&#39;d rather ask me than spending some time experimenting, please go on with reading.<br></p><br><p lang=\"zh-cn\"><br>TensorFlow/TensorTensorFlowAPITensorFlow<br></p>\n\n<h3 id=\"4-1-Saving-and-loading-your-model\"><a href=\"#4-1-Saving-and-loading-your-model\" class=\"headerlink\" title=\"4.1 Saving and loading your model\"></a>4.1 Saving and loading your model</h3><p lang=\"en-us\"><br>The very first thing that you may want to do - after training a network model with nice outcomes - would be saving it. Saving a model is fairly easy - just use a <em>tf.train.Saver</em> object. See my code below:<br></p><br><p lang=\"zh-cn\"><br><em>tf.train.Saver</em><br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    <span class=\"comment\"># Train it for some iterations</span></span><br><span class=\"line\">    <span class=\"comment\"># Train it for some iterations</span></span><br><span class=\"line\">    <span class=\"comment\"># Train it for some iterations</span></span><br><span class=\"line\">    saver = tf.train.Saver()</span><br><span class=\"line\">    saver.save(session, <span class=\"string\">'models/model.ckpt'</span>)</span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>I saved my model and variable values to &#39;models/model.ckpt&#39;. But actually, you&#39;ll find 3 files in the &#39;models&#39; directory - <em>model.ckpt.data-00000-of-00001</em>, <em>model.ckpt.meta</em> and <em>model.ckpt.index</em> - none of which is &#39;model.ckpt&#39;! That&#39;s because TensorFlow stores the graph structure separately from variables values. The <em>.meta</em> file describes the saved graph structure; the <em>.index</em> file records the mappings between tensor names and tensor metadata; and the <em>.data-00000-of-00001</em> file - which is always the biggest one - saves all the variable values. If you need the graph data together with the variable values to be loaded, use a Saver to load after creating a session:<br></p><br><p lang=\"zh-cn\"><br>&#39;models/model.ckpt&#39;models<em>model.ckpt.data-00000-of-00001</em><em>model.ckpt.meta</em><em>model.ckpt.index</em>model.ckptTensorFlow<em>.meta</em><em>.index</em>TensornameTensor<em>.data-00000-of-00001</em>TensorFlowsessionSaver<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    saver = tf.train.Saver()</span><br><span class=\"line\">    saver.restore(session, <span class=\"string\">'models/model.ckpt'</span>)</span><br><span class=\"line\">    <span class=\"comment\"># Then continue doing everything just like the model is just trained</span></span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>Remember that session.run(tf.global_variables_initializer()) shouldn&#39;t be executed, since variables are already initialized with your saved <em>.data-0000-of-00001</em> file.<br>If you only need the graph to be loaded, only use the <em>.meta</em> file:<br></p><br><p lang=\"zh-cn\"><br>session.run(tf.global_variables_initializer())checkpoint<br><em>.meta</em><br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    tf.train.import_meta_graph(<span class=\"string\">'models/model.ckpt.meta'</span>)</span><br><span class=\"line\">    <span class=\"comment\"># Then continue doing everything just like the model is just built</span></span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>Function <em>tf.train.import_meta_graph()</em> loads(appends) the graph to your current computation graph. The values of tensors are still uninitialized so you&#39;ll have to execute session.run(tf.global_variables_initializer()) again. The tensors that you defined in the model can be retrieved by their names(property of the Tensor objects, instead of Python variable names). For example:<br></p><br><p lang=\"zh-cn\"><br><em>tf.train.import_meta_graph()</em>Tensorsession.run(tf.global_variables_initializer())<br></p><br><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> tf.Session() <span class=\"keyword\">as</span> session:</span><br><span class=\"line\">    <span class=\"comment\"># Recover the model here</span></span><br><span class=\"line\"></span><br><span class=\"line\">    graph = tf.get_default_graph()</span><br><span class=\"line\">    image_tensor = graph.get_tensor_by_name(<span class=\"string\">'input_image:0'</span>)</span><br><span class=\"line\">    loss = graph.get_tensor_by_name(<span class=\"string\">'loss:0'</span>)</span><br><span class=\"line\">    train = graph.get_operation_by_name(<span class=\"string\">'train)</span></span><br></pre></td></tr></table></figure><br><br><p lang=\"en-us\"><br>To retrieve normal tensors, you&#39;ll have to append a <em>&#39;:0&#39;</em> to the name of the op. This means getting the associated tensor of the op. <em>train</em> is a little special - we only need the op, so the function is <em>get_operation_by_name()</em> so the &#39;:0&#39; is not necessary.<br></p><br><p lang=\"zh-cn\"><br>TensorTensorname<em>&#39;:0&#39;</em>Tensor<em>train</em>op<em>get_operation_by_name()</em>Tensor&#39;:0&#39;<br></p>\n\n<p lang=\"en-us\" align=\"center\"><br>[THIS SECTION IS UNDER CONSTRUCTION]<br></p><br><p lang=\"zh-cn\" align=\"center\"><br>[]<br></p>"}],"PostAsset":[],"PostCategory":[],"PostTag":[{"post_id":"cjomm0o0h0000wgwnvix643qg","tag_id":"cjomm0o0n0003wgwnw3mva6jq","_id":"cjomm0o0r0008wgwnk3avqzz2"},{"post_id":"cjomm0o0l0002wgwnjo7w83om","tag_id":"cjomm0o0r0007wgwnt3sv1gj8","_id":"cjomm0o0s000awgwn7btur74p"},{"post_id":"cjomm0o0o0004wgwngibl0t8l","tag_id":"cjomm0o0s0009wgwnp2onyebp","_id":"cjomm0o0t000cwgwncfmveiyf"},{"post_id":"cjomm0o0p0005wgwn7qj40jhu","tag_id":"cjomm0o0r0007wgwnt3sv1gj8","_id":"cjomm0o0u000ewgwnowvnycmn"},{"post_id":"cjomm0o0q0006wgwndyv0t8dm","tag_id":"cjomm0o0t000dwgwnttn2qcko","_id":"cjomm0o0u000gwgwnfpolbdty"},{"post_id":"cjomm0o0q0006wgwndyv0t8dm","tag_id":"cjomm0o0u000fwgwnwkamaboo","_id":"cjomm0o0u000hwgwn9a0l1nhz"},{"post_id":"cjomm0o16000lwgwn00b10ta4","tag_id":"cjomm0o0t000dwgwnttn2qcko","_id":"cjomm0o18000mwgwnd92uy835"},{"post_id":"cjomm0o16000lwgwn00b10ta4","tag_id":"cjomm0o0u000fwgwnwkamaboo","_id":"cjomm0o19000owgwnjftzzw3c"},{"post_id":"cjomm0o14000iwgwnlcjpthjj","tag_id":"cjomm0o0t000dwgwnttn2qcko","_id":"cjomm0o19000pwgwng8ejtoge"},{"post_id":"cjomm0o14000iwgwnlcjpthjj","tag_id":"cjomm0o16000kwgwn5p1j244w","_id":"cjomm0o19000qwgwno9t32lal"},{"post_id":"cjomm0o15000jwgwnh5f71q4o","tag_id":"cjomm0o0t000dwgwnttn2qcko","_id":"cjomm0o1a000rwgwn1rk3etce"},{"post_id":"cjomm0o15000jwgwnh5f71q4o","tag_id":"cjomm0o16000kwgwn5p1j244w","_id":"cjomm0o1a000swgwnptjroa1m"},{"post_id":"cjomm0o22000twgwn4e414y3j","tag_id":"cjomm0o0t000dwgwnttn2qcko","_id":"cjomm0o24000vwgwn5lllbrf4"},{"post_id":"cjomm0o22000twgwn4e414y3j","tag_id":"cjomm0o23000uwgwn56hzmyux","_id":"cjomm0o24000wwgwnvtvqml5i"}],"Tag":[{"name":"Travel Gallery","_id":"cjomm0o0n0003wgwnw3mva6jq"},{"name":"Blogging","_id":"cjomm0o0r0007wgwnt3sv1gj8"},{"name":"Other","_id":"cjomm0o0s0009wgwnp2onyebp"},{"name":"Deep Learning","_id":"cjomm0o0t000dwgwnttn2qcko"},{"name":"Computer Vision","_id":"cjomm0o0u000fwgwnwkamaboo"},{"name":"MineSweeping","_id":"cjomm0o16000kwgwn5p1j244w"},{"name":"DIY","_id":"cjomm0o23000uwgwn56hzmyux"}]}}