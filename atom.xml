<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Longing for sth New</title>
  
  <subtitle>Blog of linkinpark213</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://linkinpark213.com/"/>
  <updated>2019-06-23T11:25:12.806Z</updated>
  <id>http://linkinpark213.com/</id>
  
  <author>
    <name>Harper Long</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>A Summary of CVPR19 Visual Tracking Papers</title>
    <link href="http://linkinpark213.com/2019/06/11/cvpr19-track/"/>
    <id>http://linkinpark213.com/2019/06/11/cvpr19-track/</id>
    <published>2019-06-11T08:27:10.000Z</published>
    <updated>2019-06-23T11:25:12.806Z</updated>
    
    <content type="html"><![CDATA[<p>Here&#39;s my brief summary of all CVPR19 papers in the field of visual tracking. Abbreviations without parentheses are part of the paper title, and those with parentheses are added by me according to the paper.</p><a id="more"></a><h1 id="RGB-based"><a href="#RGB-based" class="headerlink" title="RGB-based"></a>RGB-based</h1><h2 id="Single-Object-Tracking"><a href="#Single-Object-Tracking" class="headerlink" title="Single-Object Tracking"></a>Single-Object Tracking</h2><p><strong>(UDT): Unsupervised Deep Tracking</strong><br><strong>Authors</strong>: Ning Wang, Yibing Song, Chao Ma, Wengang Zhou, Wei Liu, Houqiang Li<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1904.01828" target="_blank" rel="noopener">https://arxiv.org/abs/1904.01828</a><br><strong>Project Link</strong>: <a href="https://github.com/594422814/UDT" target="_blank" rel="noopener">https://github.com/594422814/UDT</a><br><strong>Summary</strong>: Train a robust siamese network on large-scale unlabeled videos in an unsupervised manner - forward-and-backward, i.e., the tracker can forward localize the target object in successive frames and backtrace to its initial position in the first frame.<br><strong>Highlights</strong>: Unsupervised learning</p><p><strong>(TADT): Target-Aware Deep Tracking</strong><br><strong>Authors</strong>: Xin Li, Chao Ma, Baoyuan Wu, Zhenyu He, Ming-Hsuan Yang<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1904.01772" target="_blank" rel="noopener">https://arxiv.org/abs/1904.01772</a><br><strong>Project Link</strong>: <a href="https://xinli-zn.github.io/TADT-project-page/" target="_blank" rel="noopener">https://xinli-zn.github.io/TADT-project-page/</a><br><strong>Summary</strong>: Targets of interest can be arbitrary object class with arbitrary forms, while pre-trained deep features are less effective in modeling these targets of arbitrary forms for distinguishing them from the background. TADT learns target-aware features, thus can better recognize the targets undergoing significant appearance variations than pre-trained deep features.<br><strong>Highlights</strong>: Target-aware features, better discrimination</p><p><strong>(SiamMask): Fast Online Object Tracking and Segmentation: A Unifying Approach</strong><br><strong>Authors</strong>: Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, Philip H.S. Torr<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1812.05050" target="_blank" rel="noopener">https://arxiv.org/abs/1812.05050</a><br><strong>Project Link</strong>: <a href="https://github.com/foolwood/SiamMask" target="_blank" rel="noopener">https://github.com/foolwood/SiamMask</a><br>Zhihu Link: <a href="https://zhuanlan.zhihu.com/p/58154634" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/58154634</a><br><strong>Summary</strong>: Perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach.<br><strong>Highlights</strong>: Mask prediction in tracking</p><p><strong>SiamRPN++: Evolution of Siamese Visual Tracking With Very Deep Networks</strong><br><strong>Authors</strong>: Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, Junjie Yan<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1812.11703" target="_blank" rel="noopener">https://arxiv.org/abs/1812.11703</a><br><strong>Project Link</strong>: <a href="http://bo-li.info/SiamRPN++/" target="_blank" rel="noopener">http://bo-li.info/SiamRPN++/</a><br><strong>Summary</strong>: SiamRPN++ breaks the translation invariance restriction through a simple yet effective spatial-aware sampling strategy. SiamRPN++ performs depth-wise and layer-wise aggregations, improving the accuracy but also reduces the model size. Current state-of-the-art in OTB2015, VOT2018, UAV123, LaSOT, and TrackingNet.<br><strong>Highlights</strong>: Deep backbones, state-of-the-art</p><p><strong>(CIR/SiamDW): Deeper and Wider Siamese Networks for Real-Time Visual Tracking</strong><br><strong>Authors</strong>: Zhipeng Zhang, Houwen Peng<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1901.01660" target="_blank" rel="noopener">https://arxiv.org/abs/1901.01660</a><br><strong>Project Link</strong>: <a href="https://github.com/researchmm/SiamDW" target="_blank" rel="noopener">https://github.com/researchmm/SiamDW</a><br><strong>Summary</strong>: SiamDW explores utilizing deeper and wider network backbones in another aspect - careful designs of residual units, considering receptive field, stride, output feature size - to eliminate the negative impact of padding in deep network backbones.<br><strong>Highlights</strong>: Cropping-Inside-Residual, eliminating the negative impact of padding</p><p><strong>(SiamC-RPN): Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking</strong><br><strong>Authors</strong>: Heng Fan, Haibin Ling<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1812.06148" target="_blank" rel="noopener">https://arxiv.org/abs/1812.06148</a><br><strong>Project Link</strong>: None<br><strong>Summary</strong>: Previously proposed one-stage Siamese-RPN trackers degenerate in presence of similar distractors and large scale variation. Advantages: 1) Each RPN in Siamese C-RPN is trained using outputs of the previous RPN, thus simulating hard negative sampling. 2) Feature transfer blocks (FTB) further improving the discriminability. 3) The location and shape of the target in each RPN is progressively refined, resulting in better localization.<br><strong>Highlights</strong>: Cascaded RPN, excellent accuracy</p><p><strong>SPM-Tracker: Series-Parallel Matching for Real-Time Visual Object Tracking</strong><br><strong>Authors</strong>: Guangting Wang, Chong Luo, Zhiwei Xiong, Wenjun Zeng<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1904.04452" target="_blank" rel="noopener">https://arxiv.org/abs/1904.04452</a><br><strong>Project Link</strong>: None<br><strong>Summary</strong>: To overcome the simultaneous requirements on robustness and discrimination power, SPM-Tracker tackle the challenge by connecting a coarse matching stage and a fine matching stage, taking advantage of both stages, resulting in superior performance, and exceeding other real-time trackers by a notable margin.<br><strong>Highlights</strong>: Coarse matching &amp; fine matching</p><p><strong>ATOM: Accurate Tracking by Overlap Maximization</strong><br><strong>Authors</strong>: Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, Michael Felsberg<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1811.07628" target="_blank" rel="noopener">https://arxiv.org/abs/1811.07628</a><br><strong>Project Link</strong>: <a href="https://github.com/visionml/pytracking" target="_blank" rel="noopener">https://github.com/visionml/pytracking</a><br><strong>Summary</strong>: Target estimation is a complex task, requiring highlevel knowledge about the object, while most trackers only resort to a simple multi-scale search. In comparison, ATOM estimate target states by predicting the overlap between the target object and an estimated bounding box. Besides, a classification component that is trained online to guarantee high discriminative power in the presence of distractors.<br><strong>Highlights</strong>: Overlap IoU prediction</p><p><strong>(GCT): Graph Convolutional Tracking</strong><br><strong>Authors</strong>: Junyu Gao, Tianzhu Zhang, Changsheng Xu<br><strong>arXiv Link</strong>: None<br><strong>PDF Link</strong>: <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Gao_Graph_Convolutional_Tracking_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Gao_Graph_Convolutional_Tracking_CVPR_2019_paper.pdf</a><br><strong>Project Link</strong>: <a href="http://nlpr-web.ia.ac.cn/mmc/homepage/jygao/gct_cvpr2019.html" target="_blank" rel="noopener">http://nlpr-web.ia.ac.cn/mmc/homepage/jygao/gct_cvpr2019.html</a><br><strong>Summary</strong>: Spatial-temporal information can provide diverse features to enhance the target representation. GCT incorporates 1) a spatial-temporal GCN to model the structured representation of historical target exemplars, and 2) a context GCN to utilize the context of the current frame to learn adaptive features for target localization.<br><strong>Highlights</strong>: Graph convolution networks, spatial-temporal information</p><p><strong>(ASRCF): Visual Tracking via Adaptive Spatially-Regularized Correlation Filters</strong><br><strong>Authors</strong>: Kenan Dai, Dong Wang, Huchuan Lu, Chong Sun, Jianhua Li<br><strong>arXiv Link</strong>: None<br><strong>Project Link</strong>: <a href="https://github.com/Daikenan/ASRCF" target="_blank" rel="noopener">https://github.com/Daikenan/ASRCF</a> (To be updated)<br><strong>Summary</strong>: ASRCF simultaneously optimize the filter coefficients and the spatial regularization weight. ASRCF applies two correlation filters (CFs) to estimate the location and scale respectively - 1) location CF model, which exploits ensembles of shallow and deep features to determine the optimal position accurately, and 2) scale CF model, which works on multi-scale shallow features to estimate the optimal scale efficiently.<br><strong>Highlights</strong>: Estimate location and scale respectively</p><p><strong>(RPCF): RoI Pooled Correlation Filters for Visual Tracking</strong><br><strong>Authors</strong>: Yuxuan Sun, Chong Sun, Dong Wang, You He, Huchuan Lu<br><strong>arXiv Link</strong>: None<br><strong>Project Link</strong>: None<br>PDF Link: <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_ROI_Pooled_Correlation_Filters_for_Visual_Tracking_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_ROI_Pooled_Correlation_Filters_for_Visual_Tracking_CVPR_2019_paper.pdf</a><br><strong>Summary</strong>: RoI-based pooling can be equivalently achieved by enforcing additional constraints on the learned filter weights and thus becomes feasible on the virtual circular samples. Considering RoI pooling in the correlation filter formula, the RPCF performs favourably against other state-of-the-art trackers.<br><strong>Highlights</strong>: RoI pooling in correlation filters</p><h2 id="Multi-Object-Tracking"><a href="#Multi-Object-Tracking" class="headerlink" title="Multi-Object Tracking"></a>Multi-Object Tracking</h2><p><strong>(TBA): Tracking by Animation: Unsupervised Learning of Multi-Object Attentive Trackers</strong><br><strong>Authors</strong>: Zhen He, Jian Li, Daxue Liu, Hangen He, David Barber<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1809.03137" target="_blank" rel="noopener">https://arxiv.org/abs/1809.03137</a><br><strong>Project Link</strong>: <a href="https://github.com/zhen-he/tracking-by-animation" target="_blank" rel="noopener">https://github.com/zhen-he/tracking-by-animation</a><br><strong>Summary</strong>: The common Tracking-by-Detection (TBD) paradigm use supervised learning and treat detection and tracking separately. Instead, TBA is a differentiable neural model that first tracks objects from input frames, animates these objects into reconstructed frames, and learns by the reconstruction error through backpropagation. Besides, a Reprioritized Attentive Tracking is proposed to improve the robustness of data association.<br><strong>Highlights</strong>: Label-free, end-to-end MOT learning</p><p><strong>Eliminating Exposure Bias and Metric Mismatch in Multiple Object Tracking</strong><br><strong>Authors</strong>: Andrii Maksai, Pascal Fua<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1811.10984" target="_blank" rel="noopener">https://arxiv.org/abs/1811.10984</a><br><strong>Project Link</strong>: None<br><strong>Summary</strong>: Many state-of-the-art MOT approaches now use sequence models to solve identity switches but their training can be affected by biases. An iterative scheme of building a rich training set is proposed and used to learn a scoring function that is an explicit proxy for the target tracking metric.<br><strong>Highlights</strong>: Eliminating loss-evaluation mismatch</p><h2 id="Pose-Tracking"><a href="#Pose-Tracking" class="headerlink" title="Pose Tracking"></a>Pose Tracking</h2><p><strong>Multi-Person Articulated Tracking With Spatial and Temporal Embeddings</strong><br><strong>Authors</strong>: Sheng Jin, Wentao Liu, Wanli Ouyang, Chen Qian<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1903.09214" target="_blank" rel="noopener">https://arxiv.org/abs/1903.09214</a><br><strong>Project Link</strong>: None<br><strong>Summary</strong>: The framework consists of a SpatialNet and a TemporalNet, predicting (body part detection heatmaps + Keypoint Embedding (KE) + Spatial Instance Embedding (SIE)) and (Human Embedding (HE) + Temporal Instance Embedding (TIE)). Besides, a differentiable Pose-Guided Grouping (PGG) module to make the whole part detection and grouping pipeline fully end-to-end trainable.<br><strong>Highlights</strong>: Spatial &amp; temporal embeddings, end-to-end learning &quot;detection and grouping&quot; pipeline</p><p><strong>(STAF): Efficient Online Multi-Person 2D Pose Tracking With Recurrent Spatio-Temporal Affinity Fields</strong><br><strong>Authors</strong>: Yaadhav Raaj, Haroon Idrees, Gines Hidalgo, Yaser Sheikh<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1811.11975" target="_blank" rel="noopener">https://arxiv.org/abs/1811.11975</a><br><strong>Project Link</strong>: None<br><strong>Summary</strong>: Upon Part Affinity Field (PAF) representation designed for static images, an architecture encoding ans predicting Spatio-Temporal Affinity Fields (STAF) across a video sequence is proposed - a novel temporal topology cross-linked across limbs which can consistently handle body motions of a wide range of magnitudes. The network ingests STAF heatmaps from previous frames and estimates those for the current frame.<br><strong>Highlights</strong>: Online, fastest and the most accurate bottom-up approach</p><h1 id="RGBD-based"><a href="#RGBD-based" class="headerlink" title="RGBD-based"></a>RGBD-based</h1><p><strong>(OTR)</strong>: Object Tracking by Reconstruction With View-Specific Discriminative Correlation Filters<br><strong>Authors</strong>: Ugur Kart, Alan Lukezic, Matej Kristan, Joni-Kristian Kamarainen, Jiri Matas<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1811.10863" target="_blank" rel="noopener">https://arxiv.org/abs/1811.10863</a><br><strong>Summary</strong>: Perform online 3D target reconstruction to facilitate robust learning of a set of view-specific discriminative correlation filters (DCFs). State-of-the-art on Princeton RGB-D tracking and STC Benchmarks.</p><h1 id="Pointcloud-based"><a href="#Pointcloud-based" class="headerlink" title="Pointcloud-based"></a>Pointcloud-based</h1><p>I&#39;m not experienced in point clouds so I couldn&#39;t make a summary for the following papers. The abstracts are given below. Check them out at arXiv to learn more if you&#39;re interested.</p><p><strong>VITAMIN-E</strong>: VIsual Tracking and MappINg With Extremely Dense Feature Points<br><strong>Authors</strong>: Masashi Yokozuka, Shuji Oishi, Simon Thompson, Atsuhiko Banno<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1904.10324" target="_blank" rel="noopener">https://arxiv.org/abs/1904.10324</a><br><strong>Project Link</strong>: None<br><strong>Abstract</strong>: In this paper, we propose a novel indirect monocular SLAM algorithm called &quot;VITAMIN-E,&quot; which is highly accurate and robust as a result of tracking extremely dense feature points. Typical indirect methods have difficulty in reconstructing dense geometry because of their careful feature point selection for accurate matching. Unlike conventional methods, the proposed method processes an enormous number of feature points by tracking the local extrema of curvature informed by dominant flow estimation. Because this may lead to high computational cost during bundle adjustment, we propose a novel optimization technique, the &quot;subspace Gauss--Newton method&quot;, that significantly improves the computational efficiency of bundle adjustment by partially updating the variables. We concurrently generate meshes from the reconstructed points and merge them for an entire 3D model. The experimental results on the SLAM benchmark dataset EuRoC demonstrated that the proposed method outperformed state-of-the-art SLAM methods, such as DSO, ORB-SLAM, and LSD-SLAM, both in terms of accuracy and robustness in trajectory estimation. The proposed method simultaneously generated significantly detailed 3D geometry from the dense feature points in real time using only a CPU.</p><p><strong>Leveraging Shape Completion for 3D Siamese Tracking</strong><br><strong>Authors</strong>: Silvio Giancola<em>, Jesus Zarzar</em>, and Bernard Ghanem<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1903.01784" target="_blank" rel="noopener">https://arxiv.org/abs/1903.01784</a><br><strong>Project Link</strong>: <a href="https://github.com/SilvioGiancola/ShapeCompletion3DTracking" target="_blank" rel="noopener">https://github.com/SilvioGiancola/ShapeCompletion3DTracking</a><br><strong>Abstract</strong>: Point clouds are challenging to process due to their sparsity, therefore autonomous vehicles rely more on appearance attributes than pure geometric features. However, 3D LIDAR perception can provide crucial information for urban navigation in challenging light or weather conditions. In this paper, we investigate the versatility of Shape Completion for 3D Object Tracking in LIDAR point clouds. We design a Siamese tracker that encodes model and candidate shapes into a compact latent representation. We regularize the encoding by enforcing the latent representation to decode into an object model shape. We observe that 3D object tracking and 3D shape completion complement each other. Learning a more meaningful latent representation shows better discriminatory capabilities, leading to improved tracking performance. We test our method on the KITTI Tracking set using car 3D bounding boxes. Our model reaches a 76.94% Success rate and 81.38% Precision for 3D Object Tracking, with the shape completion regularization leading to an improvement of 3% in both metrics.</p><h1 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h1><p><strong>LaSOT</strong>: A High-Quality Benchmark for Large-Scale Single Object Tracking<br><strong>Authors</strong>: Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, Haibin Ling<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1809.07845" target="_blank" rel="noopener">https://arxiv.org/abs/1809.07845</a><br><strong>Project Link</strong>: <a href="https://cis.temple.edu/lasot/" target="_blank" rel="noopener">https://cis.temple.edu/lasot/</a><br><strong>Summary</strong>: A high-quality benchmark for <strong>La</strong>rge-scale <strong>S</strong>ingle <strong>O</strong>bject <strong>T</strong>racking, consisting of 1,400 sequences with more than 3.5M frames.</p><p><strong>CityFlow</strong>: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification<br>Zheng Tang, Milind Naphade, Ming-Yu Liu, Xiaodong Yang, Stan Birchfield, Shuo Wang, Ratnesh Kumar, David Anastasiu, Jenq-Neng Hwang<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1903.09254" target="_blank" rel="noopener">https://arxiv.org/abs/1903.09254</a><br><strong>Project Link</strong>: <a href="https://www.aicitychallenge.org/" target="_blank" rel="noopener">https://www.aicitychallenge.org/</a><br><strong>Summary</strong>: The largest-scale dataset in terms of spatial coverage and the number of cameras/videos in an urban environment,  consisting of more than 3 hours of synchronized HD videos from 40 cameras across 10 intersections, with the longest distance between two simultaneous cameras being 2.5 km.</p><p><strong>MOTS</strong>: Multi-Object Tracking and Segmentation<br><strong>Authors</strong>: Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger, Bastian Leibe<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1902.03604" target="_blank" rel="noopener">https://arxiv.org/abs/1902.03604</a><br><strong>Project Link</strong>: <a href="https://www.vision.rwth-aachen.de/page/mots" target="_blank" rel="noopener">https://www.vision.rwth-aachen.de/page/mots</a><br><strong>Summary</strong>: Going beyond 2D bounding boxes and extending the popular task of multi-object tracking to multi-object tracking and segmentation, in tasks and metrics.<br><strong>Highlights</strong>: Extend MOT with segmentation</p><p><strong>Argoverse</strong>: 3D Tracking and Forecasting With Rich Maps<br>Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, James Hays<br><strong>arXiv Link</strong>: None<br><strong>PDF Link</strong>: <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.pdf</a><br><strong>Project Link</strong>: Argoverse.org (Not working?)<br><strong>Summary</strong>: A dataset designed to support autonomous vehicle perception tasks including 3D tracking and motion forecasting.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Here&amp;#39;s my brief summary of all CVPR19 papers in the field of visual tracking. Abbreviations without parentheses are part of the paper title, and those with parentheses are added by me according to the paper.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://linkinpark213.com/tags/Deep-Learning/"/>
    
      <category term="Visual Tracking" scheme="http://linkinpark213.com/tags/Visual-Tracking/"/>
    
  </entry>
  
  <entry>
    <title>[论文总结] 理解FishNet</title>
    <link href="http://linkinpark213.com/2019/04/21/fishnet/"/>
    <id>http://linkinpark213.com/2019/04/21/fishnet/</id>
    <published>2019-04-21T06:50:36.000Z</published>
    <updated>2019-06-23T11:26:14.762Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><p>从VALSE2019回来后，感觉自己俨然变成了欧阳万里老师的脑残粉呀╰( ᐖ╰)！会上欧阳老师介绍的FishNet简直让我眼前一亮，这么好的点子，我怎么就没想到呐！回来好好读了一下文章和代码，简单总结一下。</p><div align="center" class="figure"><br>  <img src="/images/fishnet/fish.jpg" width="15%" alt="我咸甚，此鱼何能及也"><br><br></div><a id="more"></a><h2 id="0-问题"><a href="#0-问题" class="headerlink" title="0 问题"></a>0 问题</h2><h3 id="0-1-各层特征的融合方式"><a href="#0-1-各层特征的融合方式" class="headerlink" title="0.1 各层特征的融合方式"></a>0.1 各层特征的融合方式</h3><p>较早的典型深度CNN结构大多为漏斗状，不断地进行卷积、下采样来提取、浓缩图像特征，最后用一些全连接层之类的结构来计算具体任务的输出结果。这样的设计很自然地被用于图像分类任务，因为较深的神经网络更能学习高级语义特征，最后将图像浓缩到一个像素而变成一个向量时，这个像素的每一个通道的值则代表了整个图像在这个语义特征上的表现。</p><div align="center" class="figure"><img src="/images/fishnet/vgg.png" width="40%" alt="VGG-16"><br>Fig. 1 漏斗状卷积神经网络，以VGG-16为例<br><br></div><p>但是呢，这样的结构原封不动地应用到其他任务上，效果就不是很好了。比如在分割任务中，细节特征保留得好的话，分割的效果则会更佳（例如FCN-8s的效果远好于FCN-32s）。又如在anchor-based目标检测模型中，用尺寸更大的特征图能够更好地回归较小目标的候选框（例如YOLOv3加入FPN后显著提升小物体的检测效果）。因此，出现了一些沙漏状甚至多沙漏堆叠的网络结构（U-Net，FPN，Stacked Hourglass等等）来更好地处理这些任务。</p><div align="center" class="figure"><img src="/images/fishnet/unet.png" width="40%" alt="U-Net"><br>    Fig. 2 沙漏状卷积神经网络，以U-Net为例<br><br></div><p>可以看到，类似这样的工作大多出于这样的一个想法：底层细节特征很重要，我们要把它融合到顶层语义特征里去。这样就有人问了：那语义特征是不是也能融合到细节特征里去，从而增强高分辨率特征图的效果呢？FishNet就做到了这样的融合，让网络最后一部分的各个分辨率的特征图中的底层、中层、顶层特征（作者原话为pixel-level, region-level, image-level）都能“你中有我，我中有你”。</p><h3 id="0-2-梯度反向传播的阻碍"><a href="#0-2-梯度反向传播的阻碍" class="headerlink" title="0.2 梯度反向传播的阻碍"></a>0.2 梯度反向传播的阻碍</h3><p>在ResNet中，作者用一种巧妙的办法让较浅的层也能得到有效的梯度信息——在每层层的输出上加一个identity mapping。也就是该层的输入\(x_l\)、下一层的输入\(x_{l+1}\)以及本层的运算\(\mathcal{F}(x, \mathcal{W}_l)\)之间的关系是$$x_{l+1}=x_l+\mathcal{F}(x_l, \mathcal{W_l})$$<br>再下一层的话：<br>$$x_{l+2}=x_l + \mathcal{F}(x_l, \mathcal{W_l}) + \mathcal{F}(x_{l+1}, \mathcal{W_{l+1}})$$<br>要是一直写到最后一层\(x_L\)：<br>$$x_{L}=x_l+\sum_{i=l}^{L-1}\mathcal{F}(x_i, \mathcal{W_i})$$<br>那么梯度反传时则有：<br>$$\begin{split}<br>\frac{\partial{\mathcal{E}}}{\partial{x_l}} &amp; = \frac{\partial{\mathcal{E}}}{\partial{x_L}}\frac{\partial{x_L}}{\partial{x_l}}\\<br>&amp; = \frac{\partial{\mathcal{E}}}{\partial{x_L}}\Big(1+\frac{\partial{}}{\partial{x_l}}\sum_{i=l}^{L-1}\mathcal{F}(x_i,\mathcal{W}_i)\Big)<br>\end{split}$$</p><p>然而现实是：因为中间涉及了几次下采样，采样后的特征图尺寸发生了变化，这时，那个恒等映射\(x\)上不得不加一个\(\mathcal{M}(x)\)（一般为一个\((1\times 1)\)尺寸的卷积，作者称之为I-conv，即Isolated convolution）来改变尺寸和通道数。因此，不是每一层都能保证简单的\(x_{l+1}=x_l+\mathcal{F}(x_l, \mathcal{W_l})\)，上边的梯度公式也只是一种理想情况而已。</p><div align="center" class="figure"><img src="/images/fishnet/bottleneck_alter.png" width="50%" alt="Bottlenecks in ResNet"><br>    Fig. 3 ResNet中，理想的Bottleneck模块与现实中某些Bottleneck模块<br><br></div><p>在ResNet本身里面倒还好。到了FPN甚至Stacked Hourglass中，这样的I-conv在每次特征图融合时都被使用，这就有点违背ResNet保持梯度有效反传的初衷了。而FishNet在这种情况下采用了一种更“平滑”的方式使得梯度反传受到的影响降到最低。</p><div align="center" class="figure"><img src="/images/fishnet/fish_block.png" width="36%" alt="Bottlenecks in FishNet"><br>    Fig. 4 FishNet中涉及采样的Bottleneck模块（除tail部分外）<br><br></div><h2 id="1-整体方案"><a href="#1-整体方案" class="headerlink" title="1 整体方案"></a>1 整体方案</h2><p>妙啊（👏）！那我们就来看一眼FishNet的全貌：</p><div align="center" class="figure"><img src="/images/fishnet/fishnet.png" width="70%" alt="FishNet"><br>    Fig. 5 FishNet<br><br></div><!--这才是真正的Fishnet！（斜眼<div align="center" class="figure"><img src="/images/fishnet/real_fishnet.png" width="40%" alt=""></div>--><p><s>整条鱼</s>整个FishNet由三部分构成：tail（尾巴），body（躯干）和head（头）。tail部分之前，图像先过了三层卷积层，初步从\((224\times 225 \times 3)\)尺寸的原图像提取出\((56\times 56 \times 64)\)尺寸的特征图。作者把不同阶段内同一分辨率的特征图分为同一个stage，\((56\times 56)\)的是stage 1，\((28\times 28)\)的是stage 2，\((14\times 14)\)的是stage 3，\((7\times 7)\)的是stage 4。因为分辨率相同，三个部分的特征图可以不用上/下采样而直接在channel维度上concat起来。</p><p>tail部分就是一个漏斗状的网络，涉及三次最大池化，每次池化前，最后一个卷积层输出的特征图被留下来供body部分使用。这一部分的结果就是经典的漏斗状网络，作者使用的是一个三阶段的ResNet。tail部分的最后，作者用了一个Squeeze-Excitation模块[2]，先把\((7\times 7 \times 512)\)尺寸的特征图用Global Average Pooling再加几个卷积层（实际上和全连接层并无本质区别）映射成一个\((1\times 1\times 512)\)的向量，再把这个向量的每一个值作为一个权重，乘到之前\((7\times 7\times 512)\)的特征图对应的通道上去。</p><p>body部分像FPN一样不断地用上采样来放大特征图，同时融合之前tail部分保留下来的同一分辨率的特征。</p><p>head部分则是FishNet的独创性工作，它像是body部分的反过程。以往的沙漏形网络将高层语义特征用来精化低层细节特征，而head网络反其道而行之，又用精化过的低层细节特征反过来精化高层特征。这样，再次采样得到的高层特征的质量被有效提高。</p><h2 id="2-实现细节"><a href="#2-实现细节" class="headerlink" title="2 实现细节"></a>2 实现细节</h2><h3 id="2-1-网络参数"><a href="#2-1-网络参数" class="headerlink" title="2.1 网络参数"></a>2.1 网络参数</h3><p>FishNet-99整体的各个部分的参数见下表。</p><table><thead><tr><th style="text-align:center">Part-Stage</th><th style="text-align:center">Input shape</th><th style="text-align:center">Output shape</th><th style="text-align:center">Bottlenecks</th><th style="text-align:center">I-convs</th><th style="text-align:center">Convs in total</th></tr></thead><tbody><tr><td style="text-align:center">Input</td><td style="text-align:center">\(3\times 224 \times 224\)</td><td style="text-align:center">\(64\times 56 \times 56\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(3\)</td></tr><tr><td style="text-align:center">Tail-1</td><td style="text-align:center">\(64\times 56 \times 56\)</td><td style="text-align:center">\(128\times 28 \times 28\)</td><td style="text-align:center">\(2\)</td><td style="text-align:center">\(1\)</td><td style="text-align:center">\(7\)</td></tr><tr><td style="text-align:center">Tail-2</td><td style="text-align:center">\(128\times 28 \times 28\)</td><td style="text-align:center">\(256\times 14 \times 14\)</td><td style="text-align:center">\(2\)</td><td style="text-align:center">\(1\)</td><td style="text-align:center">\(7\)</td></tr><tr><td style="text-align:center">Tail-3</td><td style="text-align:center">\(256\times 14 \times 14\)</td><td style="text-align:center">\(512\times 7 \times 7\)</td><td style="text-align:center">\(6\)</td><td style="text-align:center">\(1\)</td><td style="text-align:center">\(19\)</td></tr><tr><td style="text-align:center">SE-block</td><td style="text-align:center">\(512\times 7 \times 7\)</td><td style="text-align:center">\(512\times 7 \times 7\)</td><td style="text-align:center">\(2\)</td><td style="text-align:center">\(1\)</td><td style="text-align:center">\(11\)</td></tr><tr><td style="text-align:center">Body-3</td><td style="text-align:center">\(512\times 7 \times 7\)</td><td style="text-align:center">\(256\times 14 \times 14\)</td><td style="text-align:center">\(1 + 1\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(6\)</td></tr><tr><td style="text-align:center">Body-2</td><td style="text-align:center">\((512+256)\times 14 \times 14\)</td><td style="text-align:center">\(384\times 28 \times 28\)</td><td style="text-align:center">\(1 + 1\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(6\)</td></tr><tr><td style="text-align:center">Body-1</td><td style="text-align:center">\((384+128)\times 28 \times 28\)</td><td style="text-align:center">\(256\times 56 \times 56\)</td><td style="text-align:center">\(1 + 1\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(6\)</td></tr><tr><td style="text-align:center">Head-1</td><td style="text-align:center">\((256+64)\times 56 \times 56\)</td><td style="text-align:center">\(320\times 28 \times 28\)</td><td style="text-align:center">\(1 + 1\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(6\)</td></tr><tr><td style="text-align:center">Head-2</td><td style="text-align:center">\((320+512)\times 28 \times 28\)</td><td style="text-align:center">\(832\times 14 \times 14\)</td><td style="text-align:center">\(2 + 1\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(9\)</td></tr><tr><td style="text-align:center">Head-3</td><td style="text-align:center">\((832+768)\times 14 \times 14\)</td><td style="text-align:center">\(1600\times 7 \times 7\)</td><td style="text-align:center">\(2 + 4\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(18\)</td></tr><tr><td style="text-align:center">Score-Conv</td><td style="text-align:center">\((1600+512)\times 7 \times 7\)</td><td style="text-align:center">\(1056\times 7 \times 7\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(1\)</td></tr><tr><td style="text-align:center">Score-FC</td><td style="text-align:center">\(1056\times 7 \times 7\)</td><td style="text-align:center">\(1000\times 1 \times 1\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(1\)</td></tr></tbody></table><p>说明：</p><ul><li>第一列的Tail-1代表Tail部分的stage \(1\)。</li><li>Body-3至Head-3的Bottleneck模块数量包括两种：网络主干上的和特征图迁移模块上的。迁移模块用于将上一部分同一stage的特征图进行变换。</li></ul><p>FishNet-150的参数见下表，与FishNet-99相比而言只是各个部分Bottleneck块的数量不同，没有太大差异。</p><table><thead><tr><th style="text-align:center">Part-Stage</th><th style="text-align:center">Input shape</th><th style="text-align:center">Output shape</th><th style="text-align:center">Bottlenecks</th><th style="text-align:center">I-convs</th><th style="text-align:center">Convs in total</th></tr></thead><tbody><tr><td style="text-align:center">Input</td><td style="text-align:center">\(3\times 224 \times 224\)</td><td style="text-align:center">\(64\times 56 \times 56\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(3\)</td></tr><tr><td style="text-align:center">Tail-1</td><td style="text-align:center">\(64\times 56 \times 56\)</td><td style="text-align:center">\(128\times 28 \times 28\)</td><td style="text-align:center">\(2\)</td><td style="text-align:center">\(1\)</td><td style="text-align:center">\(7\)</td></tr><tr><td style="text-align:center">Tail-2</td><td style="text-align:center">\(128\times 28 \times 28\)</td><td style="text-align:center">\(256\times 14 \times 14\)</td><td style="text-align:center">\(4\)</td><td style="text-align:center">\(1\)</td><td style="text-align:center">\(13\)</td></tr><tr><td style="text-align:center">Tail-3</td><td style="text-align:center">\(256\times 14 \times 14\)</td><td style="text-align:center">\(512\times 7 \times 7\)</td><td style="text-align:center">\(8\)</td><td style="text-align:center">\(1\)</td><td style="text-align:center">\(25\)</td></tr><tr><td style="text-align:center">SE-block</td><td style="text-align:center">\(512\times 7 \times 7\)</td><td style="text-align:center">\(512\times 7 \times 7\)</td><td style="text-align:center">\(4\)</td><td style="text-align:center">\(1\)</td><td style="text-align:center">\(17\)</td></tr><tr><td style="text-align:center">Body-3</td><td style="text-align:center">\(512\times 7 \times 7\)</td><td style="text-align:center">\(256\times 14 \times 14\)</td><td style="text-align:center">\(2 + 2\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(12\)</td></tr><tr><td style="text-align:center">Body-2</td><td style="text-align:center">\((512+256)\times 14 \times 14\)</td><td style="text-align:center">\(384\times 28 \times 28\)</td><td style="text-align:center">\(2 + 2\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(12\)</td></tr><tr><td style="text-align:center">Body-1</td><td style="text-align:center">\((384+128)\times 28 \times 28\)</td><td style="text-align:center">\(256\times 56 \times 56\)</td><td style="text-align:center">\(2 + 2\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(12\)</td></tr><tr><td style="text-align:center">Head-1</td><td style="text-align:center">\((256+64)\times 56 \times 56\)</td><td style="text-align:center">\(320\times 28 \times 28\)</td><td style="text-align:center">\(2 + 2\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(12\)</td></tr><tr><td style="text-align:center">Head-2</td><td style="text-align:center">\((320+512)\times 28 \times 28\)</td><td style="text-align:center">\(832\times 14 \times 14\)</td><td style="text-align:center">\(2 + 2\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(12\)</td></tr><tr><td style="text-align:center">Head-3</td><td style="text-align:center">\((832+768)\times 14 \times 14\)</td><td style="text-align:center">\(1600\times 7 \times 7\)</td><td style="text-align:center">\(4 + 4\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(24\)</td></tr><tr><td style="text-align:center">Score-Conv</td><td style="text-align:center">\((1600+512)\times 7 \times 7\)</td><td style="text-align:center">\(1056\times 7 \times 7\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(1\)</td></tr><tr><td style="text-align:center">Score-FC</td><td style="text-align:center">\(1056\times 7 \times 7\)</td><td style="text-align:center">\(1000\times 1 \times 1\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(1\)</td></tr></tbody></table><p>tail，body和head三部分的主要成分都是Bottleneck模块，即下表所示的结构：</p><table><thead><tr><th style="text-align:center">Layer</th><th style="text-align:center">Type</th><th style="text-align:center">Output channels</th><th style="text-align:center">Kernel Size</th></tr></thead><tbody><tr><td style="text-align:center">(shortcut)</td><td style="text-align:center">(take shortcut)</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">relu</td><td style="text-align:center">ReLU</td><td style="text-align:center">\(C\)</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">bn1</td><td style="text-align:center">Batch Normalization</td><td style="text-align:center">\(C\)</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">conv1</td><td style="text-align:center">Convolution</td><td style="text-align:center">\(C / 4\)</td><td style="text-align:center">\(1\times 1\)</td></tr><tr><td style="text-align:center">bn2</td><td style="text-align:center">Batch Normalization</td><td style="text-align:center">\(C / 4\)</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">conv2</td><td style="text-align:center">Convolution</td><td style="text-align:center">\(C / 4\)</td><td style="text-align:center">\(3\times 3\)</td></tr><tr><td style="text-align:center">bn3</td><td style="text-align:center">Batch Normalization</td><td style="text-align:center">\(C / 4\)</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">conv3</td><td style="text-align:center">Convolution</td><td style="text-align:center">\(C&#39;\)</td><td style="text-align:center">\(1\times 1\)</td></tr><tr><td style="text-align:center">(addition)</td><td style="text-align:center">(add shortcut)</td><td style="text-align:center">\(C&#39;\)</td><td style="text-align:center">-</td></tr></tbody></table><p>在tail部分的每一个stage中，第一个Bottleneck模块会涉及通道数的变化（即\(C&#39;\neq C\)）。这时shortcut需要经过一个卷积层来变换identity mapping的通道数。因此，这三个shortcut上依旧无法避免使用Isolated convolution。在SE-block中也存在类似的情况。而在head部分中，尽管特征图仍在不断地下采样，其通道数并没有被改变，所以不需要使用这样的Isolated convolution来干扰梯度的直接反传（direct back-propagation）。</p><p>（PS：可是我数了数，FishNet-99里有100个卷积，FishNet-150里有151个卷积呀😂？个人猜测是因为Score-FC层不应该算在FishNet主干内？对了，虽然它叫做FC层，但作者代码里还是用卷积层的形式定义的哦。因为\(7\times 7\)尺寸的特征图过了一层Global Average Pooling变成了\(1\times 1\)尺寸，所以它本质上变成了一个长度为通道数的向量。）</p><h3 id="2-2-采样-amp-精化模块"><a href="#2-2-采样-amp-精化模块" class="headerlink" title="2.2 采样&amp;精化模块"></a>2.2 采样&amp;精化模块</h3><p>从body部分的stage 3开始直到head部分的stage3，每个stage的特征图将与之前部分的特征图融合（也就是图中的红色虚线和红框所表示的内容）。为了保证梯度直接反传，作者设计了UR-block (Upsampling &amp; Refinement) 和DR-block (Downsampling &amp; Refinement) 来“保持和精化”（preserve and refine）各个部分的特征。</p><h4 id="2-2-1-上采样-amp-精化（UR）模块"><a href="#2-2-1-上采样-amp-精化（UR）模块" class="headerlink" title="2.2.1 上采样&amp;精化（UR）模块"></a>2.2.1 上采样&amp;精化（UR）模块</h4><p>上边提到，FishNet中的stage号不是从浅到深依次增大的，而是与特征图的尺度相对应。设tail部分和body部分的stage \(s\)的<strong>第一层</strong>输出特征分别为\(x^t_s\)和\(x^b_s\)，则\(x^t_s\)和\(x^b_s\)的宽度和高度应该是一致的（尽管通道数可能不同）。\(x^t_s\)经过一个迁移模块\(\mathcal{T}(x)\)（transferring block，同样是带shortcut的Bottleneck模块）后与\(x^b_s\)进行连接构成融合的特征图\(\widetilde{x}^b_s\):<br>$$\widetilde{x}^b_s = concat(x^b_s, \mathcal{T}(x^t_s))$$</p><p>\(\widetilde{x}^b_s\)将继续作为body部分的stage \(s\)中后面的卷积层\(\mathcal{M}(x)\)的输入。同时，为了梯度的直接反传，另有一条恒等映射与\(\mathcal{M}(\widetilde{x}^b_s)\)相加。这里的思路与ResNet中\(\mathcal{H}(x)=x+\mathcal{F}(x)\)是一致的：<br>$$\widetilde{x}&#39;^b_s = r(\widetilde{x}^b_s) + \mathcal{M}(\widetilde{x}^b_s)$$</p><p>在body部分的stage 1中，\(\mathcal{M}(x)\)的输出值通道数与\(x\)相同，此时\(r(x)\)即为\(x\)。而stage 2和stage 1中，由于\(\mathcal{M}(x)\)中通道数会产生变化（在作者代码中，通道数减半，\(k=2\)），所以这里的\(r(x)\)需要起到缩小通道数（channel-wise reduction）的作用。<strong>还是为了梯度直接反传</strong>，这里甚至没有使用\((1\times 1)\)的卷积来变换通道数，而是直接把每\(k\)个通道求和（element-wise summation）而压缩成一个通道。\(\widetilde{x}’^b_s\)再进行一下上采样就成为body部分下一个stage（即stage \(s-1\)）的输入了：<br>$$x^b_{s-1}=up(\widetilde{x}&#39;^b_s)$$</p><div align="center" class="figure"><img src="/images/fishnet/ur.png" width="20%" alt="Upsampling & Refinement blocks"><br>    Fig. 6 上采样&amp;精化模块<br><br></div><p>（PS：为什么这里不用\((1\times 1)\)卷积，而前面tail部分要用呢？个人猜测是因为tail部分要扩大通道数而不得不用这样的方式。或许在tail部分使用与这里的\(r(x)\)相反的过程——通过把每个通道duplicate一下来达成通道数增加一倍的效果也能work呢？有兴趣的可以试一下。）</p><h4 id="2-2-2-下采样-amp-精化（DR）模块"><a href="#2-2-2-下采样-amp-精化（DR）模块" class="headerlink" title="2.2.2 下采样&amp;精化（DR）模块"></a>2.2.2 下采样&amp;精化（DR）模块</h4><p>head部分的下采样&amp;精化模块比上采样&amp;精化模块更加简单，因为这里所有的\(\mathcal{M}(x)\)都不会导致通道数的变化，UR模块用于的\(r(x)\)也就不需要了。其他的公式与UR模块基本相同：<br>$$\widetilde{x}^b_s = concat(x^b_s, \mathcal{T}(x^t_s)) \\<br>\widetilde{x}&#39;^b_s = \widetilde{x}^b_s + \mathcal{M}(\widetilde{x}^b_s) \\<br>x^b_{s+1}=down(\widetilde{x}&#39;^b_s)$$</p><div align="center" class="figure"><img src="/images/fishnet/dr.png" width="20%" alt="Downsampling & Refinement blocks"><br>    Fig. 7 下采样&amp;精化模块<br><br></div><h2 id="3-经验总结"><a href="#3-经验总结" class="headerlink" title="3 经验总结"></a>3 经验总结</h2><h3 id="3-1-低层特征对高层特征的加强"><a href="#3-1-低层特征对高层特征的加强" class="headerlink" title="3.1 低层特征对高层特征的加强"></a>3.1 低层特征对高层特征的加强</h3><p>漏斗状卷积网络里，较浅卷积层中的特征往往是较简单、像素级的特征，而更深的卷积层中的特征由于感受域较大，是更抽象、泛化的特征。由于FishNet中上采样、下采样的存在，直接以“浅层”“深层”特征来区分不同分辨率的特征似乎并不妥当。因此，这里我用“低层特征”来指代分辨率较大、较具体的特征，用“高层特征”指代分辨率较小、抽象程度较高，或者说“浓缩程度”较高的特征。</p><p>分类任务里，图像通过一个漏斗状的卷积网络即可回归出它的类别；检测任务里，通过用高层特征加强低层特征的方式可以有效提升检测效果；如果反过来再用低层特征增强高层特征，网络则可同时被用于图像级、区域级和像素级的不同任务。</p><h3 id="3-2-避免Isolated-Convolution"><a href="#3-2-避免Isolated-Convolution" class="headerlink" title="3.2 避免Isolated Convolution"></a>3.2 避免Isolated Convolution</h3><p>尽量避免在shortcut上使用I-conv。FishNet除了tail部分在涉及通道数变化的残差模块上使用了I-conv外，在body和head部分的融合时都避免使用I-conv，从而最大限度地保证了梯度的直接反传。</p><h3 id="3-3-上采样的方式"><a href="#3-3-上采样的方式" class="headerlink" title="3.3 上采样的方式"></a>3.3 上采样的方式</h3><p>上采样方式的选择上，尽可能<strong>不使用带权值的反卷积</strong>，而是用最近邻插值等方式。此举同样是为了保证梯度的直接反传。</p><h3 id="3-4-下采样的方式"><a href="#3-4-下采样的方式" class="headerlink" title="3.4 下采样的方式"></a>3.4 下采样的方式</h3><p><strong>用kernel尺寸为\((2\times 2)\)、stride也为\(2\)的MaxPooling进行下采样</strong>与其他几种典型的下采样方式相比，效果更好。用来对(diao)比(da)的另外几种下采样方式包括：</p><ul><li>最后一层卷积stride=\(2\)（干扰了梯度直接反传）</li><li>kernel size=\((3\times 3)\)、stride=\(2\)的MaxPooling（滑动窗口有交叠，扰乱了结构信息）</li><li>kernel size=\((3\times 3)\)、stride=\(2\)的AveragePooling（原文没讲，个人认为与最后一层卷积加stride=\(2\)效果类似）</li></ul><h2 id="4-个人感悟"><a href="#4-个人感悟" class="headerlink" title="4 个人感悟"></a>4 个人感悟</h2><p>“老僧三十年前，未参禅时，见山是山，见水是水。</p><p>及至后来亲见知识，有个入处，见山不是山，见水不是水。</p><p>而今得个休歇处，依前见山只是山，见水只是水。</p><p>大众，这三般见解，是同是别，有人缁素得出。” </p><p>——吉州青原惟信禅师[3]</p><p>FishNet的思想，似乎与这三重境界有什么关联？池化，插值，融合，再池化，再融合，这个过程，仿佛一个人脑海中对知识的建构、解构和重构的过程。</p><p>我在初识某些新事物时，由于对它还没有形成充分的了解，只是大致地形成了一个印象。比如十多年前，“屏幕，主机，鼠标，键盘”，这就是我脑海中一台计算机的样子，所谓“计算机科学”，在当时的自己看来也不过是用一些软件写写文档画画图之类的工作。</p><p>随着学习的逐渐深入，我从一个使用者成为了一个开发者后，关注点也不断地深入、细化：当看到一个网页的动效，我想到按F12看看它是怎么用js实现的，想到这个异步请求是怎么响应的，想到网络请求的TCP报文是怎样的，想到报文是如何经历一系列路由器传输到服务器的。在对计算机的了解不断深入的过程中，我却又对它产生了一种陌生感——这门科学还藏有多少的奥秘，其中是否有些我甚至还无法想象？</p><p>至于再将学习深入下去我会对计算机产生怎样的认识，才疏学浅，尚不得而知。也许某一天我会恍然大悟——哦，原来计算机科学就是这个样子的呀。</p><h2 id="5-参考文献"><a href="#5-参考文献" class="headerlink" title="5 参考文献"></a>5 参考文献</h2><p>[1] <a href="http://papers.nips.cc/paper/7356-fishnet-a-versatile-backbone-for-image-region-and-pixel-level-prediction.pdf" target="_blank" rel="noopener">Sun S, Pang J, Shi J, et al. Fishnet: A versatile backbone for image, region, and pixel level prediction[C]//Advances in Neural Information Processing Systems. 2018: 754-764.</a></p><p>[2] <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Hu J, Shen L, Sun G. Squeeze-and-excitation networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 7132-7141.</a></p><p>[3] <a href="http://www.shixiu.net/wenhua/tuijian/zyl/4802.html" target="_blank" rel="noopener">瞿汝稷. 指月录[M]. 出版信息不详. 卷二十八 六祖下第十四世</a></p><style type="text/css" rel="stylesheet">.markdown-body p {    text-indent: 2em}   </style>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;p&gt;从VALSE2019回来后，感觉自己俨然变成了欧阳万里老师的脑残粉呀╰( ᐖ╰)！会上欧阳老师介绍的FishNet简直让我眼前一亮，这么好的点子，我怎么就没想到呐！回来好好读了一下文章和代码，简单总结一下。&lt;/p&gt;
&lt;div align=&quot;center&quot; class=&quot;figure&quot;&gt;&lt;br&gt;  &lt;img src=&quot;/images/fishnet/fish.jpg&quot; width=&quot;15%&quot; alt=&quot;我咸甚，此鱼何能及也&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://linkinpark213.com/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://linkinpark213.com/tags/Computer-Vision/"/>
    
      <category term="Reviews" scheme="http://linkinpark213.com/tags/Reviews/"/>
    
  </entry>
  
  <entry>
    <title>A Brief Review of the R-CNN Family - Region-based CNN for Object Detection</title>
    <link href="http://linkinpark213.com/2019/03/17/rcnns/"/>
    <id>http://linkinpark213.com/2019/03/17/rcnns/</id>
    <published>2019-03-17T05:01:01.000Z</published>
    <updated>2019-04-21T07:23:02.102Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><p>The R-CNNs are awesome works on object detection, which demonstrated the effectiveness of using region proposals with deep neural networks, and have become a state-of-the-art baseline for the object detection task. In this blog post I&#39;ll make a brief review of the R-CNN family - from R-CNN to Mask R-CNN, and several related works based on the idea of R-CNNs. Implementation and evaluation details are not mentioned here. For those details, please refer to the original papers provided in the <a href="#8-References">References</a> section.</p><a id="more"></a><h2 id="0-Object-Detection-before-R-CNN"><a href="#0-Object-Detection-before-R-CNN" class="headerlink" title="0 Object Detection before R-CNN"></a>0 Object Detection before R-CNN</h2><p>Before CNN was widely adopted in object detection, SIFT or HOG features are commonly used for the detection task.</p><p>Unlike image classification, detection requires localizing objects within an image. Common approaches to localization are 1) bounding box regression, and 2) sliding-window detector. The first approach used in [1] proved to be not working very well, while the second used in [2] needs high spatial resolutions, thus deeper networks makes precise localization a challenge.</p><h2 id="1-R-CNN-Region-based-R-CNN"><a href="#1-R-CNN-Region-based-R-CNN" class="headerlink" title="1 R-CNN: Region-based R-CNN"></a>1 R-CNN: Region-based R-CNN</h2><p>R-CNN solves the CNN localization problem by operating the &quot;recognition using regions&quot; paradigm.</p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>From the input image, the method first generates around 2000 category-independent region proposals with Selective Search algorithm, and then extracts a fixed-length feature vector from each proposal using the same CNN(AlexNet). Finally, it classifies each region with category-specific linear SVMs. </p><div align="center" class="figure"><br>    <img src="/images/rcnns/R-CNN.png" width="50%" height="50%" alt><br>    Fig. 1 Overview of R-CNN.<br><br></div><p>However, the region proposal may not be that satisfactory as a final detection window. Therefore, a bounding-box regression stage is introduced to predict a new detection window given the feature map of a region proposal. As reported in [3], this simple approach fixes a large number of mislocalized detections. More details are available in the supplementary material[12] of the R-CNN paper.</p><p>Since AlexNet only takes images of size 227 × 227, the image clip in the bounding box should be resized.<br>In R-CNN, the image clip is directly warped into the demanded size.</p><div align="center" class="figure"><br>    <img src="/images/rcnns/warp.png" width="20%" alt><br>    Fig. 2 Cropping from the bounding box and warping.<br><br></div><h3 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h3><ul><li>The <em>region proposal(RoI) - feature extraction - classification</em> approach</li><li>Using <em>Selective Search</em> to generate region proposals</li><li>Using <em>bounding-box regression</em> to refine region proposals</li><li>Using <em>CNN features</em> for classification</li></ul><h3 id="Known-drawbacks"><a href="#Known-drawbacks" class="headerlink" title="Known drawbacks"></a>Known drawbacks</h3><ul><li>Run CNN feature extraction on each of the 2000 regions consumes too much computation</li><li>The warped content may result in unwanted geometric distortion</li></ul><h2 id="2-SPP-Net-Spatial-Pyramid-Pooling"><a href="#2-SPP-Net-Spatial-Pyramid-Pooling" class="headerlink" title="2 SPP-Net: Spatial Pyramid Pooling"></a>2 SPP-Net: Spatial Pyramid Pooling</h2><p>SPP-Net introduces the spatial pyramid pooling layer that takes in feature maps of arbitrary size, while also considering multi-scale features in the input image. It also solved the way-too-slow issue of R-CNN.</p><h3 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h3><p>While R-CNN extracts features from warped image clips in each proposed region, the SPP-Net first extracts the feature of the whole image and get one shared feature map. After this, the feature map is cropped according to the bounding boxes (boxes fixed by regressor, same as R-CNN). Each of the feature map clip is put into the spatial pyramid pooling layers to get a feature vector of the same length. Then the feature vectors are the inputs of following fully connected layers which are the same as R-CNN.</p><div align="center" class="figure"><br>    <img src="/images/rcnns/SPP-Net.png" width="60%" alt><br>    Fig. 3 Overview of SPP-Net.<br><br></div><div align="center" class="figure"><br>    <img src="/images/rcnns/spplayer.png" width="25%" alt><br>    Fig. 4 Spatial pyramid pooling layer.<br><br></div><p>The spatial pyramid pooling layers consider the feature map clip in different scales - it divides the feature map clip into 4 × 4, 2 × 2 and 1 × 1 grids and computes 4 × 4, 2 × 2 and 1 × 1 feature maps (channel number doesn&#39;t change). The computed feature maps are flattened and concatenated into one vector, which is the input of the following fully connected layers.</p><h3 id="Contribution-1"><a href="#Contribution-1" class="headerlink" title="Contribution"></a>Contribution</h3><ul><li>Extracting <em>feature maps first and only once</em>, greatly improves the speed of R-CNN</li><li>Using <em>spatial pyramid pooling layers</em>, avoiding geometric distortion</li></ul><h3 id="Known-drawbacks-1"><a href="#Known-drawbacks-1" class="headerlink" title="Known drawbacks"></a>Known drawbacks</h3><ul><li>Training classifier and box regressor separately requires much work</li></ul><h2 id="3-Fast-R-CNN-Feature-Extraction-Only-Once"><a href="#3-Fast-R-CNN-Feature-Extraction-Only-Once" class="headerlink" title="3 Fast R-CNN: Feature Extraction Only Once"></a>3 Fast R-CNN: Feature Extraction Only Once</h2><p>As mentioned in the paper, R-CNN is slow because it performs a ConvNet forward pass for each object proposal, without sharing computation. Fast R-CNN improved its detection efficiency by using the deeper VGG16 network, which is 213 times(nice number :D) faster than R-CNN. It also introduced RoI pooling layer, which is simple a special case of SPP-Net where only one scale is considered(only one pyramid level). Fast R-CNN uses a multi-task loss and is trained in single stage, updating all network layers. Fast R-CNN yields higher detection quality(mAP) than R-CNN and SPP-Net, while being comparatively fast to train and test.</p><h3 id="Overview-2"><a href="#Overview-2" class="headerlink" title="Overview"></a>Overview</h3><p>Similar to SPP-Net, Fast R-CNN extracts image features before the RoI-based projection to share computation and speed up detection. But differently, Fast R-CNN uses a deep neural network - <a href="http://localhost:4000/2018/04/21/vgg/" target="_blank" rel="noopener">VGG</a>16 for more efficient feature extraction. Rather than training bounding-box regressor and classifier separately, Fast R-CNN uses a streamlined training process and jointly optimize a softmax classifier and a bounding-box regressor. The RoI-fixing regressor is moved after the fully-connected layers. The multi-task loss <strong>for each RoI</strong> is defined as:</p><p>$$L(p,u,t^u,v) = L_{cls}[p,u]+\lambda[u\geq 1]L_{loc}(t^u,v)$$</p><p>in which the definition of classification loss and localization loss are:</p><p>$$L_{cls}(p,u)=-log(p_u)$$</p><p>$$L_{loc}(t^u,v)=\sum_{i\in \{x,y,w,h\}}{smooth_{L_1}(t_i^u-v_i)}$$</p><p>in which \(smooth_{L_1}\) loss is defined as:</p><p>$$smooth_{L_1}(x)=\begin{cases}<br>0.5x^2&amp; \text{if |x|&lt;1}\\<br>|x|-0.5&amp; \text{otherwise}<br>\end{cases}$$</p><p>Symbol definitions:</p><table><thead><tr><th>Symbol</th><th><center>Definition</center></th><th><center>Definition by formula</center></th></tr></thead><tbody><tr><td><center>\(p\)</center></td><td>Output of the classification layer, a vector of length \(K+1\)(K object classes and background)</td><td>\(p=(p_0,\cdots,p_K)\)</td></tr><tr><td><center>\(t\)</center></td><td>Output of the regression layer, a matrix of size \(K\times 4\).</td><td>\(t^k=(t^k_x,t^k_y,t^k_w,t^k_h)\)</td></tr><tr><td><center>\(u\)</center></td><td>True class.</td><td>\(u\in N, 1\le u \le K\)</td></tr><tr><td><center>\(v\)</center></td><td>True bounding-box regression target.</td><td>\(v=(v_x,v_y,v_w,v_h)\)</td></tr></tbody></table><div align="center" class="figure"><br>    <img src="/images/rcnns/Fast R-CNN.png" width="60%" alt><br>    Fig. 5 Overview of Fast R-CNN.<br><br></div><p>In this architecture, two of the three main procedures except region proposal are trained in single-stage with the multi-task loss. </p><p>Here is are two graphs demonstrating common pooling layers(max or avg) and RoI pooling layers. On the left is the original 5x5 feature map, and each in grid is a pixel value. During calculation, the common pooling kernel covers an area each step and calculates the maximum value or the average value in the area. With a kernel size of 3x3 and a stride of 2, a feature map of 2x2 is generated from the 5x5 feature map.</p><div align="center" class="figure"><br>    <img src="/images/rcnns/maxpool.png" width="40%" alt><br>    Fig. 6 Common pooling with kernel_size=3 and stride=2.<br><br></div><p>And in RoI pooling, the RoI is cropped from the whole feature map, and is divided into pieces with equal areas according to the output feature map size. However, it&#39;s possible that grids on the borders of different pieces have to be assigned to one piece only. In this case, there may be a little bit of &quot;injustice&quot; among the pieces. In each piece, a global average/maximum pooling is done and the result is only one number in each channel.</p><div align="center" class="figure"><br>    <img src="/images/rcnns/roipool.png" width="40%" alt><br>    Fig. 7 RoI pooling with output size=(2, 2). The black dashed line denotes the original RoI, and the colored area is the actual cropped RoI.<br><br></div><h3 id="Contribution-2"><a href="#Contribution-2" class="headerlink" title="Contribution"></a>Contribution</h3><ul><li>Deeper CNN - <em>VGG16</em> for feature extraction</li><li><em>Multi-task loss</em> &amp; <em>Single-stage training</em></li></ul><h3 id="Known-drawbacks-2"><a href="#Known-drawbacks-2" class="headerlink" title="Known drawbacks"></a>Known drawbacks</h3><ul><li>For region proposal, conventional Selective Search algorithm doesn&#39;t make use of GPU computation power, thus consuming more time</li></ul><h2 id="4-Faster-R-CNN-Region-Proposal-Networks-Speeds-You-Up"><a href="#4-Faster-R-CNN-Region-Proposal-Networks-Speeds-You-Up" class="headerlink" title="4 Faster R-CNN: Region Proposal Networks Speeds You Up"></a>4 Faster R-CNN: Region Proposal Networks Speeds You Up</h2><p>In Fast R-CNN, two of the three main procedures are trained in single-stage, except region proposal. And region proposal is the bottleneck of total detection speed, since GPU with high computation power isn&#39;t utilized here yet. Why not try training a CNN that generates region proposals?</p><h3 id="Overview-3"><a href="#Overview-3" class="headerlink" title="Overview"></a>Overview</h3><p>Simply remove the Selective Search in Fast R-CNN. In place of the SS algorithm, an RPN(Region Proposal Network) is introduced. Given the DCNN features, the RPN generates RoIs with improved speed.</p><h3 id="but-how-on-earth-does-the-RPN-work"><a href="#but-how-on-earth-does-the-RPN-work" class="headerlink" title="...but how on earth does the RPN work?"></a>...but how on earth does the RPN work?</h3><p>This is a question that had been confusing me for so long. </p><p>In a word, it&#39;s a simple CNN taking an image of any size as input, slides a window and outputs \(6k\) numbers each time the window moves. \(k\) is the number of anchors pre-defined - IT DOES NOT MEAN &quot;THOUSAND&quot;. Wait, what is an anchor?</p><p>An anchor is a box size we define first before generating data (for example, \((width=36, height=78)\) for pedestrain, and \((width=50, height=34)\) for dogs?). Though the input image is of size \(n * n\), the anchor can be in any size and any w-h ratio. The prediction of the 6 numbers are based on the anchors we define. When the RPN works, it does NOT predict the possibility that there is an object - BUT the possibility that there is an object that fits in the anchor.</p><p>Besides a classification layer predicting the possibility of there being an object and the possibility of there being nothing but background, a regression layer predicts the relative box coordinates \((t_x, t_y, t_w, t_h)\). For each anchor, its size \((w_a, h_a)\) is given and its position \((w_x, w_y)\) is decided by the center position of the sliding window. The relation between relative coordinates \((t_x, t_y, t_w, t_h)\) and absolute coordinates \((x, y, w, h)\) is:<br>$$t_x=(x-x_a)/w_a\\<br>t_y=(y-y_a)/h_a\\<br>t_w=log(w/w_a)\\<br>t_h=log(h/h_a)$$</p><p>for both prediction and ground truth.</p><div align="center" class="figure"><br>    <img src="/images/rcnns/RPN.png" width="30%" alt><br>    Fig. 8 The original graph demonstration of RPN. Keep in mind that &quot;k&quot; does not mean &quot;thousand&quot;.<br><br></div><p>But there are a great pile of boxes generated by the RPN. Some basic methods have to be taken to select the &quot;good&quot; boxes. Firstly, the boxes with low object scores and high background scores (usually thresholds are set manually) are abandoned. Secondly, using <a href="https://zh.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH" target="_blank" rel="noopener">non-maximum supression</a>, one box for each object target is elected from all boxes that mark the same object.</p><h3 id="Contribution-3"><a href="#Contribution-3" class="headerlink" title="Contribution"></a>Contribution</h3><ul><li><em>Region Proposal Network</em> - high-speed high-quality region proposals</li></ul><h2 id="5-Mask-R-CNN-Detection-and-Instance-Segmentation"><a href="#5-Mask-R-CNN-Detection-and-Instance-Segmentation" class="headerlink" title="5 Mask R-CNN: Detection and Instance Segmentation"></a>5 Mask R-CNN: Detection and Instance Segmentation</h2><p>Though Mask R-CNN is a great work, its idea is rather intuitive - since detection and classification is done, why not add a segmentation head? In this case, some instance-first instance segmentation work would be done!</p><h3 id="Overview-4"><a href="#Overview-4" class="headerlink" title="Overview"></a>Overview</h3><p>Add a small mask fully-convolutional overhead to Faster R-CNN, replace VGG net with more efficient ResNet/FPN(Residual Network / Feature Pyramid Network) and replace RoI pooling with RoI alignment.</p><div align="center" class="figure"><br>    <img src="/images/rcnns/Mask R-CNN.png" width="30%" alt><br>    Fig. 9 The Mask R-CNN framework for instance segmentation. The last convlutional layer is the newly added segmentation layer for each RoI.<br><br></div><h3 id="RoIAlign"><a href="#RoIAlign" class="headerlink" title="RoIAlign"></a>RoIAlign</h3><p>In RoI pooling, quantization will be performed when the RoI coordinates are not integers. For example, when cutting the area \((x_1=11.02, y_1=53.9, x_2=16.2, y_2=58.74)\), actually the area \((x_1=11, y_1=54, x_2=16, y_2=59)\) is what we get (nearest-neighbor).</p><p>But in RoI alignment, the area is exactly \((x_1=11.02, y_1=53.9, x_2=16.2, y_2=58.74)\). Instead of cropping it down, the feature map area is sampled using some sample points. Divide the RoI into \(n*n\)(output size) bins Using bi-linear interpolation, one value would be calculated at each sample point. In the image below is a simple example. In this case we have only one sample point for each pixel in the pooled RoI. Coordinate of the only sample point in the first area is \((12.315, 55.11)\). Calculate the weighted average of the 4 grid points nearby his sample point and we&#39;ll have the value for this pixel in the pooled feature map.</p><div align="center" class="figure"><br>    <img src="/images/rcnns/roialign1.png" width="25%" alt><br>    Fig. 10 RoI alignment with output size=(2, 2) and 1 sample point each bin.<br><br></div><p>It&#39;s obvious that one sample point each bin is far from enough in our example. So using more sample points is wiser.</p><div align="center" class="figure"><br>    <img src="/images/rcnns/roialign2.png" width="25%" alt><br>    Fig. 11 RoI alignment with output size=(2, 2) and 2×2 sample point each bin.<br><br></div><h3 id="Contribution-4"><a href="#Contribution-4" class="headerlink" title="Contribution"></a>Contribution</h3><ul><li><em>RoI Align</em> - improving mask accuracy greatly</li><li>Add a segmentation overhead on Faster R-CNN and achieve accurate instance segmentation</li></ul><h2 id="6-R-CNNs-Proposed-by-Other-Researchers"><a href="#6-R-CNNs-Proposed-by-Other-Researchers" class="headerlink" title="6 R-CNNs Proposed by Other Researchers"></a>6 R-CNNs Proposed by Other Researchers</h2><p>There are several other R-CNNs by other researchers, which are basically variants of the R-CNN architecture.</p><h3 id="Light-Head-R-CNN"><a href="#Light-Head-R-CNN" class="headerlink" title="Light-Head R-CNN"></a>Light-Head R-CNN</h3><p>arXiv: <a href="https://arxiv.org/abs/1711.07264" target="_blank" rel="noopener">https://arxiv.org/abs/1711.07264</a><br>Code(Official, TensorFlow): <a href="https://github.com/zengarden/light_head_rcnn" target="_blank" rel="noopener">https://github.com/zengarden/light_head_rcnn</a></p><h3 id="Cascade-R-CNN"><a href="#Cascade-R-CNN" class="headerlink" title="Cascade R-CNN"></a>Cascade R-CNN</h3><p>arXiv: <a href="https://arxiv.org/abs/1712.00726" target="_blank" rel="noopener">https://arxiv.org/abs/1712.00726</a><br>Code(Official, Caffe): <a href="https://github.com/zhaoweicai/cascade-rcnn" target="_blank" rel="noopener">https://github.com/zhaoweicai/cascade-rcnn</a><br>Code(PyTorch): <a href="https://github.com/guoruoqian/cascade-rcnn_Pytorch" target="_blank" rel="noopener">https://github.com/guoruoqian/cascade-rcnn_Pytorch</a></p><h3 id="Grid-R-CNN"><a href="#Grid-R-CNN" class="headerlink" title="Grid R-CNN"></a>Grid R-CNN</h3><p>arXiv: <a href="https://arxiv.org/abs/1811.12030" target="_blank" rel="noopener">https://arxiv.org/abs/1811.12030</a><br>Code: Not yet</p><h2 id="8-References"><a href="#8-References" class="headerlink" title="8 References"></a>8 References</h2><p>[1] <a href="http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf" target="_blank" rel="noopener">Szegedy, Christian, Alexander Toshev, and Dumitru Erhan. &quot;Deep neural networks for object detection.&quot; Advances in neural information processing systems. 2013.</a></p><p>[2] <a href="https://arxiv.org/pdf/1312.6229.pdf" target="_blank" rel="noopener">Sermanet, Pierre, et al. &quot;Overfeat: Integrated recognition, localization and detection using convolutional networks.&quot; arXiv preprint arXiv:1312.6229 (2013).</a></p><p>[3] <a href="https://arxiv.org/pdf/1311.2524.pdf" target="_blank" rel="noopener">Girshick, Ross, et al. &quot;Rich feature hierarchies for accurate object detection and semantic segmentation.&quot; Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.</a></p><p>[4] <a href="https://arxiv.org/pdf/1406.4729.pdf" target="_blank" rel="noopener">He, Kaiming, et al. &quot;Spatial pyramid pooling in deep convolutional networks for visual recognition.&quot; European conference on computer vision. Springer, Cham, 2014.</a></p><p>[5] <a href="https://arxiv.org/pdf/1504.08083.pdf" target="_blank" rel="noopener">Girshick, Ross. &quot;Fast r-cnn.&quot; Proceedings of the IEEE international conference on computer vision. 2015.</a></p><p>[6] <a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" target="_blank" rel="noopener">Ren, Shaoqing, et al. &quot;Faster r-cnn: Towards real-time object detection with region proposal networks.&quot; Advances in neural information processing systems. 2015.</a></p><p>[7] <a href="https://arxiv.org/pdf/1703.06870.pdf" target="_blank" rel="noopener">He, Kaiming, et al. &quot;Mask r-cnn.&quot; Computer Vision (ICCV), 2017 IEEE International Conference on. IEEE, 2017.</a></p><p>[8] <a href="https://arxiv.org/pdf/1711.07264.pdf" target="_blank" rel="noopener">Li, Zeming, et al. &quot;Light-head r-cnn: In defense of two-stage object detector.&quot; arXiv preprint arXiv:1711.07264 (2017).</a></p><p>[9] <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2603.pdf" target="_blank" rel="noopener">Cai, Zhaowei, and Nuno Vasconcelos. &quot;Cascade r-cnn: Delving into high quality object detection.&quot; IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Vol. 1. No. 2. 2018.</a></p><p>[10] <a href="https://arxiv.org/abs/1811.12030" target="_blank" rel="noopener">Xin Lu, et al. &quot;Grid R-CNN.&quot; arXiv preprint arXiv:1811.12030 (2018).</a></p><p>[11] <a href="https://dl.dropboxusercontent.com/s/1yisyl5cuxo7g9y/r-cnn-cvpr-supp.pdf?dl=0" target="_blank" rel="noopener">Girshick, Ross, et al. &quot;Rich feature hierarchies for accurate object detection and semantic segmentation.&quot; Proceedings of the IEEE conference on computer vision and pattern recognition - supplementary material. 2014.</a></p>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;p&gt;The R-CNNs are awesome works on object detection, which demonstrated the effectiveness of using region proposals with deep neural networks, and have become a state-of-the-art baseline for the object detection task. In this blog post I&amp;#39;ll make a brief review of the R-CNN family - from R-CNN to Mask R-CNN, and several related works based on the idea of R-CNNs. Implementation and evaluation details are not mentioned here. For those details, please refer to the original papers provided in the &lt;a href=&quot;#8-References&quot;&gt;References&lt;/a&gt; section.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://linkinpark213.com/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://linkinpark213.com/tags/Computer-Vision/"/>
    
      <category term="Reviews" scheme="http://linkinpark213.com/tags/Reviews/"/>
    
      <category term="Object Detection" scheme="http://linkinpark213.com/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[MineSweeping] The Long Struggle of DensePose Installation</title>
    <link href="http://linkinpark213.com/2018/11/18/densepose-minesweeping/"/>
    <id>http://linkinpark213.com/2018/11/18/densepose-minesweeping/</id>
    <published>2018-11-18T08:00:14.000Z</published>
    <updated>2019-10-11T02:06:27.271Z</updated>
    
    <content type="html"><![CDATA[<p>Update in October 2019:</p><p>DensePose has been re-implemented with the brand-new object detection framework <a href="https://github.com/facebookresearch/detectron2" target="_blank" rel="noopener">Detectron2</a>, which is based on PyTorch and much easier to install and use (You don&#39;t have to manually compile Caffe2)<br>I strongly recommend that you check out the new official DensePose code at <a href="https://github.com/facebookresearch/detectron2/tree/master/projects/DensePose" target="_blank" rel="noopener">https://github.com/facebookresearch/detectron2/tree/master/projects/DensePose</a>.</p><p><br><br>DensePose is a great work in real-time human pose estimation, which is based on Caffe2 and Detectron framework. It extracts dense human body 3D surface based on RGB images. The installation instructions are provided <a href="https://github.com/facebookresearch/DensePose/blob/master/INSTALL.md" target="_blank" rel="noopener">here</a>.</p><p>During my installation process, these are the problems that took me some time to tackle. I spent on week to finally figure out solutions to all the issues. So lucky of me not to give up too early...</p><div align="center"><br>    <img src="/images/densepose-ms/facebook.jpg" width="15%" height="15%" alt="Greetings from Facebook AI Research"><br></div><a id="more"></a><p>By the way, <strong>before you suffer too much</strong>, I strongly recommend following the step-by-step <a href="https://github.com/Johnqczhang/densepose_installation/blob/master/README.md" target="_blank" rel="noopener">Caffe2+DensePose installation guide</a> by <a href="https://github.com/Johnqczhang" target="_blank" rel="noopener">@Johnqczhang</a>. If you think you&#39;re almost there, help yourself with the solutions below~</p><h2 id="1-Environment"><a href="#1-Environment" class="headerlink" title="1 Environment"></a>1 Environment</h2><ul><li>System: Ubuntu 18.04</li><li>Linux kernel: 4.15.0-29-generic</li><li>Graphics card: NVIDIA GeForce 1080Ti</li><li>Graphics driver: 410.48</li><li>CUDA: 10.0.130</li><li>cuDNN: 7.3.1</li><li>Caffe2: Built from source</li><li>Python: 2.7.15, based on Anaconda 4.5.11</li></ul><h2 id="2-Problems-amp-Solutions"><a href="#2-Problems-amp-Solutions" class="headerlink" title="2 Problems &amp; Solutions"></a>2 Problems &amp; Solutions</h2><h3 id="2-1-Caffe2-module-not-found"><a href="#2-1-Caffe2-module-not-found" class="headerlink" title="2.1 Caffe2 module not found"></a>2.1 Caffe2 module not found</h3><h4 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>make</code>.</p><p>Main error message:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Could not find a package configuration file provided by <span class="string">"Caffe2"</span> with any  </span><br><span class="line">of the following names: </span><br><span class="line">    Caffe2Config.cmake </span><br><span class="line">    caffe2-config.cmake</span><br></pre></td></tr></table></figure></p><h4 id="Cause"><a href="#Cause" class="headerlink" title="Cause"></a>Cause</h4><p>Caffe2 build path isn&#39;t known by CMake.</p><h4 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>(Caffe2_DIR <span class="string">"/path/to/pytorch/torch/share/cmake/Caffe2/"</span>)</span><br></pre></td></tr></table></figure></p><p>(Note: <code>set(Caffe2_DIR &quot;/path/to/pytorch/build/&quot;)</code> can also fix this issue but may cause other issues.) </p><h3 id="2-2-Detectron-ops-lib-not-found"><a href="#2-2-Detectron-ops-lib-not-found" class="headerlink" title="2.2 Detectron ops lib not found"></a>2.2 Detectron ops lib not found</h3><h4 id="Details-1"><a href="#Details-1" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>python2 $DENSEPOSE/detectron/tests/test_spatial_narrow_as_op.py</code> after <code>make</code>.</p><p>Main error message:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Detectron ops lib not found; make sure that your Caffe2 version includes Detectron module.</span><br></pre></td></tr></table></figure></p><h4 id="Cause-1"><a href="#Cause-1" class="headerlink" title="Cause"></a>Cause</h4><p>Seems that the Python part of DensePose couldn&#39;t recognize Caffe2.</p><h4 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution"></a>Solution</h4><p>Add <code>/path/to/pytorch/build</code> to <code>PYTHONPATH</code> environment variable. Could be added by directly <code>export PYTHONPATH=$PYTHONPATH:/path/to/pytorch/build</code> instruction or by adding this line to <code>~/.bashrc</code>. Remember to run <code>source ~/.bashrc</code> after the modification.</p><h3 id="2-3-cmake-files-not-found-amp-Unknown-CMake-command-quot-caffe2-interface-library-quot"><a href="#2-3-cmake-files-not-found-amp-Unknown-CMake-command-quot-caffe2-interface-library-quot" class="headerlink" title="2.3 *.cmake files not found &amp; Unknown CMake command &quot;caffe2_interface_library&quot;"></a>2.3 *.cmake files not found &amp; Unknown CMake command &quot;caffe2_interface_library&quot;</h3><h4 id="Details-2"><a href="#Details-2" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p><p>Main error message:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">CMake Error at /path/to/pytorch/build/Caffe2Config.cmake:<span class="number">14</span> (<span class="keyword">include</span>):</span><br><span class="line">  <span class="keyword">include</span> could not find load file:</span><br><span class="line"></span><br><span class="line">    /path/to/pytorch/build/public/utils.cmake</span><br><span class="line">    /path/to/pytorch/build/public/threads.cmake</span><br><span class="line">    /path/to/pytorch/build/public/cuda.cmake</span><br><span class="line">    /path/to/pytorch/build/public/mkl.cmake</span><br><span class="line">    /path/to/pytorch/build/Caffe2Targets.cmake</span><br><span class="line"></span><br><span class="line">Call Stack (most recent call first):</span><br><span class="line">  CMakeLists.txt:<span class="number">8</span> (<span class="keyword">find_package</span>)</span><br><span class="line"></span><br><span class="line">CMake Error at /path/to/pytorch/build/Caffe2Config.cmake:<span class="number">117</span> (caffe2_interface_library):</span><br><span class="line">  Unknown CMake command <span class="string">"caffe2_interface_library"</span>.</span><br><span class="line">Call Stack (most recent call first):</span><br><span class="line">  CMakeLists.txt:<span class="number">8</span> (<span class="keyword">find_package</span>)</span><br></pre></td></tr></table></figure></p><p>(Several <code>*.cmake</code> files, I only showed a few.)</p><h4 id="Cause-2"><a href="#Cause-2" class="headerlink" title="Cause"></a>Cause</h4><p>These files are not in the <code>pytorch/build</code> directory. By searching, I found that they are in the <code>pytorch/torch/share/cmake/Caffe2</code> directory.</p><h4 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>(Caffe2_DIR <span class="string">"/path/to/pytorch/torch/share/cmake/Caffe2/"</span>)</span><br></pre></td></tr></table></figure></p><h3 id="2-4-quot-context-gpu-h-quot-not-found"><a href="#2-4-quot-context-gpu-h-quot-not-found" class="headerlink" title="2.4 &quot;context_gpu.h&quot; not found."></a>2.4 &quot;context_gpu.h&quot; not found.</h3><h4 id="Details-3"><a href="#Details-3" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p><p>I forgot to record the error messages, but it should be obvious that some header files(not just <code>context_gpu.h</code>) are missing.</p><h4 id="Cause-3"><a href="#Cause-3" class="headerlink" title="Cause"></a>Cause</h4><p>This time it&#39;s the include path not recognized...</p><h4 id="Solution-3"><a href="#Solution-3" class="headerlink" title="Solution"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">include_directories</span>(<span class="string">"/path/to/pytorch/torch/lib/include"</span>)</span><br></pre></td></tr></table></figure></p><h3 id="2-5-This-file-was-generated-by-a-newer-version-of-protoc-which-is-incompatible-with-your-Protocol-Buffer-headers-Please-update-your-headers"><a href="#2-5-This-file-was-generated-by-a-newer-version-of-protoc-which-is-incompatible-with-your-Protocol-Buffer-headers-Please-update-your-headers" class="headerlink" title="2.5 This file was generated by a newer version of protoc which is incompatible with your Protocol Buffer headers. Please update your headers."></a>2.5 This file was generated by a newer version of protoc which is incompatible with your Protocol Buffer headers. Please update your headers.</h3><h4 id="Details-4"><a href="#Details-4" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p><p>Main error message:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/path/to/pytorch/torch/lib/include/caffe2/proto/caffe2.pb.h:12:2: error: #error This file was generated by a newer version of protoc which is</span><br><span class="line">#error This file was generated by a newer version of protoc which is</span><br><span class="line">^</span><br><span class="line">/path/to/pytorch/torch/lib/include/caffe2/proto/caffe2.pb.h:13:2: error: #error incompatible with your Protocol Buffer headers. Please update</span><br><span class="line">#error incompatible with your Protocol Buffer headers. Please update</span><br><span class="line">^</span><br><span class="line">/path/to/pytorch/torch/lib/include/caffe2/proto/caffe2.pb.h:14:2: error: #error your headers.</span><br><span class="line">#error your headers.</span><br><span class="line">^</span><br></pre></td></tr></table></figure></p><h4 id="Cause-4"><a href="#Cause-4" class="headerlink" title="Cause"></a>Cause</h4><p>If you only have a protobuf higher than v3.6.1, this should not happen. Check if you have multiple protobufs installed from different sources. (In my case, there was a protobuf v3.2.0 installed with <code>apt-get</code> earlier)</p><h4 id="Solution-4"><a href="#Solution-4" class="headerlink" title="Solution"></a>Solution</h4><p>I can&#39;t provide an exact solution. Please try<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">which protoc</span><br></pre></td></tr></table></figure></p><p>and see where protobuf is installed. If this shows the protobuf you installed with Anaconda, remove it completely and try this again. Since DensePose tells you that you have an older version of protobuf, you should be able to locate one. After finding it, remove it or upgrade it to v3.6.1 or higher. I would prefer installing protobuf from source <a href="https://github.com/protocolbuffers/protobuf/releases" target="_blank" rel="noopener">here</a>. It&#39;s not so painful as installing DensePose.</p><h3 id="2-6-quot-mkl-cblas-h-quot-not-found"><a href="#2-6-quot-mkl-cblas-h-quot-not-found" class="headerlink" title="2.6 &quot;mkl_cblas.h&quot; not found."></a>2.6 &quot;mkl_cblas.h&quot; not found.</h3><h4 id="Details-5"><a href="#Details-5" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p><p>I forgot to record the error messages, but it should be obvious too.</p><h4 id="Cause-5"><a href="#Cause-5" class="headerlink" title="Cause"></a>Cause</h4><p>Intel Math Kernel Library was turned on but not found. (Why is it enabled when I didn&#39;t even install it???)</p><h4 id="Solution-5"><a href="#Solution-5" class="headerlink" title="Solution"></a>Solution</h4><p>Install Intel Math Kernel Library <a href="https://software.intel.com/en-us/mkl/choose-download/linux" target="_blank" rel="noopener">here</a> and add <code>/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/include</code> to <code>C_PATH</code> environment variable:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export CPATH=$CPATH:/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/include</span><br></pre></td></tr></table></figure></p><p>The exact path may vary according to the MKL version and your configuration.<br>Maybe try <code>find / -name mkl_cblas.h</code> to make sure of its location after the installation.</p><p>Adding the path to CMakeLists.txt should also be helpful, but I didn&#39;t test it:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">include_directories(&quot;/opt/intel/compilers_and_libraries_2019.1.144/Linux/mkl/include&quot;)</span><br></pre></td></tr></table></figure></p><h3 id="2-7-GetSingleArgument’-is-not-a-member-of-‘caffe2-PoolPointsInterpOp’"><a href="#2-7-GetSingleArgument’-is-not-a-member-of-‘caffe2-PoolPointsInterpOp’" class="headerlink" title="2.7 GetSingleArgument’ is not a member of ‘caffe2::PoolPointsInterpOp’"></a>2.7 GetSingleArgument<float>’ is not a member of ‘caffe2::PoolPointsInterpOp<t, context>’</t,></float></h3><h4 id="Details-6"><a href="#Details-6" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>make ops</code>.<br>Main error message:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">/path/to/pytorch/caffe2/operators/accumulate_op.h: In constructor ‘caffe2::AccumulateOp&lt;T, Context&gt;::AccumulateOp(const caffe2::OperatorDef&amp;, caffe2::Workspace*)’:</span><br><span class="line">/path/to/pytorch/caffe2/operators/accumulate_op.h:13:187: error: ‘GetSingleArgument&lt;<span class="built_in">float</span>&gt;’ is not a member of ‘caffe2::AccumulateOp&lt;T, Context&gt;’</span><br><span class="line">   AccumulateOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class="line">                                                                                                                                                                                           ^                        </span><br><span class="line">/path/to/pytorch/caffe2/operators/elementwise_ops.h: In constructor ‘caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;::BinaryElementwiseWithArgsOp(const caffe2::OperatorDef&amp;, caffe2::Workspace*)’:</span><br><span class="line">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:189: error: ‘GetSingleArgument&lt;bool&gt;’ is not a member of ‘caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;’</span><br><span class="line">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class="line">                                                                                                                                                                                             ^                       </span><br><span class="line">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:272: error: ‘GetSingleArgument&lt;int&gt;’ is not a member of ‘caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;’</span><br><span class="line">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class="line">                                                                                                                                                                                                                                                                                ^                      </span><br><span class="line">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:350: error: ‘GetSingleArgument&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;’ is not a member of ‘caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;’</span><br><span class="line">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br></pre></td></tr></table></figure></p><h4 id="Cause-6"><a href="#Cause-6" class="headerlink" title="Cause"></a>Cause</h4><p>I&#39;m not sure. Could be that <code>GetSingleArgument()</code> is defined elsewhere?</p><h4 id="Solution-6"><a href="#Solution-6" class="headerlink" title="Solution"></a>Solution</h4><p>Modify <code>/path/to/densepose/detectron/ops/pool_points_interp.h</code>. Change <code>OperatorBase::GetSingleArgument&lt;float&gt;</code> to <code>this-&gt;template GetSingleArgument&lt;float&gt;</code></p><p>(Thanks to badpx@Github: <a href="https://github.com/facebookresearch/DensePose/pull/137/commits/51389c6a02173a25e9429825db452beb5e1cf3be" target="_blank" rel="noopener">https://github.com/facebookresearch/DensePose/pull/137/commits/51389c6a02173a25e9429825db452beb5e1cf3be</a>) </p><h3 id="2-8-fatal-error-caffe2-utils-threadpool-ThreadPool-h-No-such-file-or-directory"><a href="#2-8-fatal-error-caffe2-utils-threadpool-ThreadPool-h-No-such-file-or-directory" class="headerlink" title="2.8 fatal error: caffe2/utils/threadpool/ThreadPool.h: No such file or directory"></a>2.8 fatal error: caffe2/utils/threadpool/ThreadPool.h: No such file or directory</h3><h4 id="Details-7"><a href="#Details-7" class="headerlink" title="Details"></a>Details</h4><p>Occurs when running &quot;make ops&quot;.</p><p>Main error message:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/path/to/pytorch/torch/lib/include/caffe2/core/workspace.h:19:48: fatal error: caffe2/utils/threadpool/ThreadPool.h: No such file or directory</span><br></pre></td></tr></table></figure></p><p>This should only happen when your Caffe2 is installed with Anaconda.</p><h3 id="Cause-7"><a href="#Cause-7" class="headerlink" title="Cause"></a>Cause</h3><p>If your Caffe2 is installed with Anaconda, these files may not be found anywhere in the Caffe2 directory, or in your hard disk at all.</p><h3 id="Solution-7"><a href="#Solution-7" class="headerlink" title="Solution"></a>Solution</h3><p>In <a href="https://github.com/facebookresearch/DensePose/issues/152" target="_blank" rel="noopener">Anikily@Github</a>&#39;s case, downloading Caffe2 source code and add its path to DensePose&#39;s include directories will work:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:pytorch/pytorch.git</span><br></pre></td></tr></table></figure><p>and add one line in the beginning of DensePose/CMakeLists.txt:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">include_directories(&quot;/path/to/pytorch&quot;)</span><br></pre></td></tr></table></figure></p><p>The directory you include here should contain caffe2/utils/threadpool/ThreadPool.h and all the others.</p><p>I don&#39;t think this issue should be solved this way, but I&#39;m sure that these files couldn&#39;t be found anywhere else. If anyone finds a better solution, please comment here to help the others.</p><h3 id="2-9-Undefined-symbol-ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE"><a href="#2-9-Undefined-symbol-ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE" class="headerlink" title="2.9 Undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE"></a>2.9 Undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE</h3><h4 id="Details-8"><a href="#Details-8" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>python detectron/tests/test_zero_even_op.py</code>.</p><p>Main error message:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OSError: /path/to/densepose/build/libcaffe2_detectron_custom_ops_gpu.so: undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE</span><br></pre></td></tr></table></figure></p><h4 id="Cause-8"><a href="#Cause-8" class="headerlink" title="Cause"></a>Cause</h4><p>WTF is this!???<br>As can be seen, this symbol has something to do with Google, and protobuf.<br>I guess this is caused by a different protobuf version. Good news is that a proper version of protobuf was also built with Caffe2, so why not tell this to DensePose?</p><h4 id="Solution-8"><a href="#Solution-8" class="headerlink" title="Solution"></a>Solution</h4><p>In <code>/path/to/densepose/CMakeLists.txt</code>, Add a few lines in the beginning:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">add_library</span>(libprotobuf STATIC IMPORTED) </span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(PROTOBUF_LIB <span class="string">"/path/to/pytorch/torch/lib/libprotobuf.a"</span>) </span><br><span class="line"></span><br><span class="line"><span class="keyword">set_property</span>(TARGET libprotobuf PROPERTY IMPORTED_LOCATION <span class="string">"$&#123;PROTOBUF_LIB&#125;"</span>)</span><br></pre></td></tr></table></figure></p><p>You can find two <code>target_link_libraries</code> lines in this file(they are not adjacent):<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">target_link_libraries</span>(caffe2_detectron_custom_ops caffe2_library) </span><br><span class="line"><span class="keyword">target_link_libraries</span>(caffe2_detectron_custom_ops_gpu caffe2_gpu_library)</span><br></pre></td></tr></table></figure></p><p>Edit the two lines, adding a &quot;libprotobuf&quot; at the end to each of them:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">target_link_libraries</span>(caffe2_detectron_custom_ops caffe2_library libprotobuf) </span><br><span class="line"><span class="keyword">target_link_libraries</span>(caffe2_detectron_custom_ops_gpu caffe2_gpu_library libprotobuf)</span><br></pre></td></tr></table></figure></p><p>Then run <code>make ops</code> again, and <code>python detectron/tests/test_zero_even_op.py</code> again.</p><p>(Thanks to hyounsamk@Github: <a href="https://github.com/facebookresearch/DensePose/issues/119" target="_blank" rel="noopener">https://github.com/facebookresearch/DensePose/issues/119</a>)</p><p>After fixing this issue, my DensePose passed tests and was running flawlessly. If any more issues remain, don&#39;t hesitate to comment here~</p><h3 id="2-10-Undefined-symbol-ZN6caffe219CPUOperatorRegistryB5cxx11Ev"><a href="#2-10-Undefined-symbol-ZN6caffe219CPUOperatorRegistryB5cxx11Ev" class="headerlink" title="2.10 Undefined symbol: _ZN6caffe219CPUOperatorRegistryB5cxx11Ev"></a>2.10 Undefined symbol: _ZN6caffe219CPUOperatorRegistryB5cxx11Ev</h3><h4 id="Details-9"><a href="#Details-9" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>python detectron/tests/test_zero_even_op.py</code>, with Caffe2 installed with Anaconda.</p><p>Main error message:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OSError: /path/to/densepose/build/libcaffe2_detectron_custom_ops_gpu.so: undefined symbol: _ZN6caffe219CPUOperatorRegistryB5cxx11Ev</span><br></pre></td></tr></table></figure></p><h4 id="Cause-9"><a href="#Cause-9" class="headerlink" title="Cause"></a>Cause</h4><p>As can be seen from the messy undefined symbol, this should have something to do with Caffe2 and probably CXX11(oh really???).</p><p>Run <code>ldd -r /path/to/densepose/build/libcaffe2_detectron_custom_ops.so</code> and the one or several undefined symbols with similar names will be shown, which should have been defined in <code>libcaffe2.so</code>. After running <code>strings -a /path/to/pytorch/torch/lib/libcaffe2.so | grep _ZN6caffe219CPUOperator</code>, a few similar symbols (two, in my case) would come up, but are different from the one undefined - <code>&quot;B5cxx11&quot;</code> is missing.</p><p>Why does DensePose want to find a symbol with <code>&quot;B5cxx11&quot;</code>? Who added this suffix?<br>It should be our GCC who did it when compiling DensePose with C++11 standard!</p><p>To find which version of GCC was Caffe2 built by, run <code>strings -a /path/to/pytorch/torch/lib/libcaffe2.so | grep GCC:</code>.<br>In my case, the output is:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GCC: (GNU) 4.9.2 20150212 (Red Hat 4.9.2-6)</span><br></pre></td></tr></table></figure></p><p>Oh? It seems that Caffe2 developers are Red Hat lovers!<br>The Caffe2 installed with Anaconda was built by GCC 4.9.2, which had a slightly different standard on naming symbols.</p><h4 id="Solution-9"><a href="#Solution-9" class="headerlink" title="Solution"></a>Solution</h4><p>The simpliest way out is to turn to GCC 4.9.2 for building DensePose, too.<br>Otherwise, maybe also consider compiling Caffe2/PyTorch from source code?</p><p>(Many thanks to Johnqczhang@Github: <a href="https://github.com/linkinpark213/linkinpark213.github.io/issues/12" target="_blank" rel="noopener">https://github.com/linkinpark213/linkinpark213.github.io/issues/12</a>)</p><div align="center"><br>    <img src="/images/densepose-ms/ican.jpg" width="15%" height="15%" alt="I just can!"><br></div><h2 id="0-Motivation"><a href="#0-Motivation" class="headerlink" title="0 Motivation"></a>0 Motivation</h2><p>Starting from this post, I decide to keep a record (tag: MineSweeping) of the issues I meet while working with environments and also their solutions. </p><p>Doing configurations in order to run others&#39; code may be a difficult task, and is sometimes depressing, since various issues could arise, and the it&#39;s impossible for the authors to keep providing solutions for every user in the community. What&#39;s worse, after fixing some problems with a lot of struggle, one may have to waste the same amount of time on the same issue the next time he/she run it again. That&#39;s why I decide to keep this record: to avoid wasting time twice, while also helping others deal with problems if possible.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Update in October 2019:&lt;/p&gt;
&lt;p&gt;DensePose has been re-implemented with the brand-new object detection framework &lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Detectron2&lt;/a&gt;, which is based on PyTorch and much easier to install and use (You don&amp;#39;t have to manually compile Caffe2)&lt;br&gt;I strongly recommend that you check out the new official DensePose code at &lt;a href=&quot;https://github.com/facebookresearch/detectron2/tree/master/projects/DensePose&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/facebookresearch/detectron2/tree/master/projects/DensePose&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;DensePose is a great work in real-time human pose estimation, which is based on Caffe2 and Detectron framework. It extracts dense human body 3D surface based on RGB images. The installation instructions are provided &lt;a href=&quot;https://github.com/facebookresearch/DensePose/blob/master/INSTALL.md&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;During my installation process, these are the problems that took me some time to tackle. I spent on week to finally figure out solutions to all the issues. So lucky of me not to give up too early...&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;&lt;br&gt;    &lt;img src=&quot;/images/densepose-ms/facebook.jpg&quot; width=&quot;15%&quot; height=&quot;15%&quot; alt=&quot;Greetings from Facebook AI Research&quot;&gt;&lt;br&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://linkinpark213.com/tags/Deep-Learning/"/>
    
      <category term="MineSweeping" scheme="http://linkinpark213.com/tags/MineSweeping/"/>
    
  </entry>
  
  <entry>
    <title>Hello New World!</title>
    <link href="http://linkinpark213.com/2018/07/26/hello-osaka/"/>
    <id>http://linkinpark213.com/2018/07/26/hello-osaka/</id>
    <published>2018-07-26T01:12:52.000Z</published>
    <updated>2018-11-18T05:44:45.857Z</updated>
    
    <content type="html"><![CDATA[<p><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.min.css"></p><p><link rel="stylesheet" type="text/css" href="/css/images//photo-waterfall.css"></p><script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><script src="https://unpkg.com/minigrid@3.1.1/dist/minigrid.min.js"></script><script src="/js/jquery.fancybox.min.js"></script><script src="/js/photo-waterfall.js"></script><p>Here are some photos that I took during my trip to Higashi-Osaka, Nara and Kyoto.</p><a id="more"></a><div class="ImageGrid"></div><script>photo_waterfall.init('/images/hello-osaka/');</script><script src="/js/photo-waterfall-carousel.js"></script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;/css/jquery.fancybox.min.css&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;/css/images//photo-waterfall.css&quot;&gt;&lt;/p&gt;
&lt;script src=&quot;https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;https://unpkg.com/minigrid@3.1.1/dist/minigrid.min.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;/js/jquery.fancybox.min.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;/js/photo-waterfall.js&quot;&gt;&lt;/script&gt;


&lt;p&gt;Here are some photos that I took during my trip to Higashi-Osaka, Nara and Kyoto.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Travel Gallery" scheme="http://linkinpark213.com/tags/Travel-Gallery/"/>
    
  </entry>
  
  <entry>
    <title>[PyTorch is better!] A Painless Tensorflow Basic Tutorial - Take ResNet-56 as an Example</title>
    <link href="http://linkinpark213.com/2018/04/29/diy-resnet/"/>
    <id>http://linkinpark213.com/2018/04/29/diy-resnet/</id>
    <published>2018-04-29T13:05:43.000Z</published>
    <updated>2019-03-17T06:27:21.325Z</updated>
    
    <content type="html"><![CDATA[<p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script></p><div lang="en-us"><br>Update in March 2019:<br><br>After TensorFlow developers introduced the APIs of Tensorflow 2.0 on Tensorflow Dev Summit 2019, I have made my decision to turn to PyTorch.<br><div align="center" class="figure"><br><img src="/images/tftutorial/wjz_en.gif" alt="真香！"><br></div><br></div><a id="more"></a><div lang="en-us"><br>TensorFlow is a powerful open-source deep learning framework, supporting various languages including Python. However, its APIs are far too complicated for a beginner in deep learning(especially those who are new to Python). In order to ease the pain of having to understand the mess of various elements in TensorFlow computation graphs, I made this tutorial to help beginners take the first bite of the cake.<br><br>ResNets are one of the greatest works in the deep learning field. Although they look scary with extreme depths, it&#39;s not a hard job to implement one. Now let&#39;s build one of the simplest ResNets - ResNet-56, and train it on the CIFAR-10 dataset.<br><br></div><br><div lang="zh-cn"><br>2019.3 更新:<br><br>Tensorflow Dev Summit上开发者介绍TF 2.0 API后， 我彻底下定了换用PyTorch的决心。<br><div align="center" class="figure"><br><img src="/images/tftutorial/wjz.gif" alt="真香！"><br></div><br></div><br><div lang="zh-cn"><br>TensorFlow是一个强大的开源深度学习软件库，它支持包括Python在内的多种语言。然而，由于API过于复杂（实际上还有点混乱），它往往使得一个深度学习的初学者（尤其是为此初学Python的那些）望而却步——老虎吃天，无从下口。为了减轻初学者不得不尝试理解TensorFlow中的大量概念的痛苦，我213今天带各位尝尝深度学习这片天的第一口。<br>ResNet是深度学习领域的一个重磅炸弹，尽管它们（ResNet有不同层数的多个模型）的深度看上去有点吓人，但实际上实现一个ResNet并不难。接下来，我们来实现一个较为简单的ResNet——ResNet-56，并在CIFAR-10数据集上训练一下，看看效果如何。<br></div><br><div align="center" class="figure"><br><img src="/images/tftutorial/oyo.gif" alt="Let&#39;s Rock!"><br></div><br><div lang="en-us"><br>First let&#39;s take a look at ResNet-56. It&#39;s proposed by Kaiming He et al., and is designed to confirm the effect of residual networks. It has 56 weighted layers, deep but simple. The structure is shown in the figure below:<br></div><br><div lang="zh-cn"><br>首先来看一下ResNet-56这个神经网络。它是何凯明等在ResNet论文中提出的、用于验证残差网络效果的一个相对简单的残差网络（尽管它很深，深度达到了56个权重层）。图示如下：<br></div><br><div align="center" class="figure"><br><img src="/images/tftutorial/resnet56.png" alt="ResNet-56" width="80%"><br><br>Fig. 1 The structure of ResNet-56<br></div><br><br><br><div lang="en-us"><br>Seems a little bit long? Don&#39;t worry, let&#39;s do this step by step.<br></div><br><div lang="zh-cn"><br>看起来有点长了是不是？别担心，我们一步一步来做。<br></div><h2 id="1-Ingredients"><a href="#1-Ingredients" class="headerlink" title="1 Ingredients"></a>1 Ingredients</h2><p>Python 3.6</p><p>TensorFlow 1.4.0</p><p>Numpy 1.13.3</p><p>OpenCV 3.2.0</p><p><a href="https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz" target="_blank" rel="noopener">CIFAR-10 Dataset</a></p><div lang="en-us"><br>Also prepare some basic knowledge on Python programming, digital image processing and convolutional neural networks. If you are already capable of building, training and validating your own neural networks with TensorFlow, you don&#39;t have to read this post.<br></div><br><div lang="zh-cn"><br>另外，请确保自己有一点点Python编程、数字图像处理和卷积神经网络的知识储备。如果你已经具备用TensorFlow自行搭建神经网络并进行训练、测试的能力，就不必阅读本文了。<br></div><h2 id="2-Recipe"><a href="#2-Recipe" class="headerlink" title="2 Recipe"></a>2 Recipe</h2><h3 id="2-0-Prepare-the-tools"><a href="#2-0-Prepare-the-tools" class="headerlink" title="2.0 Prepare the tools"></a>2.0 Prepare the tools</h3><div lang="en-us"><br>Prepare(import) the tools for our project, including all that I mentioned above. Like this :P<br></div><br><div lang="zh-cn"><br>准(i)备(m)所(p)需(o)工(r)具(t)，上一部分已提到过。如下：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> tensor_chain <span class="keyword">import</span> TensorChain</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>Wait... What&#39;s this? TensorChain? Another deep learning framework like TensorFlow?<br><br>Uh, nope. This is my own encapsulation of some TensorFlow APIs, for the sake of easing your pain. You&#39;ll only have to focus on &quot;what&#39;s what&quot; in the beginning. We&#39;ll look into my implementation of this encapsulation later, when you are clear how everything goes. Please <a href="/files/tensor_chain.py">download this file</a> and put it where your code file is, and import it.<br></div><br><div lang="zh-cn"><br>等等...最后这个是个什么鬼？ TensorChain？另一个深度学习框架吗？<br><br>呃...并不是。这个是我对一些TensorFlow API的封装，为了减轻你的痛苦才做的。作为初学者，你只需要关注用TensorFlow搭建网络模型的这个过程，分清东西南北。回头等你弄清了大体流程后，我们再来看这个的实现细节。请先下载<a href="/files/tensor_chain.py">这个文件</a>并把它与你的代码放在同一文件夹下，然后就可以import了。<br></div><h3 id="2-1-Decide-the-input"><a href="#2-1-Decide-the-input" class="headerlink" title="2.1 Decide the input"></a>2.1 Decide the input</h3><div lang="en-us"><br>Every neural network requires an input - you always have to identify the details of a question, before asking the computer to solve it. All of the variable, constant in TensorFlow are objects of type <em>tf.Tensors</em>. And the <em>tf.placeholder</em> of our input(s) is a special one. Images in CIFAR-10 dataset are RGB images(3 channels) of 32x32(really small), so our input should shaped like [32, 32, 3]. Also, we want to input a little <em>batch</em> of multiple images. Therefore, our input data should be an array of shape <em>[?, 32, 32, 3]</em>. Unknown dimension size can be marked as None, and it will be clear when we feed the model with the actual images. It&#39;s coded like this:<br></div><br><div lang="zh-cn"><br>每个神经网络都需要有输入——毕竟你想找电脑解决一些问题的话，你总得告诉它问题的一些细节吧？TensorFlow中所有的变量、常量都是<em>tf.Tensor</em>类型的对象，作为输入内容的占位符<em>tf.placeholder</em>也是（只不过比较特殊而已）。CIFAR-10数据集的图像都是32x32尺寸（好小哇）的RGB图像（RGB即彩色图像的三个通道），因此我们的输入给神经网络的内容将会像是[32, 32, 3]这个样子。另外呢，我们需要输入的是一个小<em>batch</em>（批）的图像，因此，输入网络的图像数据将会是一个<em>[?, 32, 32, 3]</em>的数组（也可以是numpy数组）。未知的维度大小用None代指就好，我们之后给模型喂实际图像batch时，它自然就清楚了。代码如下：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input_tensor = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br><em>Ground truth</em> data also need to be known in supervised learning, so we also have to define a placeholder for the ground truth data:<br></div><br><div lang="zh-cn"><br>监督学习中，正确标注的数据（英文为<em>ground truth</em>，目前貌似没有对这个名词的合理翻译）也是需要输入到模型中的。因此再给ground truth定义一个placeholder：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ground_truth = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>We want the label data to be in the one-hot encoding format, which means an array of length 10, denoting 10 classes. Only on one position is a &#39;1&#39;, and on other positions are &#39;0&#39;s.<br></div><br><div lang="zh-cn"><br>我们需要标记的数据呈One-Hot编码格式（又称为一位有效编码），意思是如果有10个类别，那么数组长度就是10，每一位代表一个类别。只有一个位置上是1（代表图片被分为这个类），其他位上都是0。<br></div><h3 id="2-2-Do-some-operations"><a href="#2-2-Do-some-operations" class="headerlink" title="2.2 Do some operations"></a>2.2 Do some operations</h3><div lang="en-us"><br>For now, let&#39;s use our TensorChain to build it fast. Under most circumstances that we may face, the computations are based on the input data or the result of the former computation, so our network(or say, the most of it) look more like a chain than a web. Every time we add some new operation(layer), we add it to our <em>TensorChain</em> object. Just remember to get the <em>output_tensor</em> of this object(denoting the output tensor of the last operation on the chain) when you need to ue native TensorFlow API.<br>The construction function of TensorChain class requires a Tensor object as the parameter, which is also the input tensor of this chain. As we mentioned earlier, all we have to do is add operations. See my ResNet-56 code:<br></div><br><div lang="zh-cn"><br>现在呢，我们先用TensorChain来快速盖楼。因为我们遇到的大多数情况下，所有的计算都是在输入数据或者这个计算的前一个计算结果基础上进行的，所以我们的网络（至少是它的绝大部分）会看起来像个链而不是所谓的网。每次我们添加一个新的运算（层），我们会把它加到这个独一无二的TensorChain对象。只要记得在使用原生TensorFlow API前把它的<em>output_tensor</em>属性（也就是这条链上最后一个运算的输出Tensor）取出来就好了。<br>TensorChain类的构造函数需要一个Tensor对象作为参数，这个对象也正是被拿来作为这个链的输入层。正如我们之前所说的，只要在这个对象上添加运算即可。写个ResNet-56，代码很简单：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">chain = TensorChain(input_tensor) \</span><br><span class="line">        .convolution_layer_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>, stride=<span class="number">2</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>, stride=<span class="number">2</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">        .flatten() \</span><br><span class="line">        .fully_connected_layer(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>This is it? Right, this is it! Isn&#39;t it cool? Didn&#39;t seem that high, huh? That&#39;s because I encapsulated that huge mess of weights and biases, only leaving a few parameters that decide the structure of the network. Later in this pose we&#39;ll talk about the actual work that these functions do.<br></div><br><div lang="zh-cn"><br>就这？没错呀，就这！稳不稳？似乎看起来也没56层那么高呀？毕竟这些函数被我封装得太严实了，只留出几个决定网络结构的几个参数供修改。这篇博客后边就会讲到这些函数究竟干了点什么事儿。<br></div><h3 id="2-3-Define-the-loss"><a href="#2-3-Define-the-loss" class="headerlink" title="2.3 Define the loss"></a>2.3 Define the loss</h3><div lang="en-us"><br>In supervised learning, you always have to tell the learning target to the model. To tell the model how to optimize, you have to let it know how, how much, on which direction should it change its parameters. This is done by using a loss function. Therefore, we need to define a loss function for our ResNet-56 model(which we designed for this classification problem) so that it will learn and optimize.<br>A commonly used loss function in classification problems is cross entropy. It&#39;s defined below:<br></div><br><div lang="zh-cn"><br>搞监督学习，总是要让模型按照“参考答案”去改的。要改就得让它知道怎么改、改多少、往什么方向改，这也就是<em>loss</em>（损失函数）的功劳。因此，像我们这个拿来做分类问题的ResNet-56，我们要给它定义一个损失函数来让它学习、优化。<br>分类问题上一个常用的损失函数是交叉熵。定义如下式：<br></div><br>$$C=-\frac{1}{n}\sum_x{y\ln a+(1-y)\ln(1-a)}$$<br><div lang="en-us"><br>in which \(y\) is the expected(or say correct) output and \(a\) is the actual output.<br>This seems a little bit complicated. But it&#39;s not a hard job to implement, since TensorFlow implemented it already! You can also try and implement it yourself within one line if you want. For now we use the pre-defined cross entropy loss function:<br></div><br><div lang="zh-cn"><br>其中\(y\)为期望输出（或者说参考答案），\(a\)为实际输出。<br>略复杂呀...这个用程序怎么写？其实也不难。。。毕竟TensorFlow都帮我们实现好啦！（有兴趣的话也可以自己尝试着写一下，同样一行代码即可搞定）现在你只需要来这么一句：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>and it returns a tf.Tensor that denotes an average of cross entropies(don&#39;t forget that this is a batch). As for the &#39;softmax&#39; before the &#39;cross_entropy&#39;, it&#39;s a function that project the data in an array to range 0~1, which allows us to do a comparison between our prediction and the ground truth(in one-hot code). The definition is simple too:\<br></div><br><div lang="zh-cn"><br>就可以创建一个表示交叉熵平均值（别忘了这可是一个batch）的Tensor了。至于cross_entropy前边的那个<em>softmax</em>呢，它的作用是把输入的数组内数据归一化，投射到0~1的范围内（实际上就是相当于把exp(数组各项的值)的当做频数，求出一个概率），这样子才能跟实际数据做一个比较。定义也比较简单：\<br></div><br>$$S_i=\frac{e^{V_i}}{\sum_j{e^{V_j}}}$$<br><div></div><h3 id="2-4-Define-the-train-op"><a href="#2-4-Define-the-train-op" class="headerlink" title="2.4 Define the train op"></a>2.4 Define the train op</h3><div lang="en-us"><br>Now we have the loss function. We&#39;ll have to tell its value to an <em>optimizer</em>, which make our model learn and optimize in order to minimize the loss value. Gradient Descent Optimizer, Adagrad Optimizer, Adam Optimizer and Momentum Optimizer are commonly used optimizers. Here we use an Adam Optimizer for instance. You&#39;re free to try any other one here. When<br></div><br><div lang="zh-cn"><br>现在误差函数已经有了，我们需要把它的值告诉一个优化器（<em>optimizer</em>），并让它去尽可能向着缩小误差函数值得方向努力。这样，模型才能去学习、优化。常用的优化器包括Gradient Descent Optimizer，Adagrad Optimizer，Adam Optimizer以及Momentum Optimizer等等等等。选择优化器时，我们需要给它一个初始的学习速率。这里我用了一个\(10^-3\)，如果需要提高准确率，可能后期微调还需要进一步减小。代码如下：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>Also, tell the optimizer that what the loss tensor is. The returned object is a train operation.<br></div><br><div lang="zh-cn"><br>当然还要告诉它要减小的损失函数是哪个Tensor，这个函数返回的是一个训练操作（<em>train op</em>，一种特殊的运算，或者说操作）：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train = optimizer.minimize(loss)</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>The neural network is finished. It&#39;s time to grab some data and train it.<br></div><br><div lang="zh-cn"><br>其实到这里为止，神经网络已经搭建好了。是时候搞点数据来训练它了。<br></div><h3 id="2-5-Feed-the-model-with-data-and-train-it"><a href="#2-5-Feed-the-model-with-data-and-train-it" class="headerlink" title="2.5 Feed the model with data, and train it!"></a>2.5 Feed the model with data, and train it!</h3><div lang="en-us"><br>Remember how we defined the placeholders? It&#39;s time to fetch some data that fits the placeholders and train it. See how CIFAR-10 dataset can be fetched on its <a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">website</a>.<br></div><br><div lang="zh-cn"><br>还记得我们怎么定义那些placeholder吗？现在我们要把符合它们口径的数据灌进模型。那么来看一下CIFAR-10数据集<a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">官网</a>上是怎么描述的吧。它给了这么一段代码：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpickle</span><span class="params">(file)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    <span class="keyword">with</span> open(file, <span class="string">'rb'</span>) <span class="keyword">as</span> fo:</span><br><span class="line">        dict = pickle.load(fo, encoding=<span class="string">'bytes'</span>)</span><br><span class="line">    <span class="keyword">return</span> dict</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>The returned value <em>dict</em> is a Python dictionary. Every time we unpickle a file, a dictionary would be returned. Its &#39;data&#39; key leads to 10000 RGB images of size 32x32, which is stored in a [10000, 3072] array(3072=32<em>32</em>3, I guess you know how it&#39;s stored now). The &#39;label&#39; key leads to 10000 values in range 0~9. Obviously we have to reshape the data so as to fit it into the network model:<br></div><br><div lang="zh-cn"><br>返回值<em>dict</em>是一个字典（Python的dict类型）。每读一个batch文件（比如data_batch_1），就会返回这样一个字典，它的“data”键值是10000张32x32的RGB图像（数组维数居然是[10000, 3072]，而3072=10000x32x32x3！实际上就是直接把所有像素、所有通道的值罗列在这里了）；“label”键值是10000个0-9之间的整数（代表类别）。显然，为了让数据能够成功放进模型，还需要对它进行一点处理：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">batch = unpickle(DATA_PATH + <span class="string">'data_batch_&#123;&#125;'</span>.format(i))  <span class="comment"># 'i' is the loop variable</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the image data</span></span><br><span class="line">image_data = np.reshape(batch[<span class="string">b'data'</span>], (<span class="number">10000</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>), <span class="string">'F'</span>).astype(np.float32)   </span><br><span class="line">image_data = image_data / <span class="number">255</span>                            <span class="comment"># Cast range(0, 255) to range(0, 1)</span></span><br><span class="line">image_data = np.transpose(image_data, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>))      <span class="comment"># Exchange row and column</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the label data and convert into one-hot code</span></span><br><span class="line">label_data = batch[<span class="string">b'labels'</span>]</span><br><span class="line">new_label_data = np.zeros((<span class="number">10000</span>, <span class="number">10</span>))                   </span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    new_label_data[j][label_data[j]] = <span class="number">1</span></span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>The details for data processing are not covered here. Try doing step-by-step to see the results.<br>The <em>image_data</em> and <em>new_label_data</em> are contain 10000 pieces of data each. Let&#39;s divide them into 100 small batches(100 elements each, including image and label) and feed it into the model. Do this on all the 5 batch files:<br></div><br><div lang="zh-cn"><br>处理的细节不再赘述。你可以尝试一步一步运行来看看每一步的结果。<br>这样我们拿到的<em>image_data</em>和<em>new_label_data</em>都是长度为10000的大batch，我们把它们各自分成100份，每次取100个图像+标记数据来塞进模型。对全部5个大batch文件来一遍：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    session.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">100</span>): <span class="comment"># 10000 / BATCH_SIZE</span></span><br><span class="line">        <span class="comment"># Divide them and get one part</span></span><br><span class="line">        image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class="line">        label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Feed the model</span></span><br><span class="line">        session.run(train, feed_dict=&#123;</span><br><span class="line">            input_tensor: image_batch,</span><br><span class="line">            ground_truth: label_batch</span><br><span class="line">        &#125;)</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>A <em>session</em> - created with <em>tf.Session()</em> - is required every time we run a TensorFlow model, no matter when we&#39;re training it or evaluating it. The first time you run a model, you&#39;ll need to run <em>session.run(tf.global_variables_initializer())</em> to initialize the values of the TensorFlow variables defined previously.<br>When running <em>session.run()</em>, you must first decide a TensorFlow operation(or a list of operations) that you need. If its result is dependent on some actual data(which means that some data in one or more placeholders flow to this operation), it&#39;s also required that you feed it the actual data by adding a <em>feed_dict</em> parameter. For example, I&#39;m training this ResNet-56 model, in which a loss will be calculated with my <em>ground_truth</em> and the prediction result that comes from the <em>input_tensor</em>. Therefore, I&#39;ll have to give a value for each placeholder given above(format: &quot;placeholder name: corresponding data&quot;), and fold them in one Python dictionary.<br></div><br><div lang="zh-cn"><br>每次运行一个TensorFlow模型（无论是训练还是测试）时，都需要通过tf.Session()创建一个<em>session</em>。第一次运行模型（而不是载入之前保存的模型）时，需要使用<em>session.run(tf.global_variables_initializer())</em>来初始化之前定义的一些可训练的TensorFlow变量。<br>运行<em>session.run()</em>时，需要指定一个或一组你要执行的operation，作为这个函数唯一一个必要的参数。如果它的结果依赖于一些实际数据（也就是说在计算图中，一些数据会从placeholder流向这个operation），那么就需要通过填入<em>feed_dict</em>参数的值来填装训练或测试数据。以此模型为例，我在训练它时需要算误差函数值，这需要<em>ground_truth</em>数据和预测结果来计算，而预测结果又需要用输入图像<em>input_tensor</em>来计算得到。因此，我需要给这两个占位符分别给出对应的数据（格式：“占位符名：对应数据”），并把它们封在同一个Python字典中作为feed_dict参数的值。<br></div><br><div lang="en-us"><br>I&#39;m also interested in the loss function value in each iteration(which means feeding a batch of data and executing one forward-propagation and one back-propagation) in the training process. Therefore, what I&#39;ll fill in the parameter is not just the train op, but also the loss tensor. And the session.run() above should be modified to:<br></div><br><div lang="zh-cn"><br>然而呢，我还想看看每次迭代（即把一个batch送进去，执行一次正向传播与反向传播这个过程）中损失函数变成了多大，来监控一下训练的效果。这样，需要session.run()的就不仅是那个train运算，还要加上loss运算。将上边的session.run()部分改为：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[train_, loss_value] = session.run([train, loss],</span><br><span class="line">    feed_dict=&#123;</span><br><span class="line">        input_tensor: image_batch,</span><br><span class="line">        ground_truth: label_batch</span><br><span class="line">    &#125;)</span><br><span class="line">print(<span class="string">"Loss = &#123;&#125;"</span>.format(loss_value)</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>This is when the return value of session.run() becomes useful. Its value(s) - corresponding to the first parameter of run() - is/are the actual value(s) of the tensor(s) in the first parameter. In our example, <em>loss_value</em> is the actual output of the loss tensor. As for train_, we don&#39;t care what it is. Just add it to match the dimensions.<br></div><br><div lang="zh-cn"><br>这时候，session.run()函数的返回值就有意义了。它与第一个参数的内容一一对应，分别是该参数中各个operation的实际输出值。像这个例子里边，<em>loss_value</em>接收的就是loss运算的输出内容。而train运算的输出我们并不关心，但是为了保证参数维度数与返回值一致，用一个train_变量来接收而已（实际上它的值是None）。<br></div><br><div lang="en-us"><br>Actually, one epoch(train the model once with the whole dataset) is not enough for the model to fully optimize. I trained this model for 40 epochs and added some loop variables to display the result. You can see my code and my output below. It&#39;s highly recommended that you train this with a high-performance GPU, or it would be a century before you train your model to a satisfactory degree.<br></div><br><div lang="zh-cn"><br>实际上，一个epoch（把整个数据集都在模型里过一遍的周期）并不足以让模型充分学习。我把这个模型训练了40个epoch并且加了一些循环变量来输出结果。我的代码和结果如下。强烈建议用一个高性能GPU训练（如果手头没有，可以租一个GPU服务器），不然等别人把毕设论文逗写完的时候，你还在训练就很尴尬了。<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> tensor_chain <span class="keyword">import</span> TensorChain</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpickle</span><span class="params">(file)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(file, <span class="string">'rb'</span>) <span class="keyword">as</span> fo:</span><br><span class="line">        dict = pickle.load(fo, encoding=<span class="string">'bytes'</span>)</span><br><span class="line">    <span class="keyword">return</span> dict</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    input_tensor = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>])</span><br><span class="line">    ground_truth = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">    chain = TensorChain(input_tensor) \</span><br><span class="line">            .convolution_layer_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>, stride=<span class="number">2</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>, stride=<span class="number">2</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">            .flatten() \</span><br><span class="line">            .fully_connected_layer(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    prediction = chain.output_tensor</span><br><span class="line">    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))</span><br><span class="line"></span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">    train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">        session.run(tf.global_variables_initializer())</span><br><span class="line">        iteration = <span class="number">1</span></span><br><span class="line">        BATCH_SIZE = <span class="number">100</span></span><br><span class="line">        DATA_PATH = <span class="string">'../data/cifar-10-batches-py/'</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">41</span>):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">                data = unpickle(DATA_PATH + <span class="string">'data_batch_&#123;&#125;'</span>.format(i))</span><br><span class="line">                image_data = np.reshape(data[<span class="string">b'data'</span>], (<span class="number">10000</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>), <span class="string">'F'</span>).astype(np.float32)</span><br><span class="line">                image_data = image_data / <span class="number">255</span></span><br><span class="line">                image_data = np.transpose(image_data, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">                label_data = data[<span class="string">b'labels'</span>]</span><br><span class="line">                new_label_data = np.zeros((<span class="number">10000</span>, <span class="number">10</span>))</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">                    new_label_data[j][label_data[j]] = <span class="number">1</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(int(<span class="number">10000</span> / BATCH_SIZE)):</span><br><span class="line">                    image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class="line">                    label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class="line">                    [train_, loss_] = session.run(</span><br><span class="line">                        [train, loss],</span><br><span class="line">                        feed_dict=&#123;</span><br><span class="line">                            input_tensor: image_batch,</span><br><span class="line">                            ground_truth: label_batch</span><br><span class="line">                        &#125;)</span><br><span class="line">                    iteration += <span class="number">1</span></span><br><span class="line">                    print(<span class="string">"Epoch &#123;&#125;, Iteration &#123;&#125;, Loss = &#123;&#125;"</span>.format(epoch, iteration, loss_))</span><br></pre></td></tr></table></figure><br><br><div align="center" class="figure"><br><img src="/images/tftutorial/train.png" alt="Training result" width="40%"><br><br>Fig. 2 Training result: cross entropy has dropped below 0.5<br></div><br><div></div><h3 id="2-6-Conclusion"><a href="#2-6-Conclusion" class="headerlink" title="2.6 Conclusion"></a>2.6 Conclusion</h3><div lang="en-us"><br>In a word, building &amp; training neural network models with TensorFlow involves the following steps:<br><br>1. Decide the <em>input tensor</em><br><br>2. Add operations(<em>op</em>s) based on existing tensors<br><br>3. Define the <em>loss</em> tensor, just like other tensors<br><br>4. Select an <em>optimizer</em> and define the <em>train</em> op<br><br>5. Process <em>data</em> and feed the model with them<br></div><br><div lang="zh-cn"><br>总而言之，用TensorFlow建立、训练一个神经网络模型分以下几步：<br><br>1. 定义<em>输入</em>Tensor<br><br>2. 在已有的Tensor上添加运算（<em>op</em>）<br><br>3. 像之前添加的那些运算一样，定义<em>损失</em>Tensor<br><br>4. 选择一个<em>优化器</em>并定义<em>训练</em>操作<br><br>5. 把<em>数据</em>处理为合适的shape，并喂进模型训练<br></div><h2 id="3-A-Closer-Look"><a href="#3-A-Closer-Look" class="headerlink" title="3 A Closer Look"></a>3 A Closer Look</h2><div lang="en-us"><br>Wait, it&#39;s too late to leave now!<br>TensorChain saved you from having to deal with a mess of TensorFlow classes and functions. Now it&#39;s time that we take a closer look at how TensorChain is implemented, thus understanding the native TensorFlow APIs.<br></div><br><div lang="zh-cn"><br>别走呢喂！<br>TensorChain让你不至于面对TensorFlow中乱糟糟的类型和函数而不知所措被水淹没。现在是时候近距离观察一下TensorChain是如何实现的，以便理解TensorFlowAPI了。<br></div><h3 id="3-1-TensorFlow-variables"><a href="#3-1-TensorFlow-variables" class="headerlink" title="3.1 TensorFlow variables"></a>3.1 TensorFlow variables</h3><div lang="en-us"><br>Let&#39;s begin with TensorFlow variables. Variables in TensorFlow are similar to variables in C, Java or any other strong typed programming languages - they have a type, though not necessarily explicitly decided upon definition. Usually them will change as the training process goes on, getting close to a best value.<br>The most commonly used variables in TensorFlow are weights and biases. I guess that you have seen formulae like:<br></div><br><div lang="zh-cn"><br>先说TensorFlow的变量。TensorFlow的变量和C，Java以及其他强类型语言类似——都有一个类型，尽管不一定在它的定义时就显式地声明。通常它们会随着训练的进行而不断变化，达到一个最佳的值附近。<br>TensorFlow中最常用的变量就是weights和biases（权重和偏置）。想必你应该见过这样的式子吧：<br></div><br>$$y=Wx+b$$<br><div lang="en-us"><br>The \(W\) here is the weight, and the \(b\) here is the bias. When implementing some common network layers, they two are always used as the parameters in the layers. For instance, at the very beginning of our ResNet-56, we had a 3x3 sized convolution layer with 16 channels. Its implementation in TensorChain is:<br></div><br><div lang="zh-cn"><br>这里\(W\)就是权重，\(b\)就是偏置。在定义一些常用的层时，我们往往也是用这两个变量作为这些层中的参数。比如说，在我们ResNet-56最开始，我们用到了一个3x3大小、16个通道的卷积层，TensorChain中，它的实现如下：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolution_layer_2d</span><span class="params">(self, filter_size: int, num_channels: int, stride: int = <span class="number">1</span>, name: str = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                         disable_log: bool = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Add a 2D convolution layer</span></span><br><span class="line"><span class="string">    :param filter_size: Filter size(width and height) for this operation</span></span><br><span class="line"><span class="string">    :param num_channels: Channel number of this filter</span></span><br><span class="line"><span class="string">    :param stride: Stride for this convolution operation</span></span><br><span class="line"><span class="string">    :param name: The name of the tensor</span></span><br><span class="line"><span class="string">    :param disable_log: Set it True if you don't want this layer to be recorded</span></span><br><span class="line"><span class="string">    :return: This object itself</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    filter = self._weights([filter_size, filter_size, self.num_channels, num_channels], layer_name=name,</span><br><span class="line">                           suffix=<span class="string">'filter'</span>)</span><br><span class="line">    bias = self._bias([num_channels], layer_name=name)</span><br><span class="line">    self.num_channels = num_channels</span><br><span class="line">    self.output_tensor = tf.nn.conv2d(self.output_tensor, filter,</span><br><span class="line">                                      [<span class="number">1</span>, stride, stride, <span class="number">1</span>], <span class="string">'SAME'</span>, name=name)</span><br><span class="line">    self.output_tensor = tf.add(self.output_tensor, bias)</span><br><span class="line">    self._log_layer(</span><br><span class="line">        <span class="string">'2D Convolution layer, filter size = &#123;&#125;x&#123;&#125;, stride = &#123;&#125;, &#123;&#125; channels'</span>.format(filter_size, filter_size,</span><br><span class="line">                                                                                     stride,</span><br><span class="line">                                                                                     num_channels),</span><br><span class="line">        disable=disable_log)</span><br><span class="line">    <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>See? On line 16, we used a <em>tf.nn.conv2d()</em> function, the parameters of which are <em>input</em>, <em>filter</em>, <em>strides</em>, <em>padding</em>, etc. As can be guessed from the names, this function does a convolution operation with out input and the weights(the convolution <em>filter</em> here). A <em>bias</em> is added to the result as the final output. There are also many people who argue that the bias here is meaningless and should removed. One line of code is sufficient for defining a variable:<br></div><br><div lang="zh-cn"><br>看见了吧？16行上，我们用了一个<em>tf.nn.conv2d()</em>函数，它的参数是<em>input</em>，<em>filter</em>，<em>strides</em>，<em>padding</em>等等。顾名思义，这个函数就是用我们定义的权重Tensor<em>filter</em>（在这里称之为卷积核）来与这一层的输入input做了一次运算。运算的结果加上了偏置Tensor<em>bias</em>，作为这个卷积层的最终输出。很多人认为这里的偏置bias意义不明，因此他们在卷积之后没有加上这样的一个bias变量。定义一个变量只需要这样一个语句：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable(tf.truncated_normal(shape, stddev=sigma), dtype=tf.float32, name=suffix)</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>To define weight or bias variables, create a <em>tf.Variable</em> object. Usually you&#39;ll need to give the <em>initial_value</em> which also decides the shape of this tensor. <em>tf.truncated_normal()</em> and <em>tf.constant()</em> are usually used as the initial values. Also, other APIs - function <em>tf.get_variable()</em> and package <em>tf.initializers</em> are frequently used when using some more methods for initialization. I strongly recommend that you try using these APIs yourself.<br></div><br><div lang="zh-cn"><br>要定义权重或者偏置变量，请创建一个<em>tf.Variable</em>对象。通常情况下，你会需要给出<em>initial_value</em>（TF变量的初始值），这将顺便定义了这个变量的shape（因为初始值的shape是确定的）。另外，一些新的API——<em>tf.get_variable()</em>函数和<em>tf.initializers</em>包也常用与的参数初始化，以实现更多样的初始化方法。我强烈建议自己动手实践一下，试一试这些API。<br></div><h3 id="3-2-Tensors-and-operations"><a href="#3-2-Tensors-and-operations" class="headerlink" title="3.2 Tensors and operations"></a>3.2 Tensors and operations</h3><div lang="en-us"><br>Going on with the parameters of the <em>tf.nn.conv2d()</em> function. The required parameters also include <em>strides</em> and <em>padding</em>. You should have already learned about what strides mean in convolution, and I&#39;ll only talk about their formats. <em>strides</em> require a 1-D vector with a length of 4, like [1, 2, 2, 1]. The 1st and the 4th number is always 1(in order to match dimensions with the input), while the 2nd and the 3rd means the vertical stride and the horizonal stride.<br>The 4th parameter <em>padding</em> is a little bit different from its definition in convolution operation. It requires &#39;SAME&#39; of &#39;VALID&#39;, denoting &#39;with&#39; or &#39;without&#39; zero paddings. When it&#39;s &#39;SAME&#39;, zero padding is introduced to make the shapes match as needed, equally on every side of the input map.<br></div><br><div lang="zh-cn"><br>接着说<em>tf.nn.conv2d()</em>函数的参数。需要的参数还包括<em>strides</em>和<em>padding</em>。你应该在了解卷积运算时学过stride（步幅）含义，我只说一下它的格式吧。<em>strides</em>参数需要是一个1维、长度为4的向量。第一位和第四位永远都是1，第二位和第三位分别是竖直方向和水平方向的步幅长。维持这个形式只是为了与输入的数据维度匹配，因此API看起来非常蹩脚。<br>第四个参数<em>padding</em>和卷积运算里的padding不太相同。它的值只能是&#39;SAME&#39;或&#39;VALID&#39;，分别代表“带”和“不带”零补全。如果是&#39;SAME&#39;的话，函数会均匀地在图像的上下左右使用零补全来使得运算结果与之前尽可能保持一致。（stride&gt;1时有可能输出尺寸不是正好等于原来的尺寸/stride，因为补全问题）<br></div><br><div lang="en-us"><br>tf.nn.conv2d() is just an example of TensorFlow <em>operations</em>. Other functions like <em>tf.matmul()</em>, <em>tf.reduce_mean()</em>, <em>tf.global_variables_initializer()</em>, <em>tf.losses.softmax_cross_entropy()</em>, <em>tf.truncated_normal()</em> are all operations. Operation functions return tensors(<em>tf.truncated_normal</em> also return a tensor, a tensor with initializers).<br></div><br><div lang="zh-cn"><br>tf.nn.conv2d()只是TensorFlow运算（<em>operation</em>）的一个例子。其他例如<em>tf.matmul()</em>，<em>tf.reduce_mean()</em>，<em>tf.nn.relu()</em>，<em>tf.batch_normalization()</em>，<em>tf.global_variables_initializer()</em>，<em>tf.losses.softmax_cross_entropy()</em>，<em>tf.truncated_normal()</em>之类的函数也都是TensorFlow的运算。TensorFlow的运算函数会返回一个Tensor对象（包括<em>tf.truncated_normal()</em>也是！它只不过返回的是一个带初始化器的Tensor而已）。<br></div><br><div lang="en-us"><br>All the functions in the TensorChain class are based on the most basic TensorFlow operations and variables. After learning about these basic TensorFlow concepts, actually you can already abandon TensorChain, go and try implementing your own neural networks yourself!<br></div><br><div lang="zh-cn"><br>TensorChain类中的所有成员函数都是基于最基本的TensorFlow运算和变量的。实际上，了解了这些，你现在已经可以抛开TensorChain的束缚，去尝试实现你自己的神经网络了！<br></div><h2 id="4-Spices"><a href="#4-Spices" class="headerlink" title="4 Spices"></a>4 Spices</h2><div lang="en-us"><br>I&#39;m not joking just now! But I know that there are a lot of things that you still don&#39;t understand about using TensorFlow - like &quot;how do I visualize my computation graph&quot;, &quot;how do I save/load my model to/from files&quot;, &quot;how do I record some tensors&#39; values while training&quot; or &quot;how do I view the loss curves&quot; - after all TensorFlow APIs are far more complicated than just building those nets. Those are also important techniques in your research. If you&#39;d rather ask me than spending some time experimenting, please go on with reading.<br></div><br><div lang="zh-cn"><br>我，我真没开玩笑！但是我知道关于如何使用TensorFlow，你还有许许多多的问题，好比“如何可视化地查看我的计算图结构”、“如何存储/读取模型文件”、“如何记录训练过程中某些Tensor的真实值”、“如何查看损失函数的变化曲线”——毕竟TensorFlow的API太复杂了，远比搭建神经网络那点函数复杂得多。上边说的那些是你使用TensorFlow研究过程中的重要技巧。如果你愿意听我讲而不想花些时间尝试的话，请继续读下去。<br></div><h3 id="4-1-Saving-and-loading-your-model"><a href="#4-1-Saving-and-loading-your-model" class="headerlink" title="4.1 Saving and loading your model"></a>4.1 Saving and loading your model</h3><div lang="en-us"><br>The very first thing that you may want to do - after training a network model with nice outcomes - would be saving it. Saving a model is fairly easy - just use a <em>tf.train.Saver</em> object. See my code below:<br></div><br><div lang="zh-cn"><br>训练出一个看起来输出还不错的神经网络模型后你想做的第一件事恐怕就是把它存下来了吧？保存模型其实非常简单：只要用一个<em>tf.train.Saver</em>类的对象。代码示例：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    <span class="comment"># Train it for some iterations</span></span><br><span class="line">    <span class="comment"># Train it for some iterations</span></span><br><span class="line">    <span class="comment"># Train it for some iterations</span></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    saver.save(session, <span class="string">'models/model.ckpt'</span>)</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>I saved my model and variable values to &#39;models/model.ckpt&#39;. But actually, you&#39;ll find 3 files in the &#39;models&#39; directory - <em>model.ckpt.data-00000-of-00001</em>, <em>model.ckpt.meta</em> and <em>model.ckpt.index</em> - none of which is &#39;model.ckpt&#39;! That&#39;s because TensorFlow stores the graph structure separately from variables values. The <em>.meta</em> file describes the saved graph structure; the <em>.index</em> file records the mappings between tensor names and tensor metadata; and the <em>.data-00000-of-00001</em> file - which is always the biggest one - saves all the variable values. If you need the graph data together with the variable values to be loaded, use a Saver to load after creating a session:<br></div><br><div lang="zh-cn"><br>我把我的模型和变量值存到了&#39;models/model.ckpt&#39;文件里。但是！实际上在models目录里你会找到三个文件：<em>model.ckpt.data-00000-of-00001</em>，<em>model.ckpt.meta</em>和<em>model.ckpt.index</em>——哪个也不是model.ckpt呀？那是因为TensorFlow把计算图的结构和图中各种变量的值分开存放了。<em>.meta</em>文件描述计算图的结构；<em>.index</em>文件记录各个Tensor名称（是name属性，而不是变量名）与Tensor元信息之间的映射；<em>.data-00000-of-00001</em>文件往往是最大的一个，它存储的是各个TensorFlow变量的实际值。如果读取时需要把图结构和变量值都读进来，在session创建以后，同样用一个Saver来读取即可：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    saver.restore(session, <span class="string">'models/model.ckpt'</span>)</span><br><span class="line">    <span class="comment"># Then continue doing everything just like the model is just trained</span></span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>Remember that session.run(tf.global_variables_initializer()) shouldn&#39;t be executed, since variables are already initialized with your saved <em>.data-0000-of-00001</em> file.<br>If you only need the graph to be loaded, only use the <em>.meta</em> file:<br></div><br><div lang="zh-cn"><br>记住，这时候就不要再去执行session.run(tf.global_variables_initializer())了，因为变量已经用存储的checkpoint文件内容初始化过了。<br>如果只需要读取计算图结构，只要读取<em>.meta</em>文件：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    tf.train.import_meta_graph(<span class="string">'models/model.ckpt.meta'</span>)</span><br><span class="line">    <span class="comment"># Then continue doing everything just like the model is just built</span></span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>Function <em>tf.train.import_meta_graph()</em> loads(appends) the graph to your current computation graph. The values of tensors are still uninitialized so you&#39;ll have to execute session.run(tf.global_variables_initializer()) again. The tensors that you defined in the model can be retrieved by their names(property of the Tensor objects, instead of Python variable names). For example:<br></div><br><div lang="zh-cn"><br><em>tf.train.import_meta_graph()</em>函数将文件里的计算图读到（添加到）你当前的计算图中。其中所有Tensor的值仍未初始化，所以有必要执行一下session.run(tf.global_variables_initializer())了。之前定义的变量可以按照名称取回，示例：<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    <span class="comment"># Recover the model here</span></span><br><span class="line"></span><br><span class="line">    graph = tf.get_default_graph()</span><br><span class="line">    image_tensor = graph.get_tensor_by_name(<span class="string">'input_image:0'</span>)</span><br><span class="line">    loss = graph.get_tensor_by_name(<span class="string">'loss:0'</span>)</span><br><span class="line">    train = graph.get_operation_by_name(<span class="string">'train)</span></span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>To retrieve normal tensors, you&#39;ll have to append a <em>&#39;:0&#39;</em> to the name of the op. This means getting the associated tensor of the op. <em>train</em> is a little special - we only need the op, so the function is <em>get_operation_by_name()</em> so the &#39;:0&#39; is not necessary.<br></div><br><div lang="zh-cn"><br>要取回一般的Tensor，需要在Tensor的name属性值后边加一个<em>&#39;:0&#39;</em>，意思是取这个运算对应的Tensor。训练操作<em>train</em>略有不同——我们要的就只是这个op，所以用的函数<em>get_operation_by_name()</em>跟其他Tensor不一样，而且&#39;:0&#39;也不需要加。<br></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;&lt;/p&gt;
&lt;div lang=&quot;en-us&quot;&gt;&lt;br&gt;Update in March 2019:&lt;br&gt;&lt;br&gt;After TensorFlow developers introduced the APIs of Tensorflow 2.0 on Tensorflow Dev Summit 2019, I have made my decision to turn to PyTorch.&lt;br&gt;&lt;div align=&quot;center&quot; class=&quot;figure&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/tftutorial/wjz_en.gif&quot; alt=&quot;真香！&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://linkinpark213.com/tags/Deep-Learning/"/>
    
      <category term="DIY" scheme="http://linkinpark213.com/tags/DIY/"/>
    
  </entry>
  
  <entry>
    <title>MathJax - Use Math in Hexo, Just Like Tex! (Including Common Issue Solutions)</title>
    <link href="http://linkinpark213.com/2018/04/24/mathjax/"/>
    <id>http://linkinpark213.com/2018/04/24/mathjax/</id>
    <published>2018-04-24T05:43:41.000Z</published>
    <updated>2019-01-03T05:43:17.259Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><p>Sometimes you may want to explain some algorithms or principles with beautiful formulae in your blog. How to do this? Edit them in Microsoft Word, take a screenshot, crop it and put it in the blog post? When you finish your article and find out that you missed a symbol in the pictures - oh man, gotta repeat that again? Stop using those images now! A beautiful math display engine - MathJax allows you to code math like a coder.</p><div style="font-size: 1.2em"><br>$$\mathcal{C}\phi \delta e \mathfrak{M}\alpha th \mathit{I}n \mathcal{H}ex\sigma \mathbb{N}o\omega!$$<br></div><a id="more"></a><h2 id="1-Installation"><a href="#1-Installation" class="headerlink" title="1 Installation"></a>1 Installation</h2><h3 id="1-1-With-npm-For-those-using-Hexo-like-me"><a href="#1-1-With-npm-For-those-using-Hexo-like-me" class="headerlink" title="1.1 With npm (For those using Hexo like me)"></a>1.1 With npm (For those using Hexo like me)</h3><p>First, install <em>hexo-math</em> in your Hexo blog directory.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-math --save</span><br></pre></td></tr></table></figure></p><p>Then, add <em>math</em> configurations in your <em>_config.yml</em> file.<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">math:</span></span><br><span class="line"><span class="attr">  engine:</span> <span class="string">'mathjax'</span></span><br></pre></td></tr></table></figure></p><p>Finally, also add to your <em>_config.yml</em> file in the <strong>theme directory</strong> these configurations below.<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mathjax:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  per_page:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  cdn:</span> <span class="string">//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML</span></span><br></pre></td></tr></table></figure></p><h3 id="1-2-Or-by-inserting-a-snippet-in-your-HTML-code"><a href="#1-2-Or-by-inserting-a-snippet-in-your-HTML-code" class="headerlink" title="1.2 Or by inserting a snippet in your HTML code"></a>1.2 Or by inserting a snippet in your HTML code</h3><p>Maybe you don&#39;t have to use math in every blog post. If so, insert the following snippet in your Markdown file also works.<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML'</span> <span class="attr">async</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></p><h2 id="2-Usage"><a href="#2-Usage" class="headerlink" title="2 Usage"></a>2 Usage</h2><p>MathJax supports the same grammar that LaTeX does. To learn more about LaTeX, please refer to Chapter 3 of <a href="https://tobi.oetiker.ch/lshort/lshort.pdf" target="_blank" rel="noopener">The Not So Short<br>Introduction to LATEX</a>(CN version also available <a href="http://www.mohu.org/info/lshort-cn.pdf" target="_blank" rel="noopener">here</a>).</p><p>Use a &quot;\\(&quot; and a &quot;\\)&quot; to insert a formula in the line(they decide the boundary of the formula), or two &quot;$$&quot; to insert one that occupy a new line. I&#39;ll give a few examples below.</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\\(\mathcal&#123;F&#125;(x)=\mathcal&#123;H&#125;(x)-x\\)</span><br></pre></td></tr></table></figure><p>\(\mathcal{F}(x)=\mathcal{H}(x)-x\)<br><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\\(E=mc^2\\)</span><br></pre></td></tr></table></figure></p><p>\(E=mc^2\)<br><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$$\lim_&#123;n\rightarrow \infty&#125;(1+2^n+3^n)^\frac&#123;1&#125;&#123;x+\sin n&#125;$$</span><br></pre></td></tr></table></figure></p><p>$$\lim_{n\rightarrow \infty}(1+2^n+3^n)^\frac{1}{x+\sin n}$$<br><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$$\mathcal&#123;C&#125;\phi \delta e \mathfrak&#123;M&#125;\alpha th \mathit&#123;I&#125;n \mathcal&#123;H&#125;ex\sigma \mathbb&#123;N&#125;o\omega!$$</span><br></pre></td></tr></table></figure></p><p>$$\mathcal{C}\phi \delta e \mathfrak{M}\alpha th \mathit{I}n \mathcal{H}ex\sigma \mathbb{N}o\omega!$$</p><h2 id="3-Problems-when-using-MathJax-with-Hexo-amp-Solutions"><a href="#3-Problems-when-using-MathJax-with-Hexo-amp-Solutions" class="headerlink" title="3 Problems when using MathJax with Hexo &amp; Solutions"></a>3 Problems when using MathJax with Hexo &amp; Solutions</h2><p>This list will be appended whenever I find any more.</p><h3 id="3-1-Subscript-symbol-quot-quot-gets-mistaken-as-Markdown-emphasize-symbol"><a href="#3-1-Subscript-symbol-quot-quot-gets-mistaken-as-Markdown-emphasize-symbol" class="headerlink" title="3.1 Subscript symbol &quot;_&quot; gets mistaken as Markdown emphasize symbol"></a>3.1 Subscript symbol &quot;_&quot; gets mistaken as Markdown emphasize symbol</h3><p>This is a tough problem. Hexo renderer would first render the .md file into a .html file, and the MathJax script will only work on the .html file. Therefore, when there are multiple subscript symbols, they might be rendered as &lt;em&gt;&lt;/em&gt; tags. </p><p>For example: when you actually need a full-line formula \(x_{i+1}+y_j\), perhaps you&#39;ll get a &quot;$$x<em>{i+1}+y</em>j$$&quot; instead. Look into the HTML code and you&#39;ll understand why.</p><p>My solution for now, is giving up this Markdown emphasize symbol, since both &quot;_&quot; and &quot;*&quot; can be used as emphasize tags, and the alternative symbol &quot;*&quot; will also work if we remove &quot;_&quot;. Using &quot;\_&quot; also works, but it would be frequently used(while &quot;*&quot; isn&#39;t), thus turning our math code into mess code.</p><p>How do we do this? Bravely look into the <em>node_modules</em> directory and find the renderer of the Hexo engine. My renderer is <em>marked</em>, which is the default for Hexo. There is a file named <em>marked.js</em> inside <em>node_modules/marked/lib/</em> directory. You can find two appearances of &quot;em:&quot;. Like this:<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> inline = &#123;</span><br><span class="line">  ...</span><br><span class="line">  em: <span class="regexp">/^\b_((?:[^_]|__)+?)_\b|^\*((?:\*:\*|[\s\S])+?)\*(?!\*)/</span>,</span><br><span class="line">  ...</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>and<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inline.pedantic = merge(&#123;&#125;, inline.normal, &#123;</span><br><span class="line">  ...</span><br><span class="line">  em: <span class="regexp">/^_(?=\S)([\s\S]*?\S)_(?!_)|^\*(?=\S)([\s\S]*?\S)\*(?!\*)/</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p><p>Modify the regular expression after them - remove the one about &quot;_&quot;s and leave the one about &quot;*&quot;s. The new version would be:<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> inline = &#123;</span><br><span class="line">  ...</span><br><span class="line">  em: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br><span class="line">  ...</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>and<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inline.pedantic = merge(&#123;&#125;, inline.normal, &#123;</span><br><span class="line">  ...</span><br><span class="line">  em: <span class="regexp">/^\*(?=\S)([\s\S]*?\S)\*(?!\*)/</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p><p>From now on, you can use &quot;_&quot; as the subscript in MathJax freely. You don&#39;t have to worry about its becoming &lt;em&gt;&lt;/em&gt; tags anymore.</p><h3 id="3-2-Using-quot-amp-quot-for-aligning-multi-line-equations-but-getting-a-quot-Misplaced-amp-quot"><a href="#3-2-Using-quot-amp-quot-for-aligning-multi-line-equations-but-getting-a-quot-Misplaced-amp-quot" class="headerlink" title="3.2 Using &quot;&amp;&quot; for aligning multi-line equations but getting a &quot;Misplaced &amp;&quot;"></a>3.2 Using &quot;&amp;&quot; for aligning multi-line equations but getting a &quot;Misplaced &amp;&quot;</h3><p>For example, in my previous post about ResNet, I tried to use the following code to start a new line in an equation while aligning the lines to the equal sign:<br><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$$\frac&#123;\partial&#123;\mathcal&#123;E&#125;&#125;&#125;&#123;\partial&#123;x<span class="emphasis">_l&#125;&#125; &amp; = \frac&#123;\partial&#123;\mathcal&#123;E&#125;&#125;&#125;&#123;\partial&#123;x_</span>L&#125;&#125;\frac&#123;\partial&#123;x<span class="emphasis">_L&#125;&#125;&#123;\partial&#123;x_</span>l&#125;&#125;\\\\</span><br><span class="line">&amp; = \frac&#123;\partial&#123;\mathcal&#123;E&#125;&#125;&#125;&#123;\partial&#123;x<span class="emphasis">_L&#125;&#125;\Big(1+\frac&#123;\partial&#123;&#125;&#125;&#123;\partial&#123;x_</span>l&#125;&#125;\sum<span class="emphasis">_&#123;i=l&#125;^&#123;L-1&#125;\mathcal&#123;F&#125;(x_</span>i,\mathcal&#123;W&#125;_i)\Big)$$</span><br></pre></td></tr></table></figure></p><p>The &quot;&amp;&quot; symbols were used to align the lines to a certain point. However, the result was a &quot;Misplaced &amp;&quot; prompt.</p><p>By disabling MathJax, I found out that the rendered equation was correct, which means that <strong>the problem isn&#39;t with Hexo renderer</strong>. This was when I realized that although<br><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation&#125;</span><br><span class="line">\end&#123;equation&#125;</span><br></pre></td></tr></table></figure></p><p>are not necessary,<br><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;split&#125;</span><br><span class="line">\end&#123;split&#125;</span><br></pre></td></tr></table></figure></p><p>shouldn&#39;t be removed. Surround the equation with them will work. My code is here:<br><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$$\begin&#123;split&#125;</span><br><span class="line">\frac&#123;\partial&#123;\mathcal&#123;E&#125;&#125;&#125;&#123;\partial&#123;x<span class="emphasis">_l&#125;&#125; &amp; = \frac&#123;\partial&#123;\mathcal&#123;E&#125;&#125;&#125;&#123;\partial&#123;x_</span>L&#125;&#125;\frac&#123;\partial&#123;x<span class="emphasis">_L&#125;&#125;&#123;\partial&#123;x_</span>l&#125;&#125;\\\\</span><br><span class="line">&amp; = \frac&#123;\partial&#123;\mathcal&#123;E&#125;&#125;&#125;&#123;\partial&#123;x<span class="emphasis">_L&#125;&#125;\Big(1+\frac&#123;\partial&#123;&#125;&#125;&#123;\partial&#123;x_</span>l&#125;&#125;\sum<span class="emphasis">_&#123;i=l&#125;^&#123;L-1&#125;\mathcal&#123;F&#125;(x_</span>i,\mathcal&#123;W&#125;_i)\Big)</span><br><span class="line">\end&#123;split&#125;$$</span><br></pre></td></tr></table></figure></p><p>And it runs like:<br>$$\begin{split}<br>\frac{\partial{\mathcal{E}}}{\partial{x_l}} &amp; = \frac{\partial{\mathcal{E}}}{\partial{x_L}}\frac{\partial{x_L}}{\partial{x_l}}\\<br>&amp; = \frac{\partial{\mathcal{E}}}{\partial{x_L}}\Big(1+\frac{\partial{}}{\partial{x_l}}\sum_{i=l}^{L-1}\mathcal{F}(x_i,\mathcal{W}_i)\Big)<br>\end{split}$$</p><h3 id="3-3-To-be-continued"><a href="#3-3-To-be-continued" class="headerlink" title="3.3 To be continued"></a>3.3 To be continued</h3><p>If you encounter other issues while using MathJax with Hexo(with or without a solution), feel free to leave a comment below!</p>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;p&gt;Sometimes you may want to explain some algorithms or principles with beautiful formulae in your blog. How to do this? Edit them in Microsoft Word, take a screenshot, crop it and put it in the blog post? When you finish your article and find out that you missed a symbol in the pictures - oh man, gotta repeat that again? Stop using those images now! A beautiful math display engine - MathJax allows you to code math like a coder.&lt;/p&gt;
&lt;div style=&quot;font-size: 1.2em&quot;&gt;&lt;br&gt;$$\mathcal{C}\phi \delta e \mathfrak{M}\alpha th \mathit{I}n \mathcal{H}ex\sigma \mathbb{N}o\omega!$$&lt;br&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="Blogging" scheme="http://linkinpark213.com/tags/Blogging/"/>
    
  </entry>
  
  <entry>
    <title>A Review of ResNet - Residual Networks</title>
    <link href="http://linkinpark213.com/2018/04/22/resnet/"/>
    <id>http://linkinpark213.com/2018/04/22/resnet/</id>
    <published>2018-04-22T05:55:35.000Z</published>
    <updated>2019-04-21T07:23:02.102Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="0-Introduction"><a href="#0-Introduction" class="headerlink" title="0 Introduction"></a>0 Introduction</h2><p>Deep learning researchers have been constructing skyscrapers in recent years. Especially, VGG nets and GoogLeNet have pushed the depths of convolutional networks to the extreme. But questions remain: if time and money aren&#39;t problems, are deeper networks always performing better? Not exactly.</p><p>When residual networks were proposed, researchers around the world was stunned by its depth. &quot;Jesus Christ! Is this a neural network or the Dubai Tower?&quot; But <strong>don&#39;t be afraid!</strong> These networks are deep but the structures are simple. Interestingly, these networks not only defeated all opponents in the classification, detection, localization challenges in ImageNet 2015, but were also the main innovation in the best paper of CVPR2016.</p><div align="center"><br>    <img src="/images/resnet/network_growth.jpg" width="40%" height="40%" alt="Network Growth"><br></div><a id="more"></a><h2 id="1-The-Crisis-Degradation-of-Deep-Networks"><a href="#1-The-Crisis-Degradation-of-Deep-Networks" class="headerlink" title="1 The Crisis: Degradation of Deep Networks"></a>1 The Crisis: Degradation of Deep Networks</h2><p>VGG nets proved the beneficial of representation depth of convolutional neural networks, at least within a certain range, to be exact. However, when Kaiming He et al. tried to deepen some plain networks, the training error and test error stopped decreasing after the network reached a certain depth(which is not surprising) and soon degraded. This is not an overfitting problem, because training errors also increased; nor is it a gradient vanishing problem, because there are some techniques(e.g. batch normalization[4]) that ease the pain.</p><div align="center" class="figure"><br>    <img src="/images/resnet/downgrade.png" width="60%" height="60%" alt="The Downgrade Problem"><br>    Fig.1 The downgrade problem<br><br></div><p>What seems to be the cause of this degradation? Obviously, deeper neural networks are more difficult to train, but that doesn&#39;t mean deeper neural networks would yield worse results. To explain this problem, Balduzzi et al.[3] identified shattered gradient problem - as depth increases, gradients in standard feedforward networks increasingly resemble white noise. I will write about that later.</p><h2 id="2-A-Closer-Look-at-ResNet-The-Residual-Blocks"><a href="#2-A-Closer-Look-at-ResNet-The-Residual-Blocks" class="headerlink" title="2 A Closer Look at ResNet: The Residual Blocks"></a>2 A Closer Look at ResNet: The Residual Blocks</h2><p>As the old saying goes, &quot;千里之行，始于足下&quot;. Although ResNets are as deep as a thousand layers, they are built with these basic residual blocks(the right part of the figure). </p><div align="center" class="figure"><br>    <img src="/images/resnet/residual_blocks.png" width="50%" height="50%" alt="Comparison between normal weight layers and residual blocks"><br>    Fig.2 Parts of plain networks and a residual block(or residual unit)<br><br></div><h3 id="2-1-Skip-Connections"><a href="#2-1-Skip-Connections" class="headerlink" title="2.1 Skip Connections"></a>2.1 Skip Connections</h3><p>In comparison, basic units of plain network models would look like the one on the left: one ReLU function after a weight layer(usually also with biases), repeated several times. Let&#39;s denote the desired underlying mapping(the ideal mapping) of the two layers as \(\mathcal{H}(x)\), and the real mapping as \(\mathcal{F}(x)\). Clearly, the closer \(\mathcal{F}(x)\) is to \(\mathcal{H}(x)\), the better it fits.</p><p>However, He et al. explicitly let these layers fit a residual mapping instead of the desired underlying mapping. This is implemented with &quot;shortcut connections&quot;, which skip one or more layers, simply performing identity mappings and getting added to the outputs of the stacked weight layers. This way, \(\mathcal{F}(x)\) would not try to fit \(\mathcal{H}(x)\), but \(\mathcal{H}(x)-x\). The whole structure(from the identity mapping branch, to merging the branches by the addition operation) are named &quot;residual blocks&quot;(or &quot;residual units&quot;).</p><p>What&#39;s the point in this? Let&#39;s do a simple analysis. The computation done by the original residual block is: $$y_l=h(x_l)+\mathcal{F}(x_l,\mathcal{W}_l),$$ $$x_{l+1}=f(y_l).$$</p><p>Here are the definitions of symbols:<br>\(x_l\): input features to the \(l\)-th residual block;<br>\(\mathcal{W}_{l}={W_{l,k}|_{1\leq k\leq K}}\): a set of weights(and biases) associated with the \(l\)-th residual unit. \(K\) is the number of layers in this block;<br>\(\mathcal{F}(x,\mathcal{W})\): the residual function, which we talked about earlier. It&#39;s a stack of 2 conv. layers here;<br>\(f(x)\): the activation function. We are using ReLU here;<br>\(h(x)\): identity mapping.</p><p>If \(f(x)\) is also an identity mapping(as if we&#39;re not using any activation function), the first equation would become:<br>$$x_{l+1}=x_l+\mathcal{F}(x_l,\mathcal{W}_l)$$</p><p>Therefore, we can define \(x_L\) recursively of any layer:<br>$$x_L=x_l+\sum_{i=l}^{L-1}\mathcal{F}(x_i,\mathcal{W}_i)$$</p><p>That&#39;s not the end yet! When it comes to the gradients, according to the chain rules of backpropagation, we have a beautiful definition:<br>$$\begin{split}<br>\frac{\partial{\mathcal{E}}}{\partial{x_l}} &amp; = \frac{\partial{\mathcal{E}}}{\partial{x_L}}\frac{\partial{x_L}}{\partial{x_l}}\\<br>&amp; = \frac{\partial{\mathcal{E}}}{\partial{x_L}}\Big(1+\frac{\partial{}}{\partial{x_l}}\sum_{i=l}^{L-1}\mathcal{F}(x_i,\mathcal{W}_i)\Big)<br>\end{split}$$</p><p>What does it mean? It means that the information is directly backpropagated to ANY shallower block. This way, the gradients of a layer never vanish or explode even if the weights are too small or too big.</p><h3 id="2-2-Identity-Mappings"><a href="#2-2-Identity-Mappings" class="headerlink" title="2.2 Identity Mappings"></a>2.2 Identity Mappings</h3><p>It&#39;s important that we use identity mapping here! Just consider doing a simple modification here, for example, \(h(x)=\lambda_lx_l\)(\(\lambda_l\) is a modulating scalar). The definition of \(x_L\) and \(\frac{\partial{\mathcal{E}}}{\partial{x_l}}\) would become:<br>$$x_L=(\prod_{i=l}^{L-1}\lambda_i)x_l+\sum_{i=l}^{L-1}(\prod_{j=i+1}^{L-1}\lambda_j)\mathcal{F}(x_i,\mathcal{W}_i)$$<br>$$\frac{\partial{\mathcal{E}}}{\partial{x_l}}=\frac{\partial{\mathcal{E}}}{\partial{x_L}}\Big((\prod_{i=l}^{L-1}\lambda_i)+\frac{\partial{}}{\partial{x_l}}\sum_{i=l}^{L-1}(\prod_{j=i+1}^{L-1}\lambda_j)\mathcal{F}(x_i,\mathcal{W}_i)\Big)$$</p><p>For extremely deep neural networks where \(L\) is too large, \(\prod_{i=l}^{L-1}\lambda_i\) could be either too small or too large, causing gradient vanishing or gradient explosion. For \(h(x)\) with complex definitions, the gradient could be extremely complicated, thus losing the advantage of the skip connection. Skip connection works best under the condition where the grey channel in Fig. 3 cover no operations (except the addition) and is clean.</p><p>Interestingly, this comfirmed the philosophy of &quot;大道至简&quot; once again.</p><h3 id="2-3-Post-activation-or-Pre-activation"><a href="#2-3-Post-activation-or-Pre-activation" class="headerlink" title="2.3 Post-activation or Pre-activation?"></a>2.3 Post-activation or Pre-activation?</h3><p>Wait a second... &quot;\(f(x)\) is also an identity mapping&quot; is just our assumption. The activation function is still there!</p><p>Right. There IS an activation function, but it&#39;s moved to somewhere else.  In fact, the original residual block is still a little bit problematic - the output of one residual block is not always the input of the next, since there is a ReLU activation function after the addition(It did NOT REALLY keep the identity mapping to the next block!). Therefore, in[2], He et al. fixed the residual blocks by changing the order of operations.</p><div align="center" class="figure"><br>    <img src="/images/resnet/identity_mapping.png" width="30%" height="30%" alt="New identity mapping"><br>    Fig.3 New identity mapping proposed by He et al.<br><br></div><p>Besides using a simple identity mapping, He et al. also discussed about the position of the activation function and the batch normalization operation. Assuming that we got a special(asymmetric) activation function \(\hat f(x)\), which only affects the path to the next residual unit. Now our definition of \(x_{x+1}\) would become:<br>$$x_{l+1}=x_l+\mathcal{F}(\hat f(x_l),\mathcal{W}_l)$$</p><p>With \(x_l\) still multiplied by 1, information is still fully backpropagated to shallower residual blocks. And the good thing is that using this asymmetric activation function after the addition(partial post-activation) is equivalent to using it beforehand(pre-activation)! This is why He et al. chose to use pre-activation - otherwise it would be necessary to implement that magical activation function \(\hat f(x)\).</p><div align="center" class="figure"><br>    <img src="/images/resnet/pre-activation.png" width="80%" height="80%" alt="Asymmetric after-addition activation"><br>    Fig.4 Using asymmetric after-addition activation is equivalent to constructing a pre-activation residual unit<br><br></div><h2 id="3-ResNet-Architectures"><a href="#3-ResNet-Architectures" class="headerlink" title="3 ResNet Architectures"></a>3 ResNet Architectures</h2><p>Here are the ResNet architectures for ImageNet. Building blocks are shown in brackets, with the numbers of blocks stacked. With the first block of every stack(starting from conv3_x), a downsampling is performed. Each column represents one of the residual networks, and the deepest one has 152 weight layers! Since ResNets were proposed, VGG nets - which were officially called &quot;Very Deep Convolutional Networks&quot; - are not relatively deep anymore. Maybe call them &quot;A Little Bit Deep Convolutional Networks&quot;.</p><div align="center" class="figure"><br>    <img src="/images/resnet/architectures.png" width="80%" height="80%" alt="ResNet architectures for ImageNet"><br>    Table. 1 ResNet architectures for ImageNet.<br><br></div><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h2><h3 id="4-1-Performance-on-ImageNet"><a href="#4-1-Performance-on-ImageNet" class="headerlink" title="4.1 Performance on ImageNet"></a>4.1 Performance on ImageNet</h3><p>He et al. trained ResNet-18 and ResNet-34 on the ImageNet dataset, and also compared them to plain convolutional networks. In Fig. 5, the thin curves denote training error, and the bold ones denote validation error. The figure on the left shows the results of plain convolution networks(in which the 34-layered ones has higher error rates than the 18-layered one), and the figure on the right shows that residual networks perform better than plain ones, while deeper ones perform better than shallow ones.</p><div align="center" class="figure"><br>    <img src="/images/resnet/training.png" width="80%" height="80%" alt="Training ResNet on ImageNet"><br>    Fig. 5 Training ResNet on ImageNet<br><br></div><h3 id="4-2-Effects-of-Different-Shortcut-Connections"><a href="#4-2-Effects-of-Different-Shortcut-Connections" class="headerlink" title="4.2 Effects of Different Shortcut Connections"></a>4.2 Effects of Different Shortcut Connections</h3><p>He et al. also tried various types of shortcut connections to replace the identity mapping, and various positions of activation functions / batch normalization. Experiments show that the original identity mapping and full pre-activation yield the best results.</p><div align="center" class="figure"><br>    <img src="/images/resnet/shortcut-connections.png" width="50%" height="50%" alt="Different shortcuts of residual units"><br>    Fig. 6 Various shortcuts in residual units<br><br>    <img src="/images/resnet/shortcut-connections-experiment.png" width="70%" height="70%" alt="Classification errors with different shortcuts"><br>    Table. 2 Classification error on CIFAR-10 test set with various shortcut connections in residual units<br><br>    <img src="/images/resnet/activations.png" width="70%" height="50%" alt="Different usages of activation in residual units"><br>    Fig. 7 Various usages of activation in residual units<br><br>    <img src="/images/resnet/activations-experiment.png" width="50%" height="50%" alt="Classification errors with different activations"><br>    Table. 3 Classification error on CIFAR-10 test set with various usages of activation in residual units<br><br></div><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p>Residual learning can be crowned as &quot;ONE OF THE GREATEST HITS IN DEEP LEARNING FIELDS&quot;. With a simple identity mapping, it solved the degradation problem of deep neural networks. Now that you have learned about the concept of ResNet, why not give it a try and implement your first residual learning model today?</p><div align="center" class="figure"><br>    <img src="/images/resnet/resnet-yooo.jpg" width="40%" height="40%" alt><br></div><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" target="_blank" rel="noopener">He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</a></p><p>[2] <a href="https://arxiv.org/pdf/1603.05027.pdf" target="_blank" rel="noopener">He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks[C]//European Conference on Computer Vision. Springer, Cham, 2016: 630-645.</a></p><p>[3] <a href="https://arxiv.org/pdf/1702.08591.pdf" target="_blank" rel="noopener">Balduzzi D, Frean M, Leary L, et al. The Shattered Gradients Problem: If resnets are the answer, then what is the question?[J]. arXiv preprint arXiv:1702.08591, 2017.</a></p><p>[4] <a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.</a></p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;0-Introduction&quot;&gt;&lt;a href=&quot;#0-Introduction&quot; class=&quot;headerlink&quot; title=&quot;0 Introduction&quot;&gt;&lt;/a&gt;0 Introduction&lt;/h2&gt;&lt;p&gt;Deep learning researchers have been constructing skyscrapers in recent years. Especially, VGG nets and GoogLeNet have pushed the depths of convolutional networks to the extreme. But questions remain: if time and money aren&amp;#39;t problems, are deeper networks always performing better? Not exactly.&lt;/p&gt;
&lt;p&gt;When residual networks were proposed, researchers around the world was stunned by its depth. &amp;quot;Jesus Christ! Is this a neural network or the Dubai Tower?&amp;quot; But &lt;strong&gt;don&amp;#39;t be afraid!&lt;/strong&gt; These networks are deep but the structures are simple. Interestingly, these networks not only defeated all opponents in the classification, detection, localization challenges in ImageNet 2015, but were also the main innovation in the best paper of CVPR2016.&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;&lt;br&gt;    &lt;img src=&quot;/images/resnet/network_growth.jpg&quot; width=&quot;40%&quot; height=&quot;40%&quot; alt=&quot;Network Growth&quot;&gt;&lt;br&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://linkinpark213.com/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://linkinpark213.com/tags/Computer-Vision/"/>
    
      <category term="Reviews" scheme="http://linkinpark213.com/tags/Reviews/"/>
    
  </entry>
  
  <entry>
    <title>A Review of VGG net - Very Deep Convolutional Neural Networks</title>
    <link href="http://linkinpark213.com/2018/04/21/vgg/"/>
    <id>http://linkinpark213.com/2018/04/21/vgg/</id>
    <published>2018-04-21T07:15:55.000Z</published>
    <updated>2019-04-21T07:23:02.102Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="0-Introduction"><a href="#0-Introduction" class="headerlink" title="0 Introduction"></a>0 Introduction</h2><p>Convolutional neural networks(CNN) have enjoyed great success in computer vision research fields in the past few years. A number of attempts are made based on the original CNN architecture to improve its accuracy and performance. In 2014, Karen Simonyan et al. did an investigation on the effect of depth on CNNs&#39; accuracy in large-scale image recognition (thus also proposing a series of very deep CNNs which are usually called VGG nets). The result confirmed the importance of CNN depth in visual representations.</p><h2 id="1-Background-VGG-net-39-s-ancestors"><a href="#1-Background-VGG-net-39-s-ancestors" class="headerlink" title="1 Background: VGG net&#39;s ancestors"></a>1 Background: VGG net&#39;s ancestors</h2><p>Before introducing VGG net, let&#39;s take a glance at prior convolutional neural networks. </p><h3 id="1-1-LeNet-The-Origin"><a href="#1-1-LeNet-The-Origin" class="headerlink" title="1.1 LeNet: The Origin"></a>1.1 LeNet: The Origin</h3><p>Basic neural network structures(for example, multi-layer perceptron) learn patterns on 1D vectors, which cannot cope with 2D features in images well. In 1986, Lecun et al. proposed a convolution network model called LeNet-5. Its structure is fairly simple: two convolution layers, two subsampling layers and a few fully connected layers. This network was used to solve a number recognition problem. (If you need to learn more about the convolution operation, please refer to Google or <em>Digital Image Processing</em> by Rafael C. Gonzalez)</p><a id="more"></a><div align="center" class="figure"><br>    <img src="/images/vgg/lenet.png" width="80%" height="60%" alt="LeNet"><br><br>    Fig. 1 Architecture of LeNet<br><br></div><h3 id="1-2-AlexNet-The-Powerful-Convolution"><a href="#1-2-AlexNet-The-Powerful-Convolution" class="headerlink" title="1.2 AlexNet: The Powerful Convolution"></a>1.2 AlexNet: The Powerful Convolution</h3><p>In 2012, Alex Krizhevsky et al. won the first place in ILSVRC-2012(ImageNet Large-Scale Visual Recognition Challenge 2012) and achieved the highest top-5 error rate of 15.3% with a convolutional network model, while the second-best entry only achieved 26.2%. The network, namely AlexNet, was trained on two GTX580 3GB GPUs in parallel. Since a single GTX580 GPU has only 3GB memory, the maximum size of networks is limited. This model proved the effectiveness of CNNs under complicated circumstances and the power of GPUs. So what if the network can go deeper? Will the top-5 error rate get even lower?</p><div align="center" class="figure"><br><img src="/images/vgg/alexnet.png" alt="AlexNet"><br><br>Fig. 2 Architecture of AlexNet<br></div><h2 id="2-Main-Contributions-of-VGG-Nets"><a href="#2-Main-Contributions-of-VGG-Nets" class="headerlink" title="2 Main Contributions of VGG Nets"></a>2 Main Contributions of VGG Nets</h2><p>Here comes our hero - VGG nets. By the way, VGG is not the name of the network, but the name of the authors&#39; group - <em>Visual Geometry Group</em>, from Department of Engineering Science, University of Oxford. The networks they proposed were therefore named after the group. The main contributions of VGG nets are: 1. more but smaller convolution filters; 2. great depth of networks.</p><h3 id="2-1-Stacks-of-Smaller-Convolution-Filters"><a href="#2-1-Stacks-of-Smaller-Convolution-Filters" class="headerlink" title="2.1 Stacks of Smaller Convolution Filters"></a>2.1 Stacks of Smaller Convolution Filters</h3><p>Rather than using relatively large receptive fields in the first convolution layers, Simonyan et al. selected very small 3x3 receptive fields throughout the whole net, which are convolved with the input at every pixel with a stride of 1. As is shown in the figures below, a stack of two 3x3 convolution layers has an effective receptive field of 5x5. We can also conclude that a stack of three 3x3 convolution filters has an effective receptive field of 7x7.</p><div align="center" class="figure"><br>    <img src="/images/vgg/conv1.png" width="50%" height="50%" alt="Conv5x5"><br><br>    Fig. 3 A convolution layer with one 5x5 conv. filter has a receptive field of 5x5<br><br>    <img src="/images/vgg/conv2.png" width="60%" height="60%" alt="Conv3x3x2"><br><br>    Fig. 4 A convolution layer stack with two 3x3 conv. filter also has a effective receptive field of 5x5<br><br></div><p>Now that we&#39;re clear that stacks of small-kernel convolution layers have equal sized receptive fields, why are they the better choice? Well, the first advantage is incorporating more rectification layers instead of a single one, since every convolution layer includes an activation function(usually ReLU). More rectification brings more non-linearity, and more non-linearity makes the decision function more discriminative and fit better. Also, when the receptive field isn&#39;t too large, a stack of 3x3 convolution filters have fewer parameters to train. Assuming the number of input and output channels of a convolution layer stack are equal(let&#39;s call it C) and the receptive field is 5x5, we have \(2*3*3*C*C=18C^2\) instead of \(5*5*C*C=25C^2\) parameters here. Similarly, when the receptive field is 7x7, we have \(3*3*3*C*C=27C^2\) instead of \(7*7*C*C=49C^2\). When the field gets even larger? A function with \(O(n)\) complexity only has greater advantage against an \(O(n^2)\) when \(n\) grows.</p><h3 id="2-2-Deep-Dark-Fantasy"><a href="#2-2-Deep-Dark-Fantasy" class="headerlink" title="2.2 Deep Dark Fantasy"></a>2.2 Deep Dark Fantasy</h3><p>Cliches time. Just like any blogger mentioning VGG nets would do, here are the network structures proposed by Simonyan et al.</p><div align="center" class="figure"><br>    <img src="/images/vgg/vggnets.png" width="60%" height="60%" alt="VGG Nets"><br><br>    Table. 1 VGG nets of various depths<br><br></div><p>Look at the table column-by-column. Each column(A, A-LRN, B, C, D, E) corresponds to one network structure. As you can see, their networks grew from 11 layers(in net A) to 19 layers(in net E). Each time something is added to the previous net, it would appear bold. Clearly, LRN(Local Response Normalization) didn&#39;t work well in this case(actually, A-LRN net performed worse than A, while consuming much more memory and computation time), and was thus removed. </p><p>What&#39;s worth mentioning are the 1x1 convolution layers appearing in network C. This is a way to increase non-linearity(also by introducing activation functions) of the decision function while also keeping the size of the receptive fields unchanged.</p><h2 id="3-Training-amp-Evaluation"><a href="#3-Training-amp-Evaluation" class="headerlink" title="3 Training &amp; Evaluation"></a>3 Training &amp; Evaluation</h2><p>Bad initialization could stall learning due to the instability of gradient deep networks. Therefore, the authors first trained the network A, which is shallow enough to be trained with random initialization. Then, the next networks (B to E) are initialized with the pre-trained models, and only weights of the new layers are randomly initialized.</p><p>In spite of the larger number of parameters and the greater depth of our nets compared to AlexNet, the nets required less epochs to converge due to the implicit regularization imposed by greater depth, smaller convolution filter sizes and the pre-initialization of certain layers. They also generalize well to other datasets, achieving state-of-the-art performances. Results of VGG nets in comparison against other models in ILSVRC are shown in the table below.</p><div align="center" class="figure"><br>    <img src="/images/vgg/VGG-performance-comparison.png" width="70%" height="70%" alt="VGG net results"><br><br>    Table. 2 VGG net performance, in comparison with the state of the art in ILSVRC classification<br><br></div><p>In conclusion, the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. </p><p>You might ask: Why not even deeper, with more powerful GPUs(the authors used Titan Black), we can absolutely train deeper networks that perform better! Not exactly. Problems arose as the networks get too deep, and this is where ResNet comes in.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="https://arxiv.org/pdf/1409.1556/" target="_blank" rel="noopener">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.</a></p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;0-Introduction&quot;&gt;&lt;a href=&quot;#0-Introduction&quot; class=&quot;headerlink&quot; title=&quot;0 Introduction&quot;&gt;&lt;/a&gt;0 Introduction&lt;/h2&gt;&lt;p&gt;Convolutional neural networks(CNN) have enjoyed great success in computer vision research fields in the past few years. A number of attempts are made based on the original CNN architecture to improve its accuracy and performance. In 2014, Karen Simonyan et al. did an investigation on the effect of depth on CNNs&amp;#39; accuracy in large-scale image recognition (thus also proposing a series of very deep CNNs which are usually called VGG nets). The result confirmed the importance of CNN depth in visual representations.&lt;/p&gt;
&lt;h2 id=&quot;1-Background-VGG-net-39-s-ancestors&quot;&gt;&lt;a href=&quot;#1-Background-VGG-net-39-s-ancestors&quot; class=&quot;headerlink&quot; title=&quot;1 Background: VGG net&amp;#39;s ancestors&quot;&gt;&lt;/a&gt;1 Background: VGG net&amp;#39;s ancestors&lt;/h2&gt;&lt;p&gt;Before introducing VGG net, let&amp;#39;s take a glance at prior convolutional neural networks. &lt;/p&gt;
&lt;h3 id=&quot;1-1-LeNet-The-Origin&quot;&gt;&lt;a href=&quot;#1-1-LeNet-The-Origin&quot; class=&quot;headerlink&quot; title=&quot;1.1 LeNet: The Origin&quot;&gt;&lt;/a&gt;1.1 LeNet: The Origin&lt;/h3&gt;&lt;p&gt;Basic neural network structures(for example, multi-layer perceptron) learn patterns on 1D vectors, which cannot cope with 2D features in images well. In 1986, Lecun et al. proposed a convolution network model called LeNet-5. Its structure is fairly simple: two convolution layers, two subsampling layers and a few fully connected layers. This network was used to solve a number recognition problem. (If you need to learn more about the convolution operation, please refer to Google or &lt;em&gt;Digital Image Processing&lt;/em&gt; by Rafael C. Gonzalez)&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://linkinpark213.com/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://linkinpark213.com/tags/Computer-Vision/"/>
    
      <category term="Reviews" scheme="http://linkinpark213.com/tags/Reviews/"/>
    
  </entry>
  
  <entry>
    <title>Smartypants is NOT SO SMART</title>
    <link href="http://linkinpark213.com/2018/03/20/smartypants/"/>
    <id>http://linkinpark213.com/2018/03/20/smartypants/</id>
    <published>2018-03-20T02:00:22.000Z</published>
    <updated>2018-11-18T05:44:45.857Z</updated>
    
    <content type="html"><![CDATA[<p>When blogging with Hexo, every time I type a single quotation mark(also called apostrophe) like this:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;</span><br></pre></td></tr></table></figure></p><p>, Hexo would convert it to a symbol like this<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">’</span><br></pre></td></tr></table></figure></p><p>You would say that this is also an apotrophe, but it really looks UNBEARABLE in the articles. It&#39;s been a problem bothering me for more than a month.(I&#39;m not saying that this is the reason for not updating my blog, but I don&#39;t mind if you think so!)</p><a id="more"></a><p><img src="/images/smartypants/apostrophe.png" alt="apostrophe"></p><p>Therefore, I Googled about this problem and tried to find other victims. According to their sharings, this problem is caused by <em>marked</em> -- the default markdown renderer of Hexo.The <em>&quot;smatrypants&quot;</em> function of marked was turned on by default.</p><p>Now take a look at the introduction of <em>smartypants</em> on the <em>hexo-renderer-marked</em> page:</p><blockquote><p><em>smartypants</em> - Use &quot;smart&quot; typograhic punctuation for things like quotes and dashes.</p></blockquote><p>C&#39;mon, seriously? </p><p>There are a few bloggers who solved this by adding the code below to the _config.yml file in the blog directory. </p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mark:</span></span><br><span class="line"><span class="attr">  smartypants:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>This worked for most victims (perhaps all of them), but not for me. I have no idea why those config wasn&#39;t working, so if anyone finds out the reason, please contact me by e-mail.</p><p>If you&#39;re sure <em>smartypants</em> is causing the problem, and the solution above didn&#39;t work for you either, maybe you can try my solution.</p><p>Since <em>hexo-renderer-marked</em> is installed in the blog&#39;s <em>node_modules</em> directory(may also be in your Node.js directory if installed globally), isn&#39;t it possible that we change its own configurations? I looked at the <em>index.js</em> file in the <em>node_modules/hexo-renderer-marked/</em> directory. There you are, smartypants!</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hexo.config.marked = assign(&#123;</span><br><span class="line">  gfm: <span class="literal">true</span>,</span><br><span class="line">  pedantic: <span class="literal">false</span>,</span><br><span class="line">  sanitize: <span class="literal">false</span>,</span><br><span class="line">  tables: <span class="literal">true</span>,</span><br><span class="line">  breaks: <span class="literal">true</span>,</span><br><span class="line">  smartLists: <span class="literal">true</span>,</span><br><span class="line">  smartypants: <span class="literal">true</span>,</span><br><span class="line">  modifyAnchors: <span class="string">''</span>,</span><br><span class="line">  autolink: <span class="literal">true</span></span><br><span class="line">&#125;, hexo.config.marked);</span><br></pre></td></tr></table></figure><p>Now you know what to do.</p><p>Aaaaaaaaaaaaaaaand many thanks to Xizi Wu, the artist of my new avatar! I love it!<br><img src="/images/long_nobg.png" alt="Harper Long by Xizi Wu"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;When blogging with Hexo, every time I type a single quotation mark(also called apostrophe) like this:&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;apos;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;, Hexo would convert it to a symbol like this&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;’&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;You would say that this is also an apotrophe, but it really looks UNBEARABLE in the articles. It&amp;#39;s been a problem bothering me for more than a month.(I&amp;#39;m not saying that this is the reason for not updating my blog, but I don&amp;#39;t mind if you think so!)&lt;/p&gt;
    
    </summary>
    
    
      <category term="Blogging" scheme="http://linkinpark213.com/tags/Blogging/"/>
    
  </entry>
  
  <entry>
    <title>Cheers for the 8th International linkinpark213 Day!</title>
    <link href="http://linkinpark213.com/2018/02/13/linkinpark213-day/"/>
    <id>http://linkinpark213.com/2018/02/13/linkinpark213-day/</id>
    <published>2018-02-13T09:40:21.000Z</published>
    <updated>2018-11-18T06:04:41.608Z</updated>
    
    <content type="html"><![CDATA[<p>Cheers for the 8th International linkinpark213 Day!</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="string">'Harper'</span> <span class="string">'Sweet'</span> <span class="string">'Kobayashi'</span> <span class="string">'Kawasaki'</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"I'm <span class="variable">$i</span>, cheers!"</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h2 id="What-is-linkinpark213-Day"><a href="#What-is-linkinpark213-Day" class="headerlink" title="What is linkinpark213 Day?"></a>What is linkinpark213 Day?</h2><p>International linkinpark213 Day is a global anniversary set up by Harper Long in 2011 A.D., celebrated on February 13th every year. This anniversary is officially written as &#39;linkinpark213 Day&#39;, the first letter of which is a lower-case. The establishment of the anniversary dates back to the early 10s in the 21st century.</p><p>Till now, the population for this anniversary has already reached 1e-6 million, and the distribution also expanded from a small county to the whole middle-China area, including Hebei, Henan, Shanxi and Shaanxi province. There will also be some Japanese resident who plan to celebrate this day in 2019.</p><a id="more"></a><h2 id="Why-13th-Feb"><a href="#Why-13th-Feb" class="headerlink" title="Why 13th Feb?"></a>Why 13th Feb?</h2><p>According to the modern Chinese habit of writing, &#39;13th Feb&#39; is usually written as &#39;2.13&#39;. Also, &#39;13&#39; and &#39;B&#39; look similar and are often regarded to be equal. In conclusion, &#39;13 Feb&#39; can be transformed to &#39;2B&#39;, which is a common word in Chinese. Although the word is sometimes classified as &quot;offensive&quot;, it reflects feelings of optimism, bravery and entertainment.</p><h2 id="How-do-people-celebrate-the-day"><a href="#How-do-people-celebrate-the-day" class="headerlink" title="How do people celebrate the day?"></a>How do people celebrate the day?</h2><p>When the anniversary was first set up, there was no officially specified ways of celebration. People gathered, held parties and enjoyed spending time together. </p><p>On the 3th linkinpark213 Day, a proposal by a high school Chinese teacher was taken as the official way of celebration -- on each linkinpark213 Day, a number of people participate in the Pigeon-Flying Competition funded by Harper Long. This way of celebration prevailed until now, and all participants except Harper Long won the competition every year.</p><p>Pigeon-Flying is a broadly-accepted traditional Chinese custom, the exact origin of which is too ancient to be revealed. Modern scholars tend to believe that this custom became well-known no later than 206 A.D. in China. In modern times, pigeon-flying is an activity involving &quot;making a promise&quot; and &quot;not keeping it&quot;. According to some folk stories, this activity was firstly named when it happened to a pigeon keeper when he forgot to keep a promise that he made with a friend. </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Promise promise = <span class="keyword">new</span> Promise();</span><br><span class="line">    promise.make(<span class="string">"I will come!"</span>);</span><br><span class="line">    System.exit(<span class="number">0</span>);</span><br><span class="line">    promise.keep();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>(Please notice that pigeon-flying mentioned here is not the same activity as what happens every Olympics since 1896 A.D.)</p><p>Join our celebration today! You can easily participate in the Pigeon-Flying Competition by not participating.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Cheers for the 8th International linkinpark213 Day!&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;Harper&#39;&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;Sweet&#39;&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;Kobayashi&#39;&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;Kawasaki&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;do&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;I&#39;m &lt;span class=&quot;variable&quot;&gt;$i&lt;/span&gt;, cheers!&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;done&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;What-is-linkinpark213-Day&quot;&gt;&lt;a href=&quot;#What-is-linkinpark213-Day&quot; class=&quot;headerlink&quot; title=&quot;What is linkinpark213 Day?&quot;&gt;&lt;/a&gt;What is linkinpark213 Day?&lt;/h2&gt;&lt;p&gt;International linkinpark213 Day is a global anniversary set up by Harper Long in 2011 A.D., celebrated on February 13th every year. This anniversary is officially written as &amp;#39;linkinpark213 Day&amp;#39;, the first letter of which is a lower-case. The establishment of the anniversary dates back to the early 10s in the 21st century.&lt;/p&gt;
&lt;p&gt;Till now, the population for this anniversary has already reached 1e-6 million, and the distribution also expanded from a small county to the whole middle-China area, including Hebei, Henan, Shanxi and Shaanxi province. There will also be some Japanese resident who plan to celebrate this day in 2019.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Other" scheme="http://linkinpark213.com/tags/Other/"/>
    
  </entry>
  
</feed>
