<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Linkin213&#39;s Park</title>
  
  <subtitle>Blog of linkinpark213</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://linkinpark213.com/"/>
  <updated>2018-04-22T13:53:37.810Z</updated>
  <id>http://linkinpark213.com/</id>
  
  <author>
    <name>Harper Long</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>A review of ResNet - Residual Networks</title>
    <link href="http://linkinpark213.com/2018/04/22/resnet/"/>
    <id>http://linkinpark213.com/2018/04/22/resnet/</id>
    <published>2018-04-22T06:55:35.000Z</published>
    <updated>2018-04-22T13:53:37.810Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="0-Introduction"><a href="#0-Introduction" class="headerlink" title="0 Introduction"></a>0 Introduction</h2><p>Deep learning researchers have been constructing skyscrapers in recent years. Especially, VGG nets and GoogLeNet have pushed the depths of convolutional networks to the extreme. But questions remain: if time and money aren&#39;t problems, are deeper networks always perform better? Not exactly.</p><p>When residual networks were proposed, researchers around the world was stunned by its depth. &quot;Jesus Christ! Is this a neural network or the Dubai Tower?&quot; <strong>Don&#39;t be afraid!</strong> Interestingly, these networks not only defeated all opponents in the classification, detection, localization challenges in ImageNet 2015, but were also the main innovation in the best paper of CVPR2016.</p><div align="center"><br>    <img src="/images/resnet/network_growth.jpg" width="40%" height="40%" alt="Network Growth"><br></div><a id="more"></a><h2 id="1-The-Crisis-Degradation-of-Deep-Networks"><a href="#1-The-Crisis-Degradation-of-Deep-Networks" class="headerlink" title="1 The Crisis: Degradation of Deep Networks"></a>1 The Crisis: Degradation of Deep Networks</h2><p>VGG nets proved the beneficial of representation depth of convolutional neural networks, at least within a certan range. However, when Kaiming He et al. tried to deepen some plain networks, the training error and test error stopped decreasing after the network reached a certain depth(which is not surprising) and soon degraded. This is not an overfitting problem, because training errors also increased; nor is it a gradient vanishing problem, because there are some techniques(e.g. batch normalization[4]) that ease the pain.</p><div align="center" class="figure"><br>    <img src="/images/resnet/downgrade.png" width="60%" height="60%" alt="The Downgrade Problem"><br>    Fig.1 The downgrade problem<br><br></div><p>What seems to be the cause of this degradation? Obviously, deeper neural networks are more difficult to train, but that doesn&#39;t mean deeper neural networks would yield worse results. To explain this problem, Balduzzi et al.[3] identified shattered gradient problem - as depth increases, gradients in standard feedforward networks increasingly resemble white noise. I will write about that later.</p><h2 id="2-A-Closer-Look-at-ResNet-The-Residual-Blocks"><a href="#2-A-Closer-Look-at-ResNet-The-Residual-Blocks" class="headerlink" title="2 A Closer Look at ResNet: The Residual Blocks"></a>2 A Closer Look at ResNet: The Residual Blocks</h2><p>As the old saying goes, &quot;千里之行，始于足下&quot;. Although ResNets are as deep as a thousand layers, they are built with these basic residual blocks(the right part of the figure). </p><div align="center" class="figure"><br>    <img src="/images/resnet/residual_blocks.png" width="50%" height="50%" alt="Comparison between normal weight layers and residual blocks"><br>    Fig.2 Parts of plain networks and a residual block(or residual unit)<br><br></div><h3 id="2-1-Skip-Connections"><a href="#2-1-Skip-Connections" class="headerlink" title="2.1 Skip Connections"></a>2.1 Skip Connections</h3><p>In comparison, basic units of plain network models would look like the one on the left: one ReLU function after a weight layer(usually also with biases), repeated several times. Let&#39;s denote the desired underlying mapping(the ideal mapping) of the two layers as \(\mathcal{H}(x)\), and the real mapping as \(\mathcal{F}(x)\). Clearly, the closer \(\mathcal{F}(x)\) is to \(\mathcal{H}(x)\), the better it fits.</p><p>However, He et al. explicitly let these layers fit a residual mapping instead of the desired underlying mapping. This is implemented with &quot;shortcut connections&quot;, which skip one or more layers, simply performing identity mappings and getting added to the outputs of the stacked weight layers. This way, \(\mathcal{F}(x)\) would not try to fit \(\mathcal{H}(x)\), but \(\mathcal{H}(x)-x\). The whole structure(from the identity mapping branch, to merging the branches by the addition operation) are named &quot;residual blocks&quot;. In fact, this block is still a little bit problematic - the output of one residual block is not the input of the next, since there is a ReLU activation function after the addition(It did NOT REALLY keep the identity mapping to the next block!). Therefore, in[2], He et al. fixed the residual blocks by changing the order of operations.</p><div align="center" class="figure"><br>    <img src="/images/resnet/identity_mapping.png" width="30%" height="30%" alt="New identity mapping"><br>    Fig.3 New identity mapping proposed by He et al.<br><br></div><p>What&#39;s the point in this? Let&#39;s do a simple analysis. The computation done by the original residual block is: $$y_l=h(x_l)+\mathcal{F}(x_l,\mathcal{W}_l),$$ $$x_{l+1}=f(y_l).$$</p><p>Definitions of symbols:<br>\(x_l\): input features to the \(l\)-th residual block;<br>\(\mathcal{W}_{l}={W_{l,k}|_{1\leq k\leq K}}\): a set of weights(and biases) associated with the \(l\)-th residual unit. \(K\) is the number of layers in this block;<br>\(\mathcal{F}(x,\mathcal{W})\): the residual function, which we talked about earlier. It&#39;s a stack of 2 conv. layers here;<br>\(f(x)\): the activation function. We are using ReLU here;<br>\(h(x)\): identity mapping.</p><p>If \(f(x)\) is also an identity mapping(as if we&#39;re not using any activation function), the first equation would become:<br>$$x_{l+1}=x_l+\mathcal{F}(x_l,\mathcal{W}_l)$$</p><p>Therefore, we can define \(x_L\) recursively of any layer:<br>$$x_L=x_l+\sum_{i=l}^{L}\mathcal{F}(x_i,\mathcal{W}_i)$$</p><p>That&#39;s not the end yet! When it comes to the gradients, according to the chain rules of backpropagation, we have a beautiful definition:<br>$$\frac{\partial{\mathcal{E}}}{\partial{x_l}}=\frac{\partial{\mathcal{E}}}{\partial{x_L}}\frac{\partial{x_L}}{\partial{x_l}}=\frac{\partial{\mathcal{E}}}{\partial{x_L}}\Big(1+\frac{\partial{}}{\partial{x_l}}\sum_{i=l}^{L}\mathcal{F}(x_i,\mathcal{W}_i)\Big)$$</p><p>What does it mean? it means that the information is directly backpropagated to ANY shallower block. This way, the gradients of a layer never vanish or explode even if the weights are too small or too big.</p><h3 id="2-2-Identity-Mappings"><a href="#2-2-Identity-Mappings" class="headerlink" title="2.2 Identity Mappings"></a>2.2 Identity Mappings</h3><p>It&#39;s important that we use identity mapping here! Just consider doing a simple modification here, for example, \(h(x)=\lambda_lx_l\)(\(\lambda_l\)) is a modulating scalar). The definition of \(x_L\) and \(\frac{\partial{\mathcal{E}}}{\partial{x_l}}\) woule become:<br>$$x_L=(\prod_{i=l}^{L-1}\lambda_i)x_l+\sum_{i=l}^{L}(\prod_{j=l}^{L-1}\lambda_j)\mathcal{F}(x_i,\mathcal{W}_i)$$<br>$$\frac{\partial{\mathcal{E}}}{\partial{x_l}}=\frac{\partial{\mathcal{E}}}{\partial{x_L}}\Big((\prod_{i=l}^{L-1}\lambda_i)+\frac{\partial{}}{\partial{x_l}}\sum_{i=l}^{L}(\prod_{j=l}^{L-1}\lambda_j)\mathcal{F}(x_i,\mathcal{W}_i)\Big)$$</p><p>For extremely deep neural networks where \(L\) is too large, \(\prod_{j=l}^{L-1}\lambda_j\) could be either too small or too large, causing gradient vanishing or gradient explosion. For\(h(x)\) with complex definitions, the gradient could be extremely complicated, thus losing the advantage of the skip connection. Skip connection works best under the condition where the grey channel in Fig. 3 cover no operations (except the addition) and is clean.</p><p>Interestingly, this comfirmed the philosophy of &quot;大道至简&quot; once again.</p><h3 id="2-3-Post-activation-or-Pre-activation"><a href="#2-3-Post-activation-or-Pre-activation" class="headerlink" title="2.3 Post-activation or Pre-activation?"></a>2.3 Post-activation or Pre-activation?</h3><p>Wait a second... &quot;\(f(x)\) is also an identity mapping&quot; is just out assumption. The activation function is still there!</p><p>Right. There IS an activation function, but it&#39;s moved to somewhere else.</p><p>Besides using a simple identity mapping, He et al. also discussed about the position of the activation function and the batch normalization operation. Assuming that we got a special(asymmetric) activation function \(\hat f(x)\), which only affects the path to the next residual unit. Now our definition of \(x_{x+1}\) would become:<br>$$x_{l+1}=x_l+\mathcal{F}(\hat f(x_l),\mathcal{W}_l)$$</p><p>With \(x_l\) still multiplied by 1, information is still fully backpropagated to shallower residual blocks. And the good thing is that using this asymmetric activation function after the addition(post-activation) is equivalent to using it beforehands(pre-activation)!</p><div align="center" class="figure"><br>    <img src="/images/resnet/pre-activation.png" width="80%" height="80%" alt="Asymmetric after-addition activation"><br>    Fig.4 Using asymmetric after-addition activation is equivalent to constructing a pre-activation residual unit<br><br></div><h2 id="3-ResNet-Architectures"><a href="#3-ResNet-Architectures" class="headerlink" title="3 ResNet Architectures"></a>3 ResNet Architectures</h2><p>Here are the ResNet architectures for ImageNet. Building blocks are shown in brackets, with the numbers of blocks stacked. With the first block of every stack(starting from conv3_x), a downsampling is performed. Each column represents one of the residual networks, and the deepest one has 152 weight layers! Since ResNets were proposed, VGG nets - which were officially called &quot;Very Deep Convolutional Networks&quot; - are not relatively deep anymore. Maybe call them &quot;A Little Bit Deep Convolutional Networks&quot;.</p><div align="center" class="figure"><br>    <img src="/images/resnet/architectures.png" width="80%" height="80%" alt="ResNet architectures for ImageNet"><br>    Table. 1 ResNet architectures for ImageNet.<br><br></div><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h2><h3 id="4-1-Performance-on-ImageNet"><a href="#4-1-Performance-on-ImageNet" class="headerlink" title="4.1 Performance on ImageNet"></a>4.1 Performance on ImageNet</h3><p>He et al. trained ResNet-18 and ResNet-34 on the ImageNet dataset, and also compared to plain convolutional networks. In Fig. 5, the thin curves denote training error, and the bold ones denote validation error. The figure on the left shows the results of plain convolution networks(in which the 34-layere ones has higher error rates than the 18-layered one), and the figure on the right shows that residual networks perform better than plain ones, while deeper ones perform better than shallow ones.</p><div align="center" class="figure"><br>    <img src="/images/resnet/architectures.png" width="80%" height="80%" alt="Training ResNet on ImageNet"><br>    Fig. 5 Training ResNet on ImageNet<br><br></div><h3 id="4-2-Effects-of-Different-Shortcut-Connections"><a href="#4-2-Effects-of-Different-Shortcut-Connections" class="headerlink" title="4.2 Effects of Different Shortcut Connections"></a>4.2 Effects of Different Shortcut Connections</h3><p>He et al. also tried various types of shortcut connections to replace the identity mapping, and various positions of activation functions / batch normalizations. Experiments show that the original identity mapping and full pre-activation yield the best results.</p><div align="center" class="figure"><br>    <img src="/images/resnet/shortcut-connections.png" width="50%" height="50%" alt="Different shortcuts of residual units"><br>    Fig. 6 Various shortcuts in residual units<br><br>    <img src="/images/resnet/shortcut-connections-experiment.png" width="70%" height="70%" alt="Classification errors with different shortcuts"><br>    Table. 2 Classification error on CIFAR-10 test set with various shortcut connections in residual units<br><br>    <img src="/images/resnet/activations.png" width="70%" height="50%" alt="Different usages of activation in residual units"><br>    Fig. 7 Various usages of activation in residual units<br><br>    <img src="/images/resnet/activations-experiment.png" width="50%" height="50%" alt="Classification errors with different activations"><br>    Table. 3 Classification error on CIFAR-10 test set with various usages of activation in residual units<br><br></div><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p>Residual learning can be crowned as &quot;GREATEST HITS IN DEEP LEARNING FIELDS&quot;. With a simple identity mapping, it solved the degradation problem of deep neural networks. Now that you have learned about the concept of ResNet, why not give it a try and implement your first residual learning model today?</p><div align="center" class="figure"><br>    <img src="/images/resnet/resnet-yooo.jpg" width="40%" height="40%" alt=""><br></div><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" target="_blank" rel="noopener">He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</a></p><p>[2] <a href="https://arxiv.org/pdf/1603.05027.pdf" target="_blank" rel="noopener">He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks[C]//European Conference on Computer Vision. Springer, Cham, 2016: 630-645.</a></p><p>[3] <a href="https://arxiv.org/pdf/1702.08591.pdf" target="_blank" rel="noopener">Balduzzi D, Frean M, Leary L, et al. The Shattered Gradients Problem: If resnets are the answer, then what is the question?[J]. arXiv preprint arXiv:1702.08591, 2017.</a></p><p>[4] <a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.</a></p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;0-Introduction&quot;&gt;&lt;a href=&quot;#0-Introduction&quot; class=&quot;headerlink&quot; title=&quot;0 Introduction&quot;&gt;&lt;/a&gt;0 Introduction&lt;/h2&gt;&lt;p&gt;Deep learning researchers have been constructing skyscrapers in recent years. Especially, VGG nets and GoogLeNet have pushed the depths of convolutional networks to the extreme. But questions remain: if time and money aren&amp;#39;t problems, are deeper networks always perform better? Not exactly.&lt;/p&gt;
&lt;p&gt;When residual networks were proposed, researchers around the world was stunned by its depth. &amp;quot;Jesus Christ! Is this a neural network or the Dubai Tower?&amp;quot; &lt;strong&gt;Don&amp;#39;t be afraid!&lt;/strong&gt; Interestingly, these networks not only defeated all opponents in the classification, detection, localization challenges in ImageNet 2015, but were also the main innovation in the best paper of CVPR2016.&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;&lt;br&gt;    &lt;img src=&quot;/images/resnet/network_growth.jpg&quot; width=&quot;40%&quot; height=&quot;40%&quot; alt=&quot;Network Growth&quot;&gt;&lt;br&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://linkinpark213.com/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://linkinpark213.com/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>A review of VGG net - Very Deep Convolutional Neural Networks</title>
    <link href="http://linkinpark213.com/2018/04/21/vgg/"/>
    <id>http://linkinpark213.com/2018/04/21/vgg/</id>
    <published>2018-04-21T08:15:55.000Z</published>
    <updated>2018-04-22T13:09:26.494Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="0-Introduction"><a href="#0-Introduction" class="headerlink" title="0 Introduction"></a>0 Introduction</h2><p>Convolutional neural networks(CNN) have enjoyed great success in conputer vision research fields in the past few years. A number of attempts are made based on the original CNN architecture to improve its accuracy and performance. In 2014, Karen Simonyan et al. did an investigation on the effect of depth on CNNs&#39; accuracy in large-scale image recognition (thus also proposing a series of very deep CNNs which are usually called VGG nets). The result confirmed the importance of CNN depth in visual representations.</p><h2 id="1-Background-VGG-net-39-s-ancestors"><a href="#1-Background-VGG-net-39-s-ancestors" class="headerlink" title="1 Background: VGG net&#39;s ancestors"></a>1 Background: VGG net&#39;s ancestors</h2><p>Before introcducing VGG net, let&#39;s take a glance at prior convolutional neural networks. </p><h3 id="1-1-LeNet-The-Origin"><a href="#1-1-LeNet-The-Origin" class="headerlink" title="1.1 LeNet: The Origin"></a>1.1 LeNet: The Origin</h3><p>Basic neural network structures(for example, multi-layer preceptron) learn patterns on 1D vectors, which cannot cope with 2D features in images well. In 1986, Lecun et al. proposed a convolution network model called LeNet-5. Its structure is fairly simple: two convolution layers, two subsampling layers and a few fully connected layers. This network was used to solve a number recognition problem. (If you need to learn more about the convolution operation, please refer to Google or _Digital Image Processing_ by Rafael C. Gonzalez)</p><a id="more"></a><div align="center"><br>    <img src="/images/lenet.png" width="80%" height="60%" alt="LeNet"><br></div><h3 id="1-2-AlexNet-The-Powerful-Convolution"><a href="#1-2-AlexNet-The-Powerful-Convolution" class="headerlink" title="1.2 AlexNet: The Powerful Convolution"></a>1.2 AlexNet: The Powerful Convolution</h3><p>In 2012, Alex Krizhevsky et al. won the first place in ILSVRC-2012(ImageNet Large-Scale Visual Recognition Challenge 2012) and achieved the highest top-5 error rate of 15.3% with a convolutional network model, while the second-best entry only achieved 26.2%. The network, namely AlexNet, was trained on two GTX580 3GB GPUs parallenized. Since a single GTX580 GPU has only 3GB memory, the maximum size of networks is limited. This model proved the effectiveness of CNNs under complicated circumstances and the power of GPUs. So what if the network can go deeper? Will the top-5 error rate get even lower?</p><div align="center"><br><img src="/images/alexnet.png" alt="AlexNet"><br></div><h2 id="2-Main-Contributions-of-VGG-Nets"><a href="#2-Main-Contributions-of-VGG-Nets" class="headerlink" title="2 Main Contributions of VGG Nets"></a>2 Main Contributions of VGG Nets</h2><p>Here comes our hero - VGG nets. By the way, VGG is not the name of the network, but the name of the authors&#39; group - _Visual Geometry Group_, from Department of Engineering Science, University of Oxford. The networks they proposed were therefore named after the group. The main contributions of VGG nets are: 1. more but smaller convolution filters; 2. great depth of networks.</p><h3 id="2-1-Stacks-of-Smaller-Convolution-Filters"><a href="#2-1-Stacks-of-Smaller-Convolution-Filters" class="headerlink" title="2.1 Stacks of Smaller Convolution Filters"></a>2.1 Stacks of Smaller Convolution Filters</h3><p>Rather than using relatively large receptive fields in the first convolution layers, Simonyan et al. selected very small 3x3 receptive fields throughout the whole net, which are convolved with the input at every pixel with a stride of 1. As is shown in the figures below, a stack of two 3x3 convolution layers has an effective receptive field of 5x5. We can also conclude that a stack of three 3x3 convolution filters has an effective receptive field of 7x7.</p><div align="center"><br>    <img src="/images/conv1.png" width="50%" height="50%" alt="Conv5x5"><br>    <img src="/images/conv2.png" width="60%" height="60%" alt="Conv3x3x2"><br></div><p>Now that we&#39;re clear that stacks of small-kernel convolution layers have equal sized receptive fields, why are they the better choice? Well, the first advantage is incorporating more rectification layers instead of a single one, since every convolution layer includes an activation function(usually ReLU). More rectifacation brings more non-linearity, and more non-linearity makes the decision function more discriminative and fit better. Also, when the receptive field isn&#39;t too large, a stack of 3x3 convolution filters have fewer parameters to train. Assuming the number of input and output channels of a convolution layer stack are equal(let&#39;s call it C) and the receptive field is 5x5, we have \(2*3*3*C*C=18C^2\) instead of \(5*5*C*C=25C^2\) parameters here. Similarly, when the receptive field is 7x7, we have \(3*3*3*C*C=27C^2\) instead of \(7*7*C*C=49C^2\). When the field gets even larger? A function with \(O(n)\) complexity only has greater advantage against an \(O(n^2)\) when \(n\) grows.</p><h3 id="2-2-Deep-Dark-Fantasy"><a href="#2-2-Deep-Dark-Fantasy" class="headerlink" title="2.2 Deep Dark Fantasy"></a>2.2 Deep Dark Fantasy</h3><p>Cliches time. Just like any blogger mentioning VGG nets would do, here are the network structures proposed by Simonyan et al.</p><div align="center"><br>    <img src="/images/vggnets.png" width="60%" height="60%" alt="VGG Nets"><br></div><p>Look at the table column-by-column. Each column(A, A-LRN, B, C, D, E) corresponds to one network structure. As you can see, their networks grew from 11 layers(in net A) to 19 layers(in net E). Each time something is added to the previous net, it would appear bold. Clearly, LRN(Local Response Normalization) didn&#39;t work well in this case(actually, A-LRN net performed worse than A, while comsuming much more memory and computation time), and was thus removed. </p><p>What&#39;s worth mentioning are the 1x1 convolution layers appearing in network C. This is a way to increase non-linearity(also by introducing activation functions) of the decision function while also keeping the size of the receptive fields unchanged.</p><h2 id="3-Training-amp-Evaluation"><a href="#3-Training-amp-Evaluation" class="headerlink" title="3 Training &amp; Evaluation"></a>3 Training &amp; Evaluation</h2><p>Bad initialization could stall learning due to the instability of gradient deep networks. Therefore, the authors first trained the network A, which is shallow enough to be trained with random initialization. Then, the next networks (B to E) are initialized with the pre-trained models, and only weights of the new layers are randomly initialized.</p><p>In spite of the larger number of parameters and the greater depth ofour nets compared to AlexNet, the nets required less epochs to converge due to the implicit regularization imposed by greater depth, smaller convolution filter sizes and the pre-initialisation of certain layers. They also generalize well to other datasets, achieving state-of-the-art performances. Results of VGG nets in comparison against other models in ILSVRC are shown in the table below.</p><div align="center"><br>    <img src="/images/VGG-preformance-comparison.png" width="70%" height="70%" alt="VGG net results"><br></div><p>In conclusion, the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. </p><p>You might ask: Why not even deeper, with more powerful GPUs(the authors used Titan Black), we can absolutely train deeper networks that perform better! Not exactly. Problems arose as the networks get too deep, and this is where ResNet comes in.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="https://arxiv.org/pdf/1409.1556/" target="_blank" rel="noopener">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.</a></p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;0-Introduction&quot;&gt;&lt;a href=&quot;#0-Introduction&quot; class=&quot;headerlink&quot; title=&quot;0 Introduction&quot;&gt;&lt;/a&gt;0 Introduction&lt;/h2&gt;&lt;p&gt;Convolutional neural networks(CNN) have enjoyed great success in conputer vision research fields in the past few years. A number of attempts are made based on the original CNN architecture to improve its accuracy and performance. In 2014, Karen Simonyan et al. did an investigation on the effect of depth on CNNs&amp;#39; accuracy in large-scale image recognition (thus also proposing a series of very deep CNNs which are usually called VGG nets). The result confirmed the importance of CNN depth in visual representations.&lt;/p&gt;
&lt;h2 id=&quot;1-Background-VGG-net-39-s-ancestors&quot;&gt;&lt;a href=&quot;#1-Background-VGG-net-39-s-ancestors&quot; class=&quot;headerlink&quot; title=&quot;1 Background: VGG net&amp;#39;s ancestors&quot;&gt;&lt;/a&gt;1 Background: VGG net&amp;#39;s ancestors&lt;/h2&gt;&lt;p&gt;Before introcducing VGG net, let&amp;#39;s take a glance at prior convolutional neural networks. &lt;/p&gt;
&lt;h3 id=&quot;1-1-LeNet-The-Origin&quot;&gt;&lt;a href=&quot;#1-1-LeNet-The-Origin&quot; class=&quot;headerlink&quot; title=&quot;1.1 LeNet: The Origin&quot;&gt;&lt;/a&gt;1.1 LeNet: The Origin&lt;/h3&gt;&lt;p&gt;Basic neural network structures(for example, multi-layer preceptron) learn patterns on 1D vectors, which cannot cope with 2D features in images well. In 1986, Lecun et al. proposed a convolution network model called LeNet-5. Its structure is fairly simple: two convolution layers, two subsampling layers and a few fully connected layers. This network was used to solve a number recognition problem. (If you need to learn more about the convolution operation, please refer to Google or _Digital Image Processing_ by Rafael C. Gonzalez)&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://linkinpark213.com/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://linkinpark213.com/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Smartypants is NOT SO SMART</title>
    <link href="http://linkinpark213.com/2018/03/20/smartypants/"/>
    <id>http://linkinpark213.com/2018/03/20/smartypants/</id>
    <published>2018-03-20T03:00:22.000Z</published>
    <updated>2018-03-20T12:19:17.411Z</updated>
    
    <content type="html"><![CDATA[<p>When blogging with Hexo, every time I type a single quotation mark(also called apostrophe) like this:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;</span><br></pre></td></tr></table></figure></p><p>, Hexo would convert it to a symbol like this<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">’</span><br></pre></td></tr></table></figure></p><p>You would say that this is also an apotrophe, but it really looks UNBEARABLE in the articles. It&#39;s been a problem bothering me for more than a month.(I&#39;m not saying that this is the reason for not updating my blog, but I don&#39;t mind if you think so!)</p><a id="more"></a><p><img src="/images/apostrophe.png" alt="apostrophe"></p><p>Therefore, I Googled about this problem and tried to find other victims. According to their sharings, this problem is caused by <em>marked</em> -- the default markdown renderer of Hexo.The <em>&quot;smatrypants&quot;</em> function of marked was turned on by default.</p><p>Now take a look at the introduction of <em>smartypants</em> on the <em>hexo-renderer-marked</em> page:</p><blockquote><p><em>smartypants</em> - Use &quot;smart&quot; typograhic punctuation for things like quotes and dashes.</p></blockquote><p>C&#39;mon, seriously? </p><p>There are a few bloggers who solved this by adding the code below to the _config.yml file in the blog directory. </p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mark:</span></span><br><span class="line"><span class="attr">  smartypants:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>This worked for most victims (perhaps all of them), but not for me. I have no idea why those config wasn&#39;t working, so if anyone finds out the reason, please contact me by e-mail.</p><p>If you&#39;re sure <em>smartypants</em> is causing the problem, and the solution above didn&#39;t work for you either, maybe you can try my solution.</p><p>Since <em>hexo-renderer-marked</em> is installed in the blog&#39;s <em>node_modules</em> directory(may also be in your Node.js directory if installed globally), isn&#39;t it possible that we change its own configurations? I looked at the <em>index.js</em> file in the <em>node_modules/hexo-renderer-marked/</em> directory. There you are, smartypants!</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hexo.config.marked = assign(&#123;</span><br><span class="line">  gfm: <span class="literal">true</span>,</span><br><span class="line">  pedantic: <span class="literal">false</span>,</span><br><span class="line">  sanitize: <span class="literal">false</span>,</span><br><span class="line">  tables: <span class="literal">true</span>,</span><br><span class="line">  breaks: <span class="literal">true</span>,</span><br><span class="line">  smartLists: <span class="literal">true</span>,</span><br><span class="line">  smartypants: <span class="literal">true</span>,</span><br><span class="line">  modifyAnchors: <span class="string">''</span>,</span><br><span class="line">  autolink: <span class="literal">true</span></span><br><span class="line">&#125;, hexo.config.marked);</span><br></pre></td></tr></table></figure><p>Now you know what to do.</p><p>Aaaaaaaaaaaaaaaand many thanks to Xizi Wu, the artist of my new avatar! I love it!<br><img src="/images/long_nobg.png" alt="Harper Long by Xizi Wu"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;When blogging with Hexo, every time I type a single quotation mark(also called apostrophe) like this:&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;apos;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;, Hexo would convert it to a symbol like this&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;’&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;You would say that this is also an apotrophe, but it really looks UNBEARABLE in the articles. It&amp;#39;s been a problem bothering me for more than a month.(I&amp;#39;m not saying that this is the reason for not updating my blog, but I don&amp;#39;t mind if you think so!)&lt;/p&gt;
    
    </summary>
    
    
      <category term="Blogging" scheme="http://linkinpark213.com/tags/Blogging/"/>
    
  </entry>
  
  <entry>
    <title>Cheers for the 8th International linkinpark213 Day!</title>
    <link href="http://linkinpark213.com/2018/02/13/linkinpark213-day/"/>
    <id>http://linkinpark213.com/2018/02/13/linkinpark213-day/</id>
    <published>2018-02-13T07:03:02.047Z</published>
    <updated>2018-03-20T12:23:12.255Z</updated>
    
    <content type="html"><![CDATA[<p>Cheers for the 8th International linkinpark213 Day!</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="string">'Harper'</span> <span class="string">'Sweet'</span> <span class="string">'Kobayashi'</span> <span class="string">'Kawasaki'</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"I'm <span class="variable">$i</span>, cheers!"</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h2 id="What-is-linkinpark213-Day"><a href="#What-is-linkinpark213-Day" class="headerlink" title="What is linkinpark213 Day?"></a>What is linkinpark213 Day?</h2><p>International linkinpark213 Day is a global anniversary set up by Harper Long in 2011 A.D., celebrated on February 13th every year. This anniversary is officially written as &#39;linkinpark213 Day&#39;, the first letter of which is a lower-case. The establishment of the anniversary dates back to the early 10s in the 21st century.</p><p>Till now, the population for this anniversary has already reached 1e-6 million, and the distribution also expanded from a small county to the whole middle-China area, including Hebei, Henan, Shanxi and Shaanxi province. There will also be some Japanese resident who plan to celebrate this day in 2019.</p><a id="more"></a><h2 id="Why-13th-Feb"><a href="#Why-13th-Feb" class="headerlink" title="Why 13th Feb?"></a>Why 13th Feb?</h2><p>According to the modern Chinese habit of writing, &#39;13th Feb&#39; is usually written as &#39;2.13&#39;. Also, &#39;13&#39; and &#39;B&#39; look similar and are often regarded to be equal. In conclusion, &#39;13 Feb&#39; can be transformed to &#39;2B&#39;, which is a common word in Chinese. Although the word is sometimes classified as &quot;offensive&quot;, it reflects feelings of optimism, bravery and entertainment.</p><h2 id="How-do-people-celebrate-the-day"><a href="#How-do-people-celebrate-the-day" class="headerlink" title="How do people celebrate the day?"></a>How do people celebrate the day?</h2><p>When the anniversary was first set up, there was no officially specified ways of celebration. People gathered, held parties and enjoyed spending time together. </p><p>On the 3th linkinpark213 Day, a proposal by a high school Chinese teacher was taken as the official way of celebration -- on each linkinpark213 Day, a number of people participate in the Pigeon-Flying Competition funded by Harper Long. This way of celebration prevailed until now, and all participants except Harper Long won the competition every year.</p><p>Pigeon-Flying is a broadly-accepted traditional Chinese custom, the exact origin of which is too ancient to be revealed. Modern scholars tend to believe that this custom became well-known no later than 206 A.D. in China. In modern times, pigeon-flying is an activity involving &quot;making a promise&quot; and &quot;not keeping it&quot;. According to some folk stories, this activity was firstly named when it happened to a pigeon keeper when he forgot to keep a promise that he made with a friend. </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Promise promise = <span class="keyword">new</span> Promise();</span><br><span class="line">    promise.make(<span class="string">"I will come!"</span>);</span><br><span class="line">    System.exit(<span class="number">0</span>);</span><br><span class="line">    promise.keep();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>(Please notice that pigeon-flying mentioned here is not the same activity as what happens every Olympics since 1896 A.D.)</p><p>Join our celebration today! You can easily participate in the Pigeon-Flying Competition by not participating.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Cheers for the 8th International linkinpark213 Day!&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;Harper&#39;&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;Sweet&#39;&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;Kobayashi&#39;&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;Kawasaki&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;do&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;I&#39;m &lt;span class=&quot;variable&quot;&gt;$i&lt;/span&gt;, cheers!&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;done&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;What-is-linkinpark213-Day&quot;&gt;&lt;a href=&quot;#What-is-linkinpark213-Day&quot; class=&quot;headerlink&quot; title=&quot;What is linkinpark213 Day?&quot;&gt;&lt;/a&gt;What is linkinpark213 Day?&lt;/h2&gt;&lt;p&gt;International linkinpark213 Day is a global anniversary set up by Harper Long in 2011 A.D., celebrated on February 13th every year. This anniversary is officially written as &amp;#39;linkinpark213 Day&amp;#39;, the first letter of which is a lower-case. The establishment of the anniversary dates back to the early 10s in the 21st century.&lt;/p&gt;
&lt;p&gt;Till now, the population for this anniversary has already reached 1e-6 million, and the distribution also expanded from a small county to the whole middle-China area, including Hebei, Henan, Shanxi and Shaanxi province. There will also be some Japanese resident who plan to celebrate this day in 2019.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Other" scheme="http://linkinpark213.com/tags/Other/"/>
    
  </entry>
  
</feed>
