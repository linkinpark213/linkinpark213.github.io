<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Longing for sth New</title>
  
  <subtitle>Blog of linkinpark213</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://linkinpark213.com/"/>
  <updated>2019-06-23T11:25:12.806Z</updated>
  <id>http://linkinpark213.com/</id>
  
  <author>
    <name>Harper Long</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>A Summary of CVPR19 Visual Tracking Papers</title>
    <link href="http://linkinpark213.com/2019/06/11/cvpr19-track/"/>
    <id>http://linkinpark213.com/2019/06/11/cvpr19-track/</id>
    <published>2019-06-11T08:27:10.000Z</published>
    <updated>2019-06-23T11:25:12.806Z</updated>
    
    <content type="html"><![CDATA[<p>Here&#39;s my brief summary of all CVPR19 papers in the field of visual tracking. Abbreviations without parentheses are part of the paper title, and those with parentheses are added by me according to the paper.</p><a id="more"></a><h1 id="RGB-based"><a href="#RGB-based" class="headerlink" title="RGB-based"></a>RGB-based</h1><h2 id="Single-Object-Tracking"><a href="#Single-Object-Tracking" class="headerlink" title="Single-Object Tracking"></a>Single-Object Tracking</h2><p><strong>(UDT): Unsupervised Deep Tracking</strong><br><strong>Authors</strong>: Ning Wang, Yibing Song, Chao Ma, Wengang Zhou, Wei Liu, Houqiang Li<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1904.01828" target="_blank" rel="noopener">https://arxiv.org/abs/1904.01828</a><br><strong>Project Link</strong>: <a href="https://github.com/594422814/UDT" target="_blank" rel="noopener">https://github.com/594422814/UDT</a><br><strong>Summary</strong>: Train a robust siamese network on large-scale unlabeled videos in an unsupervised manner - forward-and-backward, i.e., the tracker can forward localize the target object in successive frames and backtrace to its initial position in the first frame.<br><strong>Highlights</strong>: Unsupervised learning</p><p><strong>(TADT): Target-Aware Deep Tracking</strong><br><strong>Authors</strong>: Xin Li, Chao Ma, Baoyuan Wu, Zhenyu He, Ming-Hsuan Yang<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1904.01772" target="_blank" rel="noopener">https://arxiv.org/abs/1904.01772</a><br><strong>Project Link</strong>: <a href="https://xinli-zn.github.io/TADT-project-page/" target="_blank" rel="noopener">https://xinli-zn.github.io/TADT-project-page/</a><br><strong>Summary</strong>: Targets of interest can be arbitrary object class with arbitrary forms, while pre-trained deep features are less effective in modeling these targets of arbitrary forms for distinguishing them from the background. TADT learns target-aware features, thus can better recognize the targets undergoing significant appearance variations than pre-trained deep features.<br><strong>Highlights</strong>: Target-aware features, better discrimination</p><p><strong>(SiamMask): Fast Online Object Tracking and Segmentation: A Unifying Approach</strong><br><strong>Authors</strong>: Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, Philip H.S. Torr<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1812.05050" target="_blank" rel="noopener">https://arxiv.org/abs/1812.05050</a><br><strong>Project Link</strong>: <a href="https://github.com/foolwood/SiamMask" target="_blank" rel="noopener">https://github.com/foolwood/SiamMask</a><br>Zhihu Link: <a href="https://zhuanlan.zhihu.com/p/58154634" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/58154634</a><br><strong>Summary</strong>: Perform both visual object tracking and semi-supervised video object segmentation, in real-time, with a single simple approach.<br><strong>Highlights</strong>: Mask prediction in tracking</p><p><strong>SiamRPN++: Evolution of Siamese Visual Tracking With Very Deep Networks</strong><br><strong>Authors</strong>: Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, Junjie Yan<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1812.11703" target="_blank" rel="noopener">https://arxiv.org/abs/1812.11703</a><br><strong>Project Link</strong>: <a href="http://bo-li.info/SiamRPN++/" target="_blank" rel="noopener">http://bo-li.info/SiamRPN++/</a><br><strong>Summary</strong>: SiamRPN++ breaks the translation invariance restriction through a simple yet effective spatial-aware sampling strategy. SiamRPN++ performs depth-wise and layer-wise aggregations, improving the accuracy but also reduces the model size. Current state-of-the-art in OTB2015, VOT2018, UAV123, LaSOT, and TrackingNet.<br><strong>Highlights</strong>: Deep backbones, state-of-the-art</p><p><strong>(CIR/SiamDW): Deeper and Wider Siamese Networks for Real-Time Visual Tracking</strong><br><strong>Authors</strong>: Zhipeng Zhang, Houwen Peng<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1901.01660" target="_blank" rel="noopener">https://arxiv.org/abs/1901.01660</a><br><strong>Project Link</strong>: <a href="https://github.com/researchmm/SiamDW" target="_blank" rel="noopener">https://github.com/researchmm/SiamDW</a><br><strong>Summary</strong>: SiamDW explores utilizing deeper and wider network backbones in another aspect - careful designs of residual units, considering receptive field, stride, output feature size - to eliminate the negative impact of padding in deep network backbones.<br><strong>Highlights</strong>: Cropping-Inside-Residual, eliminating the negative impact of padding</p><p><strong>(SiamC-RPN): Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking</strong><br><strong>Authors</strong>: Heng Fan, Haibin Ling<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1812.06148" target="_blank" rel="noopener">https://arxiv.org/abs/1812.06148</a><br><strong>Project Link</strong>: None<br><strong>Summary</strong>: Previously proposed one-stage Siamese-RPN trackers degenerate in presence of similar distractors and large scale variation. Advantages: 1) Each RPN in Siamese C-RPN is trained using outputs of the previous RPN, thus simulating hard negative sampling. 2) Feature transfer blocks (FTB) further improving the discriminability. 3) The location and shape of the target in each RPN is progressively refined, resulting in better localization.<br><strong>Highlights</strong>: Cascaded RPN, excellent accuracy</p><p><strong>SPM-Tracker: Series-Parallel Matching for Real-Time Visual Object Tracking</strong><br><strong>Authors</strong>: Guangting Wang, Chong Luo, Zhiwei Xiong, Wenjun Zeng<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1904.04452" target="_blank" rel="noopener">https://arxiv.org/abs/1904.04452</a><br><strong>Project Link</strong>: None<br><strong>Summary</strong>: To overcome the simultaneous requirements on robustness and discrimination power, SPM-Tracker tackle the challenge by connecting a coarse matching stage and a fine matching stage, taking advantage of both stages, resulting in superior performance, and exceeding other real-time trackers by a notable margin.<br><strong>Highlights</strong>: Coarse matching &amp; fine matching</p><p><strong>ATOM: Accurate Tracking by Overlap Maximization</strong><br><strong>Authors</strong>: Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, Michael Felsberg<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1811.07628" target="_blank" rel="noopener">https://arxiv.org/abs/1811.07628</a><br><strong>Project Link</strong>: <a href="https://github.com/visionml/pytracking" target="_blank" rel="noopener">https://github.com/visionml/pytracking</a><br><strong>Summary</strong>: Target estimation is a complex task, requiring highlevel knowledge about the object, while most trackers only resort to a simple multi-scale search. In comparison, ATOM estimate target states by predicting the overlap between the target object and an estimated bounding box. Besides, a classification component that is trained online to guarantee high discriminative power in the presence of distractors.<br><strong>Highlights</strong>: Overlap IoU prediction</p><p><strong>(GCT): Graph Convolutional Tracking</strong><br><strong>Authors</strong>: Junyu Gao, Tianzhu Zhang, Changsheng Xu<br><strong>arXiv Link</strong>: None<br><strong>PDF Link</strong>: <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Gao_Graph_Convolutional_Tracking_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Gao_Graph_Convolutional_Tracking_CVPR_2019_paper.pdf</a><br><strong>Project Link</strong>: <a href="http://nlpr-web.ia.ac.cn/mmc/homepage/jygao/gct_cvpr2019.html" target="_blank" rel="noopener">http://nlpr-web.ia.ac.cn/mmc/homepage/jygao/gct_cvpr2019.html</a><br><strong>Summary</strong>: Spatial-temporal information can provide diverse features to enhance the target representation. GCT incorporates 1) a spatial-temporal GCN to model the structured representation of historical target exemplars, and 2) a context GCN to utilize the context of the current frame to learn adaptive features for target localization.<br><strong>Highlights</strong>: Graph convolution networks, spatial-temporal information</p><p><strong>(ASRCF): Visual Tracking via Adaptive Spatially-Regularized Correlation Filters</strong><br><strong>Authors</strong>: Kenan Dai, Dong Wang, Huchuan Lu, Chong Sun, Jianhua Li<br><strong>arXiv Link</strong>: None<br><strong>Project Link</strong>: <a href="https://github.com/Daikenan/ASRCF" target="_blank" rel="noopener">https://github.com/Daikenan/ASRCF</a> (To be updated)<br><strong>Summary</strong>: ASRCF simultaneously optimize the filter coefficients and the spatial regularization weight. ASRCF applies two correlation filters (CFs) to estimate the location and scale respectively - 1) location CF model, which exploits ensembles of shallow and deep features to determine the optimal position accurately, and 2) scale CF model, which works on multi-scale shallow features to estimate the optimal scale efficiently.<br><strong>Highlights</strong>: Estimate location and scale respectively</p><p><strong>(RPCF): RoI Pooled Correlation Filters for Visual Tracking</strong><br><strong>Authors</strong>: Yuxuan Sun, Chong Sun, Dong Wang, You He, Huchuan Lu<br><strong>arXiv Link</strong>: None<br><strong>Project Link</strong>: None<br>PDF Link: <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_ROI_Pooled_Correlation_Filters_for_Visual_Tracking_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Sun_ROI_Pooled_Correlation_Filters_for_Visual_Tracking_CVPR_2019_paper.pdf</a><br><strong>Summary</strong>: RoI-based pooling can be equivalently achieved by enforcing additional constraints on the learned filter weights and thus becomes feasible on the virtual circular samples. Considering RoI pooling in the correlation filter formula, the RPCF performs favourably against other state-of-the-art trackers.<br><strong>Highlights</strong>: RoI pooling in correlation filters</p><h2 id="Multi-Object-Tracking"><a href="#Multi-Object-Tracking" class="headerlink" title="Multi-Object Tracking"></a>Multi-Object Tracking</h2><p><strong>(TBA): Tracking by Animation: Unsupervised Learning of Multi-Object Attentive Trackers</strong><br><strong>Authors</strong>: Zhen He, Jian Li, Daxue Liu, Hangen He, David Barber<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1809.03137" target="_blank" rel="noopener">https://arxiv.org/abs/1809.03137</a><br><strong>Project Link</strong>: <a href="https://github.com/zhen-he/tracking-by-animation" target="_blank" rel="noopener">https://github.com/zhen-he/tracking-by-animation</a><br><strong>Summary</strong>: The common Tracking-by-Detection (TBD) paradigm use supervised learning and treat detection and tracking separately. Instead, TBA is a differentiable neural model that first tracks objects from input frames, animates these objects into reconstructed frames, and learns by the reconstruction error through backpropagation. Besides, a Reprioritized Attentive Tracking is proposed to improve the robustness of data association.<br><strong>Highlights</strong>: Label-free, end-to-end MOT learning</p><p><strong>Eliminating Exposure Bias and Metric Mismatch in Multiple Object Tracking</strong><br><strong>Authors</strong>: Andrii Maksai, Pascal Fua<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1811.10984" target="_blank" rel="noopener">https://arxiv.org/abs/1811.10984</a><br><strong>Project Link</strong>: None<br><strong>Summary</strong>: Many state-of-the-art MOT approaches now use sequence models to solve identity switches but their training can be affected by biases. An iterative scheme of building a rich training set is proposed and used to learn a scoring function that is an explicit proxy for the target tracking metric.<br><strong>Highlights</strong>: Eliminating loss-evaluation mismatch</p><h2 id="Pose-Tracking"><a href="#Pose-Tracking" class="headerlink" title="Pose Tracking"></a>Pose Tracking</h2><p><strong>Multi-Person Articulated Tracking With Spatial and Temporal Embeddings</strong><br><strong>Authors</strong>: Sheng Jin, Wentao Liu, Wanli Ouyang, Chen Qian<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1903.09214" target="_blank" rel="noopener">https://arxiv.org/abs/1903.09214</a><br><strong>Project Link</strong>: None<br><strong>Summary</strong>: The framework consists of a SpatialNet and a TemporalNet, predicting (body part detection heatmaps + Keypoint Embedding (KE) + Spatial Instance Embedding (SIE)) and (Human Embedding (HE) + Temporal Instance Embedding (TIE)). Besides, a differentiable Pose-Guided Grouping (PGG) module to make the whole part detection and grouping pipeline fully end-to-end trainable.<br><strong>Highlights</strong>: Spatial &amp; temporal embeddings, end-to-end learning &quot;detection and grouping&quot; pipeline</p><p><strong>(STAF): Efficient Online Multi-Person 2D Pose Tracking With Recurrent Spatio-Temporal Affinity Fields</strong><br><strong>Authors</strong>: Yaadhav Raaj, Haroon Idrees, Gines Hidalgo, Yaser Sheikh<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1811.11975" target="_blank" rel="noopener">https://arxiv.org/abs/1811.11975</a><br><strong>Project Link</strong>: None<br><strong>Summary</strong>: Upon Part Affinity Field (PAF) representation designed for static images, an architecture encoding ans predicting Spatio-Temporal Affinity Fields (STAF) across a video sequence is proposed - a novel temporal topology cross-linked across limbs which can consistently handle body motions of a wide range of magnitudes. The network ingests STAF heatmaps from previous frames and estimates those for the current frame.<br><strong>Highlights</strong>: Online, fastest and the most accurate bottom-up approach</p><h1 id="RGBD-based"><a href="#RGBD-based" class="headerlink" title="RGBD-based"></a>RGBD-based</h1><p><strong>(OTR)</strong>: Object Tracking by Reconstruction With View-Specific Discriminative Correlation Filters<br><strong>Authors</strong>: Ugur Kart, Alan Lukezic, Matej Kristan, Joni-Kristian Kamarainen, Jiri Matas<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1811.10863" target="_blank" rel="noopener">https://arxiv.org/abs/1811.10863</a><br><strong>Summary</strong>: Perform online 3D target reconstruction to facilitate robust learning of a set of view-specific discriminative correlation filters (DCFs). State-of-the-art on Princeton RGB-D tracking and STC Benchmarks.</p><h1 id="Pointcloud-based"><a href="#Pointcloud-based" class="headerlink" title="Pointcloud-based"></a>Pointcloud-based</h1><p>I&#39;m not experienced in point clouds so I couldn&#39;t make a summary for the following papers. The abstracts are given below. Check them out at arXiv to learn more if you&#39;re interested.</p><p><strong>VITAMIN-E</strong>: VIsual Tracking and MappINg With Extremely Dense Feature Points<br><strong>Authors</strong>: Masashi Yokozuka, Shuji Oishi, Simon Thompson, Atsuhiko Banno<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1904.10324" target="_blank" rel="noopener">https://arxiv.org/abs/1904.10324</a><br><strong>Project Link</strong>: None<br><strong>Abstract</strong>: In this paper, we propose a novel indirect monocular SLAM algorithm called &quot;VITAMIN-E,&quot; which is highly accurate and robust as a result of tracking extremely dense feature points. Typical indirect methods have difficulty in reconstructing dense geometry because of their careful feature point selection for accurate matching. Unlike conventional methods, the proposed method processes an enormous number of feature points by tracking the local extrema of curvature informed by dominant flow estimation. Because this may lead to high computational cost during bundle adjustment, we propose a novel optimization technique, the &quot;subspace Gauss--Newton method&quot;, that significantly improves the computational efficiency of bundle adjustment by partially updating the variables. We concurrently generate meshes from the reconstructed points and merge them for an entire 3D model. The experimental results on the SLAM benchmark dataset EuRoC demonstrated that the proposed method outperformed state-of-the-art SLAM methods, such as DSO, ORB-SLAM, and LSD-SLAM, both in terms of accuracy and robustness in trajectory estimation. The proposed method simultaneously generated significantly detailed 3D geometry from the dense feature points in real time using only a CPU.</p><p><strong>Leveraging Shape Completion for 3D Siamese Tracking</strong><br><strong>Authors</strong>: Silvio Giancola<em>, Jesus Zarzar</em>, and Bernard Ghanem<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1903.01784" target="_blank" rel="noopener">https://arxiv.org/abs/1903.01784</a><br><strong>Project Link</strong>: <a href="https://github.com/SilvioGiancola/ShapeCompletion3DTracking" target="_blank" rel="noopener">https://github.com/SilvioGiancola/ShapeCompletion3DTracking</a><br><strong>Abstract</strong>: Point clouds are challenging to process due to their sparsity, therefore autonomous vehicles rely more on appearance attributes than pure geometric features. However, 3D LIDAR perception can provide crucial information for urban navigation in challenging light or weather conditions. In this paper, we investigate the versatility of Shape Completion for 3D Object Tracking in LIDAR point clouds. We design a Siamese tracker that encodes model and candidate shapes into a compact latent representation. We regularize the encoding by enforcing the latent representation to decode into an object model shape. We observe that 3D object tracking and 3D shape completion complement each other. Learning a more meaningful latent representation shows better discriminatory capabilities, leading to improved tracking performance. We test our method on the KITTI Tracking set using car 3D bounding boxes. Our model reaches a 76.94% Success rate and 81.38% Precision for 3D Object Tracking, with the shape completion regularization leading to an improvement of 3% in both metrics.</p><h1 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h1><p><strong>LaSOT</strong>: A High-Quality Benchmark for Large-Scale Single Object Tracking<br><strong>Authors</strong>: Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, Haibin Ling<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1809.07845" target="_blank" rel="noopener">https://arxiv.org/abs/1809.07845</a><br><strong>Project Link</strong>: <a href="https://cis.temple.edu/lasot/" target="_blank" rel="noopener">https://cis.temple.edu/lasot/</a><br><strong>Summary</strong>: A high-quality benchmark for <strong>La</strong>rge-scale <strong>S</strong>ingle <strong>O</strong>bject <strong>T</strong>racking, consisting of 1,400 sequences with more than 3.5M frames.</p><p><strong>CityFlow</strong>: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification<br>Zheng Tang, Milind Naphade, Ming-Yu Liu, Xiaodong Yang, Stan Birchfield, Shuo Wang, Ratnesh Kumar, David Anastasiu, Jenq-Neng Hwang<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1903.09254" target="_blank" rel="noopener">https://arxiv.org/abs/1903.09254</a><br><strong>Project Link</strong>: <a href="https://www.aicitychallenge.org/" target="_blank" rel="noopener">https://www.aicitychallenge.org/</a><br><strong>Summary</strong>: The largest-scale dataset in terms of spatial coverage and the number of cameras/videos in an urban environment,  consisting of more than 3 hours of synchronized HD videos from 40 cameras across 10 intersections, with the longest distance between two simultaneous cameras being 2.5 km.</p><p><strong>MOTS</strong>: Multi-Object Tracking and Segmentation<br><strong>Authors</strong>: Paul Voigtlaender, Michael Krause, Aljosa Osep, Jonathon Luiten, Berin Balachandar Gnana Sekar, Andreas Geiger, Bastian Leibe<br><strong>arXiv Link</strong>: <a href="https://arxiv.org/abs/1902.03604" target="_blank" rel="noopener">https://arxiv.org/abs/1902.03604</a><br><strong>Project Link</strong>: <a href="https://www.vision.rwth-aachen.de/page/mots" target="_blank" rel="noopener">https://www.vision.rwth-aachen.de/page/mots</a><br><strong>Summary</strong>: Going beyond 2D bounding boxes and extending the popular task of multi-object tracking to multi-object tracking and segmentation, in tasks and metrics.<br><strong>Highlights</strong>: Extend MOT with segmentation</p><p><strong>Argoverse</strong>: 3D Tracking and Forecasting With Rich Maps<br>Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, James Hays<br><strong>arXiv Link</strong>: None<br><strong>PDF Link</strong>: <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_CVPR_2019/papers/Chang_Argoverse_3D_Tracking_and_Forecasting_With_Rich_Maps_CVPR_2019_paper.pdf</a><br><strong>Project Link</strong>: Argoverse.org (Not working?)<br><strong>Summary</strong>: A dataset designed to support autonomous vehicle perception tasks including 3D tracking and motion forecasting.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Here&amp;#39;s my brief summary of all CVPR19 papers in the field of visual tracking. Abbreviations without parentheses are part of the paper title, and those with parentheses are added by me according to the paper.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://linkinpark213.com/tags/Deep-Learning/"/>
    
      <category term="Visual Tracking" scheme="http://linkinpark213.com/tags/Visual-Tracking/"/>
    
  </entry>
  
  <entry>
    <title>[è®ºæ–‡æ€»ç»“] ç†è§£FishNet</title>
    <link href="http://linkinpark213.com/2019/04/21/fishnet/"/>
    <id>http://linkinpark213.com/2019/04/21/fishnet/</id>
    <published>2019-04-21T06:50:36.000Z</published>
    <updated>2019-06-23T11:26:14.762Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><p>ä»VALSE2019å›æ¥åï¼Œæ„Ÿè§‰è‡ªå·±ä¿¨ç„¶å˜æˆäº†æ¬§é˜³ä¸‡é‡Œè€å¸ˆçš„è„‘æ®‹ç²‰å‘€â•°( á–â•°)ï¼ä¼šä¸Šæ¬§é˜³è€å¸ˆä»‹ç»çš„FishNetç®€ç›´è®©æˆ‘çœ¼å‰ä¸€äº®ï¼Œè¿™ä¹ˆå¥½çš„ç‚¹å­ï¼Œæˆ‘æ€ä¹ˆå°±æ²¡æƒ³åˆ°å‘ï¼å›æ¥å¥½å¥½è¯»äº†ä¸€ä¸‹æ–‡ç« å’Œä»£ç ï¼Œç®€å•æ€»ç»“ä¸€ä¸‹ã€‚</p><div align="center" class="figure"><br>  <img src="/images/fishnet/fish.jpg" width="15%" alt="æˆ‘å’¸ç”šï¼Œæ­¤é±¼ä½•èƒ½åŠä¹Ÿ"><br><br></div><a id="more"></a><h2 id="0-é—®é¢˜"><a href="#0-é—®é¢˜" class="headerlink" title="0 é—®é¢˜"></a>0 é—®é¢˜</h2><h3 id="0-1-å„å±‚ç‰¹å¾çš„èåˆæ–¹å¼"><a href="#0-1-å„å±‚ç‰¹å¾çš„èåˆæ–¹å¼" class="headerlink" title="0.1 å„å±‚ç‰¹å¾çš„èåˆæ–¹å¼"></a>0.1 å„å±‚ç‰¹å¾çš„èåˆæ–¹å¼</h3><p>è¾ƒæ—©çš„å…¸å‹æ·±åº¦CNNç»“æ„å¤§å¤šä¸ºæ¼æ–—çŠ¶ï¼Œä¸æ–­åœ°è¿›è¡Œå·ç§¯ã€ä¸‹é‡‡æ ·æ¥æå–ã€æµ“ç¼©å›¾åƒç‰¹å¾ï¼Œæœ€åç”¨ä¸€äº›å…¨è¿æ¥å±‚ä¹‹ç±»çš„ç»“æ„æ¥è®¡ç®—å…·ä½“ä»»åŠ¡çš„è¾“å‡ºç»“æœã€‚è¿™æ ·çš„è®¾è®¡å¾ˆè‡ªç„¶åœ°è¢«ç”¨äºå›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œå› ä¸ºè¾ƒæ·±çš„ç¥ç»ç½‘ç»œæ›´èƒ½å­¦ä¹ é«˜çº§è¯­ä¹‰ç‰¹å¾ï¼Œæœ€åå°†å›¾åƒæµ“ç¼©åˆ°ä¸€ä¸ªåƒç´ è€Œå˜æˆä¸€ä¸ªå‘é‡æ—¶ï¼Œè¿™ä¸ªåƒç´ çš„æ¯ä¸€ä¸ªé€šé“çš„å€¼åˆ™ä»£è¡¨äº†æ•´ä¸ªå›¾åƒåœ¨è¿™ä¸ªè¯­ä¹‰ç‰¹å¾ä¸Šçš„è¡¨ç°ã€‚</p><div align="center" class="figure"><img src="/images/fishnet/vgg.png" width="40%" alt="VGG-16"><br>Fig. 1 æ¼æ–—çŠ¶å·ç§¯ç¥ç»ç½‘ç»œï¼Œä»¥VGG-16ä¸ºä¾‹<br><br></div><p>ä½†æ˜¯å‘¢ï¼Œè¿™æ ·çš„ç»“æ„åŸå°ä¸åŠ¨åœ°åº”ç”¨åˆ°å…¶ä»–ä»»åŠ¡ä¸Šï¼Œæ•ˆæœå°±ä¸æ˜¯å¾ˆå¥½äº†ã€‚æ¯”å¦‚åœ¨åˆ†å‰²ä»»åŠ¡ä¸­ï¼Œç»†èŠ‚ç‰¹å¾ä¿ç•™å¾—å¥½çš„è¯ï¼Œåˆ†å‰²çš„æ•ˆæœåˆ™ä¼šæ›´ä½³ï¼ˆä¾‹å¦‚FCN-8sçš„æ•ˆæœè¿œå¥½äºFCN-32sï¼‰ã€‚åˆå¦‚åœ¨anchor-basedç›®æ ‡æ£€æµ‹æ¨¡å‹ä¸­ï¼Œç”¨å°ºå¯¸æ›´å¤§çš„ç‰¹å¾å›¾èƒ½å¤Ÿæ›´å¥½åœ°å›å½’è¾ƒå°ç›®æ ‡çš„å€™é€‰æ¡†ï¼ˆä¾‹å¦‚YOLOv3åŠ å…¥FPNåæ˜¾è‘—æå‡å°ç‰©ä½“çš„æ£€æµ‹æ•ˆæœï¼‰ã€‚å› æ­¤ï¼Œå‡ºç°äº†ä¸€äº›æ²™æ¼çŠ¶ç”šè‡³å¤šæ²™æ¼å †å çš„ç½‘ç»œç»“æ„ï¼ˆU-Netï¼ŒFPNï¼ŒStacked Hourglassç­‰ç­‰ï¼‰æ¥æ›´å¥½åœ°å¤„ç†è¿™äº›ä»»åŠ¡ã€‚</p><div align="center" class="figure"><img src="/images/fishnet/unet.png" width="40%" alt="U-Net"><br>    Fig. 2 æ²™æ¼çŠ¶å·ç§¯ç¥ç»ç½‘ç»œï¼Œä»¥U-Netä¸ºä¾‹<br><br></div><p>å¯ä»¥çœ‹åˆ°ï¼Œç±»ä¼¼è¿™æ ·çš„å·¥ä½œå¤§å¤šå‡ºäºè¿™æ ·çš„ä¸€ä¸ªæƒ³æ³•ï¼šåº•å±‚ç»†èŠ‚ç‰¹å¾å¾ˆé‡è¦ï¼Œæˆ‘ä»¬è¦æŠŠå®ƒèåˆåˆ°é¡¶å±‚è¯­ä¹‰ç‰¹å¾é‡Œå»ã€‚è¿™æ ·å°±æœ‰äººé—®äº†ï¼šé‚£è¯­ä¹‰ç‰¹å¾æ˜¯ä¸æ˜¯ä¹Ÿèƒ½èåˆåˆ°ç»†èŠ‚ç‰¹å¾é‡Œå»ï¼Œä»è€Œå¢å¼ºé«˜åˆ†è¾¨ç‡ç‰¹å¾å›¾çš„æ•ˆæœå‘¢ï¼ŸFishNetå°±åšåˆ°äº†è¿™æ ·çš„èåˆï¼Œè®©ç½‘ç»œæœ€åä¸€éƒ¨åˆ†çš„å„ä¸ªåˆ†è¾¨ç‡çš„ç‰¹å¾å›¾ä¸­çš„åº•å±‚ã€ä¸­å±‚ã€é¡¶å±‚ç‰¹å¾ï¼ˆä½œè€…åŸè¯ä¸ºpixel-level, region-level, image-levelï¼‰éƒ½èƒ½â€œä½ ä¸­æœ‰æˆ‘ï¼Œæˆ‘ä¸­æœ‰ä½ â€ã€‚</p><h3 id="0-2-æ¢¯åº¦åå‘ä¼ æ’­çš„é˜»ç¢"><a href="#0-2-æ¢¯åº¦åå‘ä¼ æ’­çš„é˜»ç¢" class="headerlink" title="0.2 æ¢¯åº¦åå‘ä¼ æ’­çš„é˜»ç¢"></a>0.2 æ¢¯åº¦åå‘ä¼ æ’­çš„é˜»ç¢</h3><p>åœ¨ResNetä¸­ï¼Œä½œè€…ç”¨ä¸€ç§å·§å¦™çš„åŠæ³•è®©è¾ƒæµ…çš„å±‚ä¹Ÿèƒ½å¾—åˆ°æœ‰æ•ˆçš„æ¢¯åº¦ä¿¡æ¯â€”â€”åœ¨æ¯å±‚å±‚çš„è¾“å‡ºä¸ŠåŠ ä¸€ä¸ªidentity mappingã€‚ä¹Ÿå°±æ˜¯è¯¥å±‚çš„è¾“å…¥\(x_l\)ã€ä¸‹ä¸€å±‚çš„è¾“å…¥\(x_{l+1}\)ä»¥åŠæœ¬å±‚çš„è¿ç®—\(\mathcal{F}(x, \mathcal{W}_l)\)ä¹‹é—´çš„å…³ç³»æ˜¯$$x_{l+1}=x_l+\mathcal{F}(x_l, \mathcal{W_l})$$<br>å†ä¸‹ä¸€å±‚çš„è¯ï¼š<br>$$x_{l+2}=x_l + \mathcal{F}(x_l, \mathcal{W_l}) + \mathcal{F}(x_{l+1}, \mathcal{W_{l+1}})$$<br>è¦æ˜¯ä¸€ç›´å†™åˆ°æœ€åä¸€å±‚\(x_L\)ï¼š<br>$$x_{L}=x_l+\sum_{i=l}^{L-1}\mathcal{F}(x_i, \mathcal{W_i})$$<br>é‚£ä¹ˆæ¢¯åº¦åä¼ æ—¶åˆ™æœ‰ï¼š<br>$$\begin{split}<br>\frac{\partial{\mathcal{E}}}{\partial{x_l}} &amp; = \frac{\partial{\mathcal{E}}}{\partial{x_L}}\frac{\partial{x_L}}{\partial{x_l}}\\<br>&amp; = \frac{\partial{\mathcal{E}}}{\partial{x_L}}\Big(1+\frac{\partial{}}{\partial{x_l}}\sum_{i=l}^{L-1}\mathcal{F}(x_i,\mathcal{W}_i)\Big)<br>\end{split}$$</p><p>ç„¶è€Œç°å®æ˜¯ï¼šå› ä¸ºä¸­é—´æ¶‰åŠäº†å‡ æ¬¡ä¸‹é‡‡æ ·ï¼Œé‡‡æ ·åçš„ç‰¹å¾å›¾å°ºå¯¸å‘ç”Ÿäº†å˜åŒ–ï¼Œè¿™æ—¶ï¼Œé‚£ä¸ªæ’ç­‰æ˜ å°„\(x\)ä¸Šä¸å¾—ä¸åŠ ä¸€ä¸ª\(\mathcal{M}(x)\)ï¼ˆä¸€èˆ¬ä¸ºä¸€ä¸ª\((1\times 1)\)å°ºå¯¸çš„å·ç§¯ï¼Œä½œè€…ç§°ä¹‹ä¸ºI-convï¼Œå³Isolated convolutionï¼‰æ¥æ”¹å˜å°ºå¯¸å’Œé€šé“æ•°ã€‚å› æ­¤ï¼Œä¸æ˜¯æ¯ä¸€å±‚éƒ½èƒ½ä¿è¯ç®€å•çš„\(x_{l+1}=x_l+\mathcal{F}(x_l, \mathcal{W_l})\)ï¼Œä¸Šè¾¹çš„æ¢¯åº¦å…¬å¼ä¹Ÿåªæ˜¯ä¸€ç§ç†æƒ³æƒ…å†µè€Œå·²ã€‚</p><div align="center" class="figure"><img src="/images/fishnet/bottleneck_alter.png" width="50%" alt="Bottlenecks in ResNet"><br>    Fig. 3 ResNetä¸­ï¼Œç†æƒ³çš„Bottleneckæ¨¡å—ä¸ç°å®ä¸­æŸäº›Bottleneckæ¨¡å—<br><br></div><p>åœ¨ResNetæœ¬èº«é‡Œé¢å€’è¿˜å¥½ã€‚åˆ°äº†FPNç”šè‡³Stacked Hourglassä¸­ï¼Œè¿™æ ·çš„I-convåœ¨æ¯æ¬¡ç‰¹å¾å›¾èåˆæ—¶éƒ½è¢«ä½¿ç”¨ï¼Œè¿™å°±æœ‰ç‚¹è¿èƒŒResNetä¿æŒæ¢¯åº¦æœ‰æ•ˆåä¼ çš„åˆè¡·äº†ã€‚è€ŒFishNetåœ¨è¿™ç§æƒ…å†µä¸‹é‡‡ç”¨äº†ä¸€ç§æ›´â€œå¹³æ»‘â€çš„æ–¹å¼ä½¿å¾—æ¢¯åº¦åä¼ å—åˆ°çš„å½±å“é™åˆ°æœ€ä½ã€‚</p><div align="center" class="figure"><img src="/images/fishnet/fish_block.png" width="36%" alt="Bottlenecks in FishNet"><br>    Fig. 4 FishNetä¸­æ¶‰åŠé‡‡æ ·çš„Bottleneckæ¨¡å—ï¼ˆé™¤tailéƒ¨åˆ†å¤–ï¼‰<br><br></div><h2 id="1-æ•´ä½“æ–¹æ¡ˆ"><a href="#1-æ•´ä½“æ–¹æ¡ˆ" class="headerlink" title="1 æ•´ä½“æ–¹æ¡ˆ"></a>1 æ•´ä½“æ–¹æ¡ˆ</h2><p>å¦™å•Šï¼ˆğŸ‘ï¼‰ï¼é‚£æˆ‘ä»¬å°±æ¥çœ‹ä¸€çœ¼FishNetçš„å…¨è²Œï¼š</p><div align="center" class="figure"><img src="/images/fishnet/fishnet.png" width="70%" alt="FishNet"><br>    Fig. 5 FishNet<br><br></div><!--è¿™æ‰æ˜¯çœŸæ­£çš„Fishnetï¼ï¼ˆæ–œçœ¼<div align="center" class="figure"><img src="/images/fishnet/real_fishnet.png" width="40%" alt=""></div>--><p><s>æ•´æ¡é±¼</s>æ•´ä¸ªFishNetç”±ä¸‰éƒ¨åˆ†æ„æˆï¼štailï¼ˆå°¾å·´ï¼‰ï¼Œbodyï¼ˆèº¯å¹²ï¼‰å’Œheadï¼ˆå¤´ï¼‰ã€‚tailéƒ¨åˆ†ä¹‹å‰ï¼Œå›¾åƒå…ˆè¿‡äº†ä¸‰å±‚å·ç§¯å±‚ï¼Œåˆæ­¥ä»\((224\times 225 \times 3)\)å°ºå¯¸çš„åŸå›¾åƒæå–å‡º\((56\times 56 \times 64)\)å°ºå¯¸çš„ç‰¹å¾å›¾ã€‚ä½œè€…æŠŠä¸åŒé˜¶æ®µå†…åŒä¸€åˆ†è¾¨ç‡çš„ç‰¹å¾å›¾åˆ†ä¸ºåŒä¸€ä¸ªstageï¼Œ\((56\times 56)\)çš„æ˜¯stage 1ï¼Œ\((28\times 28)\)çš„æ˜¯stage 2ï¼Œ\((14\times 14)\)çš„æ˜¯stage 3ï¼Œ\((7\times 7)\)çš„æ˜¯stage 4ã€‚å› ä¸ºåˆ†è¾¨ç‡ç›¸åŒï¼Œä¸‰ä¸ªéƒ¨åˆ†çš„ç‰¹å¾å›¾å¯ä»¥ä¸ç”¨ä¸Š/ä¸‹é‡‡æ ·è€Œç›´æ¥åœ¨channelç»´åº¦ä¸Šconcatèµ·æ¥ã€‚</p><p>tailéƒ¨åˆ†å°±æ˜¯ä¸€ä¸ªæ¼æ–—çŠ¶çš„ç½‘ç»œï¼Œæ¶‰åŠä¸‰æ¬¡æœ€å¤§æ± åŒ–ï¼Œæ¯æ¬¡æ± åŒ–å‰ï¼Œæœ€åä¸€ä¸ªå·ç§¯å±‚è¾“å‡ºçš„ç‰¹å¾å›¾è¢«ç•™ä¸‹æ¥ä¾›bodyéƒ¨åˆ†ä½¿ç”¨ã€‚è¿™ä¸€éƒ¨åˆ†çš„ç»“æœå°±æ˜¯ç»å…¸çš„æ¼æ–—çŠ¶ç½‘ç»œï¼Œä½œè€…ä½¿ç”¨çš„æ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µçš„ResNetã€‚tailéƒ¨åˆ†çš„æœ€åï¼Œä½œè€…ç”¨äº†ä¸€ä¸ªSqueeze-Excitationæ¨¡å—[2]ï¼Œå…ˆæŠŠ\((7\times 7 \times 512)\)å°ºå¯¸çš„ç‰¹å¾å›¾ç”¨Global Average Poolingå†åŠ å‡ ä¸ªå·ç§¯å±‚ï¼ˆå®é™…ä¸Šå’Œå…¨è¿æ¥å±‚å¹¶æ— æœ¬è´¨åŒºåˆ«ï¼‰æ˜ å°„æˆä¸€ä¸ª\((1\times 1\times 512)\)çš„å‘é‡ï¼Œå†æŠŠè¿™ä¸ªå‘é‡çš„æ¯ä¸€ä¸ªå€¼ä½œä¸ºä¸€ä¸ªæƒé‡ï¼Œä¹˜åˆ°ä¹‹å‰\((7\times 7\times 512)\)çš„ç‰¹å¾å›¾å¯¹åº”çš„é€šé“ä¸Šå»ã€‚</p><p>bodyéƒ¨åˆ†åƒFPNä¸€æ ·ä¸æ–­åœ°ç”¨ä¸Šé‡‡æ ·æ¥æ”¾å¤§ç‰¹å¾å›¾ï¼ŒåŒæ—¶èåˆä¹‹å‰tailéƒ¨åˆ†ä¿ç•™ä¸‹æ¥çš„åŒä¸€åˆ†è¾¨ç‡çš„ç‰¹å¾ã€‚</p><p>headéƒ¨åˆ†åˆ™æ˜¯FishNetçš„ç‹¬åˆ›æ€§å·¥ä½œï¼Œå®ƒåƒæ˜¯bodyéƒ¨åˆ†çš„åè¿‡ç¨‹ã€‚ä»¥å¾€çš„æ²™æ¼å½¢ç½‘ç»œå°†é«˜å±‚è¯­ä¹‰ç‰¹å¾ç”¨æ¥ç²¾åŒ–ä½å±‚ç»†èŠ‚ç‰¹å¾ï¼Œè€Œheadç½‘ç»œåå…¶é“è€Œè¡Œä¹‹ï¼Œåˆç”¨ç²¾åŒ–è¿‡çš„ä½å±‚ç»†èŠ‚ç‰¹å¾åè¿‡æ¥ç²¾åŒ–é«˜å±‚ç‰¹å¾ã€‚è¿™æ ·ï¼Œå†æ¬¡é‡‡æ ·å¾—åˆ°çš„é«˜å±‚ç‰¹å¾çš„è´¨é‡è¢«æœ‰æ•ˆæé«˜ã€‚</p><h2 id="2-å®ç°ç»†èŠ‚"><a href="#2-å®ç°ç»†èŠ‚" class="headerlink" title="2 å®ç°ç»†èŠ‚"></a>2 å®ç°ç»†èŠ‚</h2><h3 id="2-1-ç½‘ç»œå‚æ•°"><a href="#2-1-ç½‘ç»œå‚æ•°" class="headerlink" title="2.1 ç½‘ç»œå‚æ•°"></a>2.1 ç½‘ç»œå‚æ•°</h3><p>FishNet-99æ•´ä½“çš„å„ä¸ªéƒ¨åˆ†çš„å‚æ•°è§ä¸‹è¡¨ã€‚</p><table><thead><tr><th style="text-align:center">Part-Stage</th><th style="text-align:center">Input shape</th><th style="text-align:center">Output shape</th><th style="text-align:center">Bottlenecks</th><th style="text-align:center">I-convs</th><th style="text-align:center">Convs in total</th></tr></thead><tbody><tr><td style="text-align:center">Input</td><td style="text-align:center">\(3\times 224 \times 224\)</td><td style="text-align:center">\(64\times 56 \times 56\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(3\)</td></tr><tr><td style="text-align:center">Tail-1</td><td style="text-align:center">\(64\times 56 \times 56\)</td><td style="text-align:center">\(128\times 28 \times 28\)</td><td style="text-align:center">\(2\)</td><td style="text-align:center">\(1\)</td><td style="text-align:center">\(7\)</td></tr><tr><td style="text-align:center">Tail-2</td><td style="text-align:center">\(128\times 28 \times 28\)</td><td style="text-align:center">\(256\times 14 \times 14\)</td><td style="text-align:center">\(2\)</td><td style="text-align:center">\(1\)</td><td style="text-align:center">\(7\)</td></tr><tr><td style="text-align:center">Tail-3</td><td style="text-align:center">\(256\times 14 \times 14\)</td><td style="text-align:center">\(512\times 7 \times 7\)</td><td style="text-align:center">\(6\)</td><td style="text-align:center">\(1\)</td><td style="text-align:center">\(19\)</td></tr><tr><td style="text-align:center">SE-block</td><td style="text-align:center">\(512\times 7 \times 7\)</td><td style="text-align:center">\(512\times 7 \times 7\)</td><td style="text-align:center">\(2\)</td><td style="text-align:center">\(1\)</td><td style="text-align:center">\(11\)</td></tr><tr><td style="text-align:center">Body-3</td><td style="text-align:center">\(512\times 7 \times 7\)</td><td style="text-align:center">\(256\times 14 \times 14\)</td><td style="text-align:center">\(1 + 1\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(6\)</td></tr><tr><td style="text-align:center">Body-2</td><td style="text-align:center">\((512+256)\times 14 \times 14\)</td><td style="text-align:center">\(384\times 28 \times 28\)</td><td style="text-align:center">\(1 + 1\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(6\)</td></tr><tr><td style="text-align:center">Body-1</td><td style="text-align:center">\((384+128)\times 28 \times 28\)</td><td style="text-align:center">\(256\times 56 \times 56\)</td><td style="text-align:center">\(1 + 1\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(6\)</td></tr><tr><td style="text-align:center">Head-1</td><td style="text-align:center">\((256+64)\times 56 \times 56\)</td><td style="text-align:center">\(320\times 28 \times 28\)</td><td style="text-align:center">\(1 + 1\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(6\)</td></tr><tr><td style="text-align:center">Head-2</td><td style="text-align:center">\((320+512)\times 28 \times 28\)</td><td style="text-align:center">\(832\times 14 \times 14\)</td><td style="text-align:center">\(2 + 1\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(9\)</td></tr><tr><td style="text-align:center">Head-3</td><td style="text-align:center">\((832+768)\times 14 \times 14\)</td><td style="text-align:center">\(1600\times 7 \times 7\)</td><td style="text-align:center">\(2 + 4\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(18\)</td></tr><tr><td style="text-align:center">Score-Conv</td><td style="text-align:center">\((1600+512)\times 7 \times 7\)</td><td style="text-align:center">\(1056\times 7 \times 7\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(1\)</td></tr><tr><td style="text-align:center">Score-FC</td><td style="text-align:center">\(1056\times 7 \times 7\)</td><td style="text-align:center">\(1000\times 1 \times 1\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(1\)</td></tr></tbody></table><p>è¯´æ˜ï¼š</p><ul><li>ç¬¬ä¸€åˆ—çš„Tail-1ä»£è¡¨Tailéƒ¨åˆ†çš„stage \(1\)ã€‚</li><li>Body-3è‡³Head-3çš„Bottleneckæ¨¡å—æ•°é‡åŒ…æ‹¬ä¸¤ç§ï¼šç½‘ç»œä¸»å¹²ä¸Šçš„å’Œç‰¹å¾å›¾è¿ç§»æ¨¡å—ä¸Šçš„ã€‚è¿ç§»æ¨¡å—ç”¨äºå°†ä¸Šä¸€éƒ¨åˆ†åŒä¸€stageçš„ç‰¹å¾å›¾è¿›è¡Œå˜æ¢ã€‚</li></ul><p>FishNet-150çš„å‚æ•°è§ä¸‹è¡¨ï¼Œä¸FishNet-99ç›¸æ¯”è€Œè¨€åªæ˜¯å„ä¸ªéƒ¨åˆ†Bottleneckå—çš„æ•°é‡ä¸åŒï¼Œæ²¡æœ‰å¤ªå¤§å·®å¼‚ã€‚</p><table><thead><tr><th style="text-align:center">Part-Stage</th><th style="text-align:center">Input shape</th><th style="text-align:center">Output shape</th><th style="text-align:center">Bottlenecks</th><th style="text-align:center">I-convs</th><th style="text-align:center">Convs in total</th></tr></thead><tbody><tr><td style="text-align:center">Input</td><td style="text-align:center">\(3\times 224 \times 224\)</td><td style="text-align:center">\(64\times 56 \times 56\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(3\)</td></tr><tr><td style="text-align:center">Tail-1</td><td style="text-align:center">\(64\times 56 \times 56\)</td><td style="text-align:center">\(128\times 28 \times 28\)</td><td style="text-align:center">\(2\)</td><td style="text-align:center">\(1\)</td><td style="text-align:center">\(7\)</td></tr><tr><td style="text-align:center">Tail-2</td><td style="text-align:center">\(128\times 28 \times 28\)</td><td style="text-align:center">\(256\times 14 \times 14\)</td><td style="text-align:center">\(4\)</td><td style="text-align:center">\(1\)</td><td style="text-align:center">\(13\)</td></tr><tr><td style="text-align:center">Tail-3</td><td style="text-align:center">\(256\times 14 \times 14\)</td><td style="text-align:center">\(512\times 7 \times 7\)</td><td style="text-align:center">\(8\)</td><td style="text-align:center">\(1\)</td><td style="text-align:center">\(25\)</td></tr><tr><td style="text-align:center">SE-block</td><td style="text-align:center">\(512\times 7 \times 7\)</td><td style="text-align:center">\(512\times 7 \times 7\)</td><td style="text-align:center">\(4\)</td><td style="text-align:center">\(1\)</td><td style="text-align:center">\(17\)</td></tr><tr><td style="text-align:center">Body-3</td><td style="text-align:center">\(512\times 7 \times 7\)</td><td style="text-align:center">\(256\times 14 \times 14\)</td><td style="text-align:center">\(2 + 2\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(12\)</td></tr><tr><td style="text-align:center">Body-2</td><td style="text-align:center">\((512+256)\times 14 \times 14\)</td><td style="text-align:center">\(384\times 28 \times 28\)</td><td style="text-align:center">\(2 + 2\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(12\)</td></tr><tr><td style="text-align:center">Body-1</td><td style="text-align:center">\((384+128)\times 28 \times 28\)</td><td style="text-align:center">\(256\times 56 \times 56\)</td><td style="text-align:center">\(2 + 2\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(12\)</td></tr><tr><td style="text-align:center">Head-1</td><td style="text-align:center">\((256+64)\times 56 \times 56\)</td><td style="text-align:center">\(320\times 28 \times 28\)</td><td style="text-align:center">\(2 + 2\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(12\)</td></tr><tr><td style="text-align:center">Head-2</td><td style="text-align:center">\((320+512)\times 28 \times 28\)</td><td style="text-align:center">\(832\times 14 \times 14\)</td><td style="text-align:center">\(2 + 2\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(12\)</td></tr><tr><td style="text-align:center">Head-3</td><td style="text-align:center">\((832+768)\times 14 \times 14\)</td><td style="text-align:center">\(1600\times 7 \times 7\)</td><td style="text-align:center">\(4 + 4\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(24\)</td></tr><tr><td style="text-align:center">Score-Conv</td><td style="text-align:center">\((1600+512)\times 7 \times 7\)</td><td style="text-align:center">\(1056\times 7 \times 7\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(1\)</td></tr><tr><td style="text-align:center">Score-FC</td><td style="text-align:center">\(1056\times 7 \times 7\)</td><td style="text-align:center">\(1000\times 1 \times 1\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(0\)</td><td style="text-align:center">\(1\)</td></tr></tbody></table><p>tailï¼Œbodyå’Œheadä¸‰éƒ¨åˆ†çš„ä¸»è¦æˆåˆ†éƒ½æ˜¯Bottleneckæ¨¡å—ï¼Œå³ä¸‹è¡¨æ‰€ç¤ºçš„ç»“æ„ï¼š</p><table><thead><tr><th style="text-align:center">Layer</th><th style="text-align:center">Type</th><th style="text-align:center">Output channels</th><th style="text-align:center">Kernel Size</th></tr></thead><tbody><tr><td style="text-align:center">(shortcut)</td><td style="text-align:center">(take shortcut)</td><td style="text-align:center">-</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">relu</td><td style="text-align:center">ReLU</td><td style="text-align:center">\(C\)</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">bn1</td><td style="text-align:center">Batch Normalization</td><td style="text-align:center">\(C\)</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">conv1</td><td style="text-align:center">Convolution</td><td style="text-align:center">\(C / 4\)</td><td style="text-align:center">\(1\times 1\)</td></tr><tr><td style="text-align:center">bn2</td><td style="text-align:center">Batch Normalization</td><td style="text-align:center">\(C / 4\)</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">conv2</td><td style="text-align:center">Convolution</td><td style="text-align:center">\(C / 4\)</td><td style="text-align:center">\(3\times 3\)</td></tr><tr><td style="text-align:center">bn3</td><td style="text-align:center">Batch Normalization</td><td style="text-align:center">\(C / 4\)</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">conv3</td><td style="text-align:center">Convolution</td><td style="text-align:center">\(C&#39;\)</td><td style="text-align:center">\(1\times 1\)</td></tr><tr><td style="text-align:center">(addition)</td><td style="text-align:center">(add shortcut)</td><td style="text-align:center">\(C&#39;\)</td><td style="text-align:center">-</td></tr></tbody></table><p>åœ¨tailéƒ¨åˆ†çš„æ¯ä¸€ä¸ªstageä¸­ï¼Œç¬¬ä¸€ä¸ªBottleneckæ¨¡å—ä¼šæ¶‰åŠé€šé“æ•°çš„å˜åŒ–ï¼ˆå³\(C&#39;\neq C\)ï¼‰ã€‚è¿™æ—¶shortcutéœ€è¦ç»è¿‡ä¸€ä¸ªå·ç§¯å±‚æ¥å˜æ¢identity mappingçš„é€šé“æ•°ã€‚å› æ­¤ï¼Œè¿™ä¸‰ä¸ªshortcutä¸Šä¾æ—§æ— æ³•é¿å…ä½¿ç”¨Isolated convolutionã€‚åœ¨SE-blockä¸­ä¹Ÿå­˜åœ¨ç±»ä¼¼çš„æƒ…å†µã€‚è€Œåœ¨headéƒ¨åˆ†ä¸­ï¼Œå°½ç®¡ç‰¹å¾å›¾ä»åœ¨ä¸æ–­åœ°ä¸‹é‡‡æ ·ï¼Œå…¶é€šé“æ•°å¹¶æ²¡æœ‰è¢«æ”¹å˜ï¼Œæ‰€ä»¥ä¸éœ€è¦ä½¿ç”¨è¿™æ ·çš„Isolated convolutionæ¥å¹²æ‰°æ¢¯åº¦çš„ç›´æ¥åä¼ ï¼ˆdirect back-propagationï¼‰ã€‚</p><p>ï¼ˆPSï¼šå¯æ˜¯æˆ‘æ•°äº†æ•°ï¼ŒFishNet-99é‡Œæœ‰100ä¸ªå·ç§¯ï¼ŒFishNet-150é‡Œæœ‰151ä¸ªå·ç§¯å‘€ğŸ˜‚ï¼Ÿä¸ªäººçŒœæµ‹æ˜¯å› ä¸ºScore-FCå±‚ä¸åº”è¯¥ç®—åœ¨FishNetä¸»å¹²å†…ï¼Ÿå¯¹äº†ï¼Œè™½ç„¶å®ƒå«åšFCå±‚ï¼Œä½†ä½œè€…ä»£ç é‡Œè¿˜æ˜¯ç”¨å·ç§¯å±‚çš„å½¢å¼å®šä¹‰çš„å“¦ã€‚å› ä¸º\(7\times 7\)å°ºå¯¸çš„ç‰¹å¾å›¾è¿‡äº†ä¸€å±‚Global Average Poolingå˜æˆäº†\(1\times 1\)å°ºå¯¸ï¼Œæ‰€ä»¥å®ƒæœ¬è´¨ä¸Šå˜æˆäº†ä¸€ä¸ªé•¿åº¦ä¸ºé€šé“æ•°çš„å‘é‡ã€‚ï¼‰</p><h3 id="2-2-é‡‡æ ·-amp-ç²¾åŒ–æ¨¡å—"><a href="#2-2-é‡‡æ ·-amp-ç²¾åŒ–æ¨¡å—" class="headerlink" title="2.2 é‡‡æ ·&amp;ç²¾åŒ–æ¨¡å—"></a>2.2 é‡‡æ ·&amp;ç²¾åŒ–æ¨¡å—</h3><p>ä»bodyéƒ¨åˆ†çš„stage 3å¼€å§‹ç›´åˆ°headéƒ¨åˆ†çš„stage3ï¼Œæ¯ä¸ªstageçš„ç‰¹å¾å›¾å°†ä¸ä¹‹å‰éƒ¨åˆ†çš„ç‰¹å¾å›¾èåˆï¼ˆä¹Ÿå°±æ˜¯å›¾ä¸­çš„çº¢è‰²è™šçº¿å’Œçº¢æ¡†æ‰€è¡¨ç¤ºçš„å†…å®¹ï¼‰ã€‚ä¸ºäº†ä¿è¯æ¢¯åº¦ç›´æ¥åä¼ ï¼Œä½œè€…è®¾è®¡äº†UR-block (Upsampling &amp; Refinement) å’ŒDR-block (Downsampling &amp; Refinement) æ¥â€œä¿æŒå’Œç²¾åŒ–â€ï¼ˆpreserve and refineï¼‰å„ä¸ªéƒ¨åˆ†çš„ç‰¹å¾ã€‚</p><h4 id="2-2-1-ä¸Šé‡‡æ ·-amp-ç²¾åŒ–ï¼ˆURï¼‰æ¨¡å—"><a href="#2-2-1-ä¸Šé‡‡æ ·-amp-ç²¾åŒ–ï¼ˆURï¼‰æ¨¡å—" class="headerlink" title="2.2.1 ä¸Šé‡‡æ ·&amp;ç²¾åŒ–ï¼ˆURï¼‰æ¨¡å—"></a>2.2.1 ä¸Šé‡‡æ ·&amp;ç²¾åŒ–ï¼ˆURï¼‰æ¨¡å—</h4><p>ä¸Šè¾¹æåˆ°ï¼ŒFishNetä¸­çš„stageå·ä¸æ˜¯ä»æµ…åˆ°æ·±ä¾æ¬¡å¢å¤§çš„ï¼Œè€Œæ˜¯ä¸ç‰¹å¾å›¾çš„å°ºåº¦ç›¸å¯¹åº”ã€‚è®¾tailéƒ¨åˆ†å’Œbodyéƒ¨åˆ†çš„stage \(s\)çš„<strong>ç¬¬ä¸€å±‚</strong>è¾“å‡ºç‰¹å¾åˆ†åˆ«ä¸º\(x^t_s\)å’Œ\(x^b_s\)ï¼Œåˆ™\(x^t_s\)å’Œ\(x^b_s\)çš„å®½åº¦å’Œé«˜åº¦åº”è¯¥æ˜¯ä¸€è‡´çš„ï¼ˆå°½ç®¡é€šé“æ•°å¯èƒ½ä¸åŒï¼‰ã€‚\(x^t_s\)ç»è¿‡ä¸€ä¸ªè¿ç§»æ¨¡å—\(\mathcal{T}(x)\)ï¼ˆtransferring blockï¼ŒåŒæ ·æ˜¯å¸¦shortcutçš„Bottleneckæ¨¡å—ï¼‰åä¸\(x^b_s\)è¿›è¡Œè¿æ¥æ„æˆèåˆçš„ç‰¹å¾å›¾\(\widetilde{x}^b_s\):<br>$$\widetilde{x}^b_s = concat(x^b_s, \mathcal{T}(x^t_s))$$</p><p>\(\widetilde{x}^b_s\)å°†ç»§ç»­ä½œä¸ºbodyéƒ¨åˆ†çš„stage \(s\)ä¸­åé¢çš„å·ç§¯å±‚\(\mathcal{M}(x)\)çš„è¾“å…¥ã€‚åŒæ—¶ï¼Œä¸ºäº†æ¢¯åº¦çš„ç›´æ¥åä¼ ï¼Œå¦æœ‰ä¸€æ¡æ’ç­‰æ˜ å°„ä¸\(\mathcal{M}(\widetilde{x}^b_s)\)ç›¸åŠ ã€‚è¿™é‡Œçš„æ€è·¯ä¸ResNetä¸­\(\mathcal{H}(x)=x+\mathcal{F}(x)\)æ˜¯ä¸€è‡´çš„ï¼š<br>$$\widetilde{x}&#39;^b_s = r(\widetilde{x}^b_s) + \mathcal{M}(\widetilde{x}^b_s)$$</p><p>åœ¨bodyéƒ¨åˆ†çš„stage 1ä¸­ï¼Œ\(\mathcal{M}(x)\)çš„è¾“å‡ºå€¼é€šé“æ•°ä¸\(x\)ç›¸åŒï¼Œæ­¤æ—¶\(r(x)\)å³ä¸º\(x\)ã€‚è€Œstage 2å’Œstage 1ä¸­ï¼Œç”±äº\(\mathcal{M}(x)\)ä¸­é€šé“æ•°ä¼šäº§ç”Ÿå˜åŒ–ï¼ˆåœ¨ä½œè€…ä»£ç ä¸­ï¼Œé€šé“æ•°å‡åŠï¼Œ\(k=2\)ï¼‰ï¼Œæ‰€ä»¥è¿™é‡Œçš„\(r(x)\)éœ€è¦èµ·åˆ°ç¼©å°é€šé“æ•°ï¼ˆchannel-wise reductionï¼‰çš„ä½œç”¨ã€‚<strong>è¿˜æ˜¯ä¸ºäº†æ¢¯åº¦ç›´æ¥åä¼ </strong>ï¼Œè¿™é‡Œç”šè‡³æ²¡æœ‰ä½¿ç”¨\((1\times 1)\)çš„å·ç§¯æ¥å˜æ¢é€šé“æ•°ï¼Œè€Œæ˜¯ç›´æ¥æŠŠæ¯\(k\)ä¸ªé€šé“æ±‚å’Œï¼ˆelement-wise summationï¼‰è€Œå‹ç¼©æˆä¸€ä¸ªé€šé“ã€‚\(\widetilde{x}â€™^b_s\)å†è¿›è¡Œä¸€ä¸‹ä¸Šé‡‡æ ·å°±æˆä¸ºbodyéƒ¨åˆ†ä¸‹ä¸€ä¸ªstageï¼ˆå³stage \(s-1\)ï¼‰çš„è¾“å…¥äº†ï¼š<br>$$x^b_{s-1}=up(\widetilde{x}&#39;^b_s)$$</p><div align="center" class="figure"><img src="/images/fishnet/ur.png" width="20%" alt="Upsampling & Refinement blocks"><br>    Fig. 6 ä¸Šé‡‡æ ·&amp;ç²¾åŒ–æ¨¡å—<br><br></div><p>ï¼ˆPSï¼šä¸ºä»€ä¹ˆè¿™é‡Œä¸ç”¨\((1\times 1)\)å·ç§¯ï¼Œè€Œå‰é¢tailéƒ¨åˆ†è¦ç”¨å‘¢ï¼Ÿä¸ªäººçŒœæµ‹æ˜¯å› ä¸ºtailéƒ¨åˆ†è¦æ‰©å¤§é€šé“æ•°è€Œä¸å¾—ä¸ç”¨è¿™æ ·çš„æ–¹å¼ã€‚æˆ–è®¸åœ¨tailéƒ¨åˆ†ä½¿ç”¨ä¸è¿™é‡Œçš„\(r(x)\)ç›¸åçš„è¿‡ç¨‹â€”â€”é€šè¿‡æŠŠæ¯ä¸ªé€šé“duplicateä¸€ä¸‹æ¥è¾¾æˆé€šé“æ•°å¢åŠ ä¸€å€çš„æ•ˆæœä¹Ÿèƒ½workå‘¢ï¼Ÿæœ‰å…´è¶£çš„å¯ä»¥è¯•ä¸€ä¸‹ã€‚ï¼‰</p><h4 id="2-2-2-ä¸‹é‡‡æ ·-amp-ç²¾åŒ–ï¼ˆDRï¼‰æ¨¡å—"><a href="#2-2-2-ä¸‹é‡‡æ ·-amp-ç²¾åŒ–ï¼ˆDRï¼‰æ¨¡å—" class="headerlink" title="2.2.2 ä¸‹é‡‡æ ·&amp;ç²¾åŒ–ï¼ˆDRï¼‰æ¨¡å—"></a>2.2.2 ä¸‹é‡‡æ ·&amp;ç²¾åŒ–ï¼ˆDRï¼‰æ¨¡å—</h4><p>headéƒ¨åˆ†çš„ä¸‹é‡‡æ ·&amp;ç²¾åŒ–æ¨¡å—æ¯”ä¸Šé‡‡æ ·&amp;ç²¾åŒ–æ¨¡å—æ›´åŠ ç®€å•ï¼Œå› ä¸ºè¿™é‡Œæ‰€æœ‰çš„\(\mathcal{M}(x)\)éƒ½ä¸ä¼šå¯¼è‡´é€šé“æ•°çš„å˜åŒ–ï¼ŒURæ¨¡å—ç”¨äºçš„\(r(x)\)ä¹Ÿå°±ä¸éœ€è¦äº†ã€‚å…¶ä»–çš„å…¬å¼ä¸URæ¨¡å—åŸºæœ¬ç›¸åŒï¼š<br>$$\widetilde{x}^b_s = concat(x^b_s, \mathcal{T}(x^t_s)) \\<br>\widetilde{x}&#39;^b_s = \widetilde{x}^b_s + \mathcal{M}(\widetilde{x}^b_s) \\<br>x^b_{s+1}=down(\widetilde{x}&#39;^b_s)$$</p><div align="center" class="figure"><img src="/images/fishnet/dr.png" width="20%" alt="Downsampling & Refinement blocks"><br>    Fig. 7 ä¸‹é‡‡æ ·&amp;ç²¾åŒ–æ¨¡å—<br><br></div><h2 id="3-ç»éªŒæ€»ç»“"><a href="#3-ç»éªŒæ€»ç»“" class="headerlink" title="3 ç»éªŒæ€»ç»“"></a>3 ç»éªŒæ€»ç»“</h2><h3 id="3-1-ä½å±‚ç‰¹å¾å¯¹é«˜å±‚ç‰¹å¾çš„åŠ å¼º"><a href="#3-1-ä½å±‚ç‰¹å¾å¯¹é«˜å±‚ç‰¹å¾çš„åŠ å¼º" class="headerlink" title="3.1 ä½å±‚ç‰¹å¾å¯¹é«˜å±‚ç‰¹å¾çš„åŠ å¼º"></a>3.1 ä½å±‚ç‰¹å¾å¯¹é«˜å±‚ç‰¹å¾çš„åŠ å¼º</h3><p>æ¼æ–—çŠ¶å·ç§¯ç½‘ç»œé‡Œï¼Œè¾ƒæµ…å·ç§¯å±‚ä¸­çš„ç‰¹å¾å¾€å¾€æ˜¯è¾ƒç®€å•ã€åƒç´ çº§çš„ç‰¹å¾ï¼Œè€Œæ›´æ·±çš„å·ç§¯å±‚ä¸­çš„ç‰¹å¾ç”±äºæ„Ÿå—åŸŸè¾ƒå¤§ï¼Œæ˜¯æ›´æŠ½è±¡ã€æ³›åŒ–çš„ç‰¹å¾ã€‚ç”±äºFishNetä¸­ä¸Šé‡‡æ ·ã€ä¸‹é‡‡æ ·çš„å­˜åœ¨ï¼Œç›´æ¥ä»¥â€œæµ…å±‚â€â€œæ·±å±‚â€ç‰¹å¾æ¥åŒºåˆ†ä¸åŒåˆ†è¾¨ç‡çš„ç‰¹å¾ä¼¼ä¹å¹¶ä¸å¦¥å½“ã€‚å› æ­¤ï¼Œè¿™é‡Œæˆ‘ç”¨â€œä½å±‚ç‰¹å¾â€æ¥æŒ‡ä»£åˆ†è¾¨ç‡è¾ƒå¤§ã€è¾ƒå…·ä½“çš„ç‰¹å¾ï¼Œç”¨â€œé«˜å±‚ç‰¹å¾â€æŒ‡ä»£åˆ†è¾¨ç‡è¾ƒå°ã€æŠ½è±¡ç¨‹åº¦è¾ƒé«˜ï¼Œæˆ–è€…è¯´â€œæµ“ç¼©ç¨‹åº¦â€è¾ƒé«˜çš„ç‰¹å¾ã€‚</p><p>åˆ†ç±»ä»»åŠ¡é‡Œï¼Œå›¾åƒé€šè¿‡ä¸€ä¸ªæ¼æ–—çŠ¶çš„å·ç§¯ç½‘ç»œå³å¯å›å½’å‡ºå®ƒçš„ç±»åˆ«ï¼›æ£€æµ‹ä»»åŠ¡é‡Œï¼Œé€šè¿‡ç”¨é«˜å±‚ç‰¹å¾åŠ å¼ºä½å±‚ç‰¹å¾çš„æ–¹å¼å¯ä»¥æœ‰æ•ˆæå‡æ£€æµ‹æ•ˆæœï¼›å¦‚æœåè¿‡æ¥å†ç”¨ä½å±‚ç‰¹å¾å¢å¼ºé«˜å±‚ç‰¹å¾ï¼Œç½‘ç»œåˆ™å¯åŒæ—¶è¢«ç”¨äºå›¾åƒçº§ã€åŒºåŸŸçº§å’Œåƒç´ çº§çš„ä¸åŒä»»åŠ¡ã€‚</p><h3 id="3-2-é¿å…Isolated-Convolution"><a href="#3-2-é¿å…Isolated-Convolution" class="headerlink" title="3.2 é¿å…Isolated Convolution"></a>3.2 é¿å…Isolated Convolution</h3><p>å°½é‡é¿å…åœ¨shortcutä¸Šä½¿ç”¨I-convã€‚FishNeté™¤äº†tailéƒ¨åˆ†åœ¨æ¶‰åŠé€šé“æ•°å˜åŒ–çš„æ®‹å·®æ¨¡å—ä¸Šä½¿ç”¨äº†I-convå¤–ï¼Œåœ¨bodyå’Œheadéƒ¨åˆ†çš„èåˆæ—¶éƒ½é¿å…ä½¿ç”¨I-convï¼Œä»è€Œæœ€å¤§é™åº¦åœ°ä¿è¯äº†æ¢¯åº¦çš„ç›´æ¥åä¼ ã€‚</p><h3 id="3-3-ä¸Šé‡‡æ ·çš„æ–¹å¼"><a href="#3-3-ä¸Šé‡‡æ ·çš„æ–¹å¼" class="headerlink" title="3.3 ä¸Šé‡‡æ ·çš„æ–¹å¼"></a>3.3 ä¸Šé‡‡æ ·çš„æ–¹å¼</h3><p>ä¸Šé‡‡æ ·æ–¹å¼çš„é€‰æ‹©ä¸Šï¼Œå°½å¯èƒ½<strong>ä¸ä½¿ç”¨å¸¦æƒå€¼çš„åå·ç§¯</strong>ï¼Œè€Œæ˜¯ç”¨æœ€è¿‘é‚»æ’å€¼ç­‰æ–¹å¼ã€‚æ­¤ä¸¾åŒæ ·æ˜¯ä¸ºäº†ä¿è¯æ¢¯åº¦çš„ç›´æ¥åä¼ ã€‚</p><h3 id="3-4-ä¸‹é‡‡æ ·çš„æ–¹å¼"><a href="#3-4-ä¸‹é‡‡æ ·çš„æ–¹å¼" class="headerlink" title="3.4 ä¸‹é‡‡æ ·çš„æ–¹å¼"></a>3.4 ä¸‹é‡‡æ ·çš„æ–¹å¼</h3><p><strong>ç”¨kernelå°ºå¯¸ä¸º\((2\times 2)\)ã€strideä¹Ÿä¸º\(2\)çš„MaxPoolingè¿›è¡Œä¸‹é‡‡æ ·</strong>ä¸å…¶ä»–å‡ ç§å…¸å‹çš„ä¸‹é‡‡æ ·æ–¹å¼ç›¸æ¯”ï¼Œæ•ˆæœæ›´å¥½ã€‚ç”¨æ¥å¯¹(diao)æ¯”(da)çš„å¦å¤–å‡ ç§ä¸‹é‡‡æ ·æ–¹å¼åŒ…æ‹¬ï¼š</p><ul><li>æœ€åä¸€å±‚å·ç§¯stride=\(2\)ï¼ˆå¹²æ‰°äº†æ¢¯åº¦ç›´æ¥åä¼ ï¼‰</li><li>kernel size=\((3\times 3)\)ã€stride=\(2\)çš„MaxPoolingï¼ˆæ»‘åŠ¨çª—å£æœ‰äº¤å ï¼Œæ‰°ä¹±äº†ç»“æ„ä¿¡æ¯ï¼‰</li><li>kernel size=\((3\times 3)\)ã€stride=\(2\)çš„AveragePoolingï¼ˆåŸæ–‡æ²¡è®²ï¼Œä¸ªäººè®¤ä¸ºä¸æœ€åä¸€å±‚å·ç§¯åŠ stride=\(2\)æ•ˆæœç±»ä¼¼ï¼‰</li></ul><h2 id="4-ä¸ªäººæ„Ÿæ‚Ÿ"><a href="#4-ä¸ªäººæ„Ÿæ‚Ÿ" class="headerlink" title="4 ä¸ªäººæ„Ÿæ‚Ÿ"></a>4 ä¸ªäººæ„Ÿæ‚Ÿ</h2><p>â€œè€åƒ§ä¸‰åå¹´å‰ï¼Œæœªå‚ç¦…æ—¶ï¼Œè§å±±æ˜¯å±±ï¼Œè§æ°´æ˜¯æ°´ã€‚</p><p>åŠè‡³åæ¥äº²è§çŸ¥è¯†ï¼Œæœ‰ä¸ªå…¥å¤„ï¼Œè§å±±ä¸æ˜¯å±±ï¼Œè§æ°´ä¸æ˜¯æ°´ã€‚</p><p>è€Œä»Šå¾—ä¸ªä¼‘æ­‡å¤„ï¼Œä¾å‰è§å±±åªæ˜¯å±±ï¼Œè§æ°´åªæ˜¯æ°´ã€‚</p><p>å¤§ä¼—ï¼Œè¿™ä¸‰èˆ¬è§è§£ï¼Œæ˜¯åŒæ˜¯åˆ«ï¼Œæœ‰äººç¼ç´ å¾—å‡ºã€‚â€ </p><p>â€”â€”å‰å·é’åŸæƒŸä¿¡ç¦…å¸ˆ[3]</p><p>FishNetçš„æ€æƒ³ï¼Œä¼¼ä¹ä¸è¿™ä¸‰é‡å¢ƒç•Œæœ‰ä»€ä¹ˆå…³è”ï¼Ÿæ± åŒ–ï¼Œæ’å€¼ï¼Œèåˆï¼Œå†æ± åŒ–ï¼Œå†èåˆï¼Œè¿™ä¸ªè¿‡ç¨‹ï¼Œä»¿ä½›ä¸€ä¸ªäººè„‘æµ·ä¸­å¯¹çŸ¥è¯†çš„å»ºæ„ã€è§£æ„å’Œé‡æ„çš„è¿‡ç¨‹ã€‚</p><p>æˆ‘åœ¨åˆè¯†æŸäº›æ–°äº‹ç‰©æ—¶ï¼Œç”±äºå¯¹å®ƒè¿˜æ²¡æœ‰å½¢æˆå……åˆ†çš„äº†è§£ï¼Œåªæ˜¯å¤§è‡´åœ°å½¢æˆäº†ä¸€ä¸ªå°è±¡ã€‚æ¯”å¦‚åå¤šå¹´å‰ï¼Œâ€œå±å¹•ï¼Œä¸»æœºï¼Œé¼ æ ‡ï¼Œé”®ç›˜â€ï¼Œè¿™å°±æ˜¯æˆ‘è„‘æµ·ä¸­ä¸€å°è®¡ç®—æœºçš„æ ·å­ï¼Œæ‰€è°“â€œè®¡ç®—æœºç§‘å­¦â€ï¼Œåœ¨å½“æ—¶çš„è‡ªå·±çœ‹æ¥ä¹Ÿä¸è¿‡æ˜¯ç”¨ä¸€äº›è½¯ä»¶å†™å†™æ–‡æ¡£ç”»ç”»å›¾ä¹‹ç±»çš„å·¥ä½œã€‚</p><p>éšç€å­¦ä¹ çš„é€æ¸æ·±å…¥ï¼Œæˆ‘ä»ä¸€ä¸ªä½¿ç”¨è€…æˆä¸ºäº†ä¸€ä¸ªå¼€å‘è€…åï¼Œå…³æ³¨ç‚¹ä¹Ÿä¸æ–­åœ°æ·±å…¥ã€ç»†åŒ–ï¼šå½“çœ‹åˆ°ä¸€ä¸ªç½‘é¡µçš„åŠ¨æ•ˆï¼Œæˆ‘æƒ³åˆ°æŒ‰F12çœ‹çœ‹å®ƒæ˜¯æ€ä¹ˆç”¨jså®ç°çš„ï¼Œæƒ³åˆ°è¿™ä¸ªå¼‚æ­¥è¯·æ±‚æ˜¯æ€ä¹ˆå“åº”çš„ï¼Œæƒ³åˆ°ç½‘ç»œè¯·æ±‚çš„TCPæŠ¥æ–‡æ˜¯æ€æ ·çš„ï¼Œæƒ³åˆ°æŠ¥æ–‡æ˜¯å¦‚ä½•ç»å†ä¸€ç³»åˆ—è·¯ç”±å™¨ä¼ è¾“åˆ°æœåŠ¡å™¨çš„ã€‚åœ¨å¯¹è®¡ç®—æœºçš„äº†è§£ä¸æ–­æ·±å…¥çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘å´åˆå¯¹å®ƒäº§ç”Ÿäº†ä¸€ç§é™Œç”Ÿæ„Ÿâ€”â€”è¿™é—¨ç§‘å­¦è¿˜è—æœ‰å¤šå°‘çš„å¥¥ç§˜ï¼Œå…¶ä¸­æ˜¯å¦æœ‰äº›æˆ‘ç”šè‡³è¿˜æ— æ³•æƒ³è±¡ï¼Ÿ</p><p>è‡³äºå†å°†å­¦ä¹ æ·±å…¥ä¸‹å»æˆ‘ä¼šå¯¹è®¡ç®—æœºäº§ç”Ÿæ€æ ·çš„è®¤è¯†ï¼Œæ‰ç–å­¦æµ…ï¼Œå°šä¸å¾—è€ŒçŸ¥ã€‚ä¹Ÿè®¸æŸä¸€å¤©æˆ‘ä¼šæç„¶å¤§æ‚Ÿâ€”â€”å“¦ï¼ŒåŸæ¥è®¡ç®—æœºç§‘å­¦å°±æ˜¯è¿™ä¸ªæ ·å­çš„å‘€ã€‚</p><h2 id="5-å‚è€ƒæ–‡çŒ®"><a href="#5-å‚è€ƒæ–‡çŒ®" class="headerlink" title="5 å‚è€ƒæ–‡çŒ®"></a>5 å‚è€ƒæ–‡çŒ®</h2><p>[1] <a href="http://papers.nips.cc/paper/7356-fishnet-a-versatile-backbone-for-image-region-and-pixel-level-prediction.pdf" target="_blank" rel="noopener">Sun S, Pang J, Shi J, et al. Fishnet: A versatile backbone for image, region, and pixel level prediction[C]//Advances in Neural Information Processing Systems. 2018: 754-764.</a></p><p>[2] <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.pdf" target="_blank" rel="noopener">Hu J, Shen L, Sun G. Squeeze-and-excitation networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 7132-7141.</a></p><p>[3] <a href="http://www.shixiu.net/wenhua/tuijian/zyl/4802.html" target="_blank" rel="noopener">ç¿æ±ç¨·. æŒ‡æœˆå½•[M]. å‡ºç‰ˆä¿¡æ¯ä¸è¯¦. å·äºŒåå…« å…­ç¥–ä¸‹ç¬¬åå››ä¸–</a></p><style type="text/css" rel="stylesheet">.markdown-body p {    text-indent: 2em}   </style>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;p&gt;ä»VALSE2019å›æ¥åï¼Œæ„Ÿè§‰è‡ªå·±ä¿¨ç„¶å˜æˆäº†æ¬§é˜³ä¸‡é‡Œè€å¸ˆçš„è„‘æ®‹ç²‰å‘€â•°( á–â•°)ï¼ä¼šä¸Šæ¬§é˜³è€å¸ˆä»‹ç»çš„FishNetç®€ç›´è®©æˆ‘çœ¼å‰ä¸€äº®ï¼Œè¿™ä¹ˆå¥½çš„ç‚¹å­ï¼Œæˆ‘æ€ä¹ˆå°±æ²¡æƒ³åˆ°å‘ï¼å›æ¥å¥½å¥½è¯»äº†ä¸€ä¸‹æ–‡ç« å’Œä»£ç ï¼Œç®€å•æ€»ç»“ä¸€ä¸‹ã€‚&lt;/p&gt;
&lt;div align=&quot;center&quot; class=&quot;figure&quot;&gt;&lt;br&gt;  &lt;img src=&quot;/images/fishnet/fish.jpg&quot; width=&quot;15%&quot; alt=&quot;æˆ‘å’¸ç”šï¼Œæ­¤é±¼ä½•èƒ½åŠä¹Ÿ&quot;&gt;&lt;br&gt;&lt;br&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://linkinpark213.com/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://linkinpark213.com/tags/Computer-Vision/"/>
    
      <category term="Reviews" scheme="http://linkinpark213.com/tags/Reviews/"/>
    
  </entry>
  
  <entry>
    <title>A Brief Review of the R-CNN Family - Region-based CNN for Object Detection</title>
    <link href="http://linkinpark213.com/2019/03/17/rcnns/"/>
    <id>http://linkinpark213.com/2019/03/17/rcnns/</id>
    <published>2019-03-17T05:01:01.000Z</published>
    <updated>2019-04-21T07:23:02.102Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><p>The R-CNNs are awesome works on object detection, which demonstrated the effectiveness of using region proposals with deep neural networks, and have become a state-of-the-art baseline for the object detection task. In this blog post I&#39;ll make a brief review of the R-CNN family - from R-CNN to Mask R-CNN, and several related works based on the idea of R-CNNs. Implementation and evaluation details are not mentioned here. For those details, please refer to the original papers provided in the <a href="#8-References">References</a> section.</p><a id="more"></a><h2 id="0-Object-Detection-before-R-CNN"><a href="#0-Object-Detection-before-R-CNN" class="headerlink" title="0 Object Detection before R-CNN"></a>0 Object Detection before R-CNN</h2><p>Before CNN was widely adopted in object detection, SIFT or HOG features are commonly used for the detection task.</p><p>Unlike image classification, detection requires localizing objects within an image. Common approaches to localization are 1) bounding box regression, and 2) sliding-window detector. The first approach used in [1] proved to be not working very well, while the second used in [2] needs high spatial resolutions, thus deeper networks makes precise localization a challenge.</p><h2 id="1-R-CNN-Region-based-R-CNN"><a href="#1-R-CNN-Region-based-R-CNN" class="headerlink" title="1 R-CNN: Region-based R-CNN"></a>1 R-CNN: Region-based R-CNN</h2><p>R-CNN solves the CNN localization problem by operating the &quot;recognition using regions&quot; paradigm.</p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>From the input image, the method first generates around 2000 category-independent region proposals with Selective Search algorithm, and then extracts a fixed-length feature vector from each proposal using the same CNN(AlexNet). Finally, it classifies each region with category-specific linear SVMs. </p><div align="center" class="figure"><br>    <img src="/images/rcnns/R-CNN.png" width="50%" height="50%" alt><br>    Fig. 1 Overview of R-CNN.<br><br></div><p>However, the region proposal may not be that satisfactory as a final detection window. Therefore, a bounding-box regression stage is introduced to predict a new detection window given the feature map of a region proposal. As reported in [3], this simple approach fixes a large number of mislocalized detections. More details are available in the supplementary material[12] of the R-CNN paper.</p><p>Since AlexNet only takes images of size 227 Ã— 227, the image clip in the bounding box should be resized.<br>In R-CNN, the image clip is directly warped into the demanded size.</p><div align="center" class="figure"><br>    <img src="/images/rcnns/warp.png" width="20%" alt><br>    Fig. 2 Cropping from the bounding box and warping.<br><br></div><h3 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h3><ul><li>The <em>region proposal(RoI) - feature extraction - classification</em> approach</li><li>Using <em>Selective Search</em> to generate region proposals</li><li>Using <em>bounding-box regression</em> to refine region proposals</li><li>Using <em>CNN features</em> for classification</li></ul><h3 id="Known-drawbacks"><a href="#Known-drawbacks" class="headerlink" title="Known drawbacks"></a>Known drawbacks</h3><ul><li>Run CNN feature extraction on each of the 2000 regions consumes too much computation</li><li>The warped content may result in unwanted geometric distortion</li></ul><h2 id="2-SPP-Net-Spatial-Pyramid-Pooling"><a href="#2-SPP-Net-Spatial-Pyramid-Pooling" class="headerlink" title="2 SPP-Net: Spatial Pyramid Pooling"></a>2 SPP-Net: Spatial Pyramid Pooling</h2><p>SPP-Net introduces the spatial pyramid pooling layer that takes in feature maps of arbitrary size, while also considering multi-scale features in the input image. It also solved the way-too-slow issue of R-CNN.</p><h3 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h3><p>While R-CNN extracts features from warped image clips in each proposed region, the SPP-Net first extracts the feature of the whole image and get one shared feature map. After this, the feature map is cropped according to the bounding boxes (boxes fixed by regressor, same as R-CNN). Each of the feature map clip is put into the spatial pyramid pooling layers to get a feature vector of the same length. Then the feature vectors are the inputs of following fully connected layers which are the same as R-CNN.</p><div align="center" class="figure"><br>    <img src="/images/rcnns/SPP-Net.png" width="60%" alt><br>    Fig. 3 Overview of SPP-Net.<br><br></div><div align="center" class="figure"><br>    <img src="/images/rcnns/spplayer.png" width="25%" alt><br>    Fig. 4 Spatial pyramid pooling layer.<br><br></div><p>The spatial pyramid pooling layers consider the feature map clip in different scales - it divides the feature map clip into 4 Ã— 4, 2 Ã— 2 and 1 Ã— 1 grids and computes 4 Ã— 4, 2 Ã— 2 and 1 Ã— 1 feature maps (channel number doesn&#39;t change). The computed feature maps are flattened and concatenated into one vector, which is the input of the following fully connected layers.</p><h3 id="Contribution-1"><a href="#Contribution-1" class="headerlink" title="Contribution"></a>Contribution</h3><ul><li>Extracting <em>feature maps first and only once</em>, greatly improves the speed of R-CNN</li><li>Using <em>spatial pyramid pooling layers</em>, avoiding geometric distortion</li></ul><h3 id="Known-drawbacks-1"><a href="#Known-drawbacks-1" class="headerlink" title="Known drawbacks"></a>Known drawbacks</h3><ul><li>Training classifier and box regressor separately requires much work</li></ul><h2 id="3-Fast-R-CNN-Feature-Extraction-Only-Once"><a href="#3-Fast-R-CNN-Feature-Extraction-Only-Once" class="headerlink" title="3 Fast R-CNN: Feature Extraction Only Once"></a>3 Fast R-CNN: Feature Extraction Only Once</h2><p>As mentioned in the paper, R-CNN is slow because it performs a ConvNet forward pass for each object proposal, without sharing computation. Fast R-CNN improved its detection efficiency by using the deeper VGG16 network, which is 213 times(nice number :D) faster than R-CNN. It also introduced RoI pooling layer, which is simple a special case of SPP-Net where only one scale is considered(only one pyramid level). Fast R-CNN uses a multi-task loss and is trained in single stage, updating all network layers. Fast R-CNN yields higher detection quality(mAP) than R-CNN and SPP-Net, while being comparatively fast to train and test.</p><h3 id="Overview-2"><a href="#Overview-2" class="headerlink" title="Overview"></a>Overview</h3><p>Similar to SPP-Net, Fast R-CNN extracts image features before the RoI-based projection to share computation and speed up detection. But differently, Fast R-CNN uses a deep neural network - <a href="http://localhost:4000/2018/04/21/vgg/" target="_blank" rel="noopener">VGG</a>16 for more efficient feature extraction. Rather than training bounding-box regressor and classifier separately, Fast R-CNN uses a streamlined training process and jointly optimize a softmax classifier and a bounding-box regressor. The RoI-fixing regressor is moved after the fully-connected layers. The multi-task loss <strong>for each RoI</strong> is defined as:</p><p>$$L(p,u,t^u,v) = L_{cls}[p,u]+\lambda[u\geq 1]L_{loc}(t^u,v)$$</p><p>in which the definition of classification loss and localization loss are:</p><p>$$L_{cls}(p,u)=-log(p_u)$$</p><p>$$L_{loc}(t^u,v)=\sum_{i\in \{x,y,w,h\}}{smooth_{L_1}(t_i^u-v_i)}$$</p><p>in which \(smooth_{L_1}\) loss is defined as:</p><p>$$smooth_{L_1}(x)=\begin{cases}<br>0.5x^2&amp; \text{if |x|&lt;1}\\<br>|x|-0.5&amp; \text{otherwise}<br>\end{cases}$$</p><p>Symbol definitions:</p><table><thead><tr><th>Symbol</th><th><center>Definition</center></th><th><center>Definition by formula</center></th></tr></thead><tbody><tr><td><center>\(p\)</center></td><td>Output of the classification layer, a vector of length \(K+1\)(K object classes and background)</td><td>\(p=(p_0,\cdots,p_K)\)</td></tr><tr><td><center>\(t\)</center></td><td>Output of the regression layer, a matrix of size \(K\times 4\).</td><td>\(t^k=(t^k_x,t^k_y,t^k_w,t^k_h)\)</td></tr><tr><td><center>\(u\)</center></td><td>True class.</td><td>\(u\in N, 1\le u \le K\)</td></tr><tr><td><center>\(v\)</center></td><td>True bounding-box regression target.</td><td>\(v=(v_x,v_y,v_w,v_h)\)</td></tr></tbody></table><div align="center" class="figure"><br>    <img src="/images/rcnns/Fast R-CNN.png" width="60%" alt><br>    Fig. 5 Overview of Fast R-CNN.<br><br></div><p>In this architecture, two of the three main procedures except region proposal are trained in single-stage with the multi-task loss. </p><p>Here is are two graphs demonstrating common pooling layers(max or avg) and RoI pooling layers. On the left is the original 5x5 feature map, and each in grid is a pixel value. During calculation, the common pooling kernel covers an area each step and calculates the maximum value or the average value in the area. With a kernel size of 3x3 and a stride of 2, a feature map of 2x2 is generated from the 5x5 feature map.</p><div align="center" class="figure"><br>    <img src="/images/rcnns/maxpool.png" width="40%" alt><br>    Fig. 6 Common pooling with kernel_size=3 and stride=2.<br><br></div><p>And in RoI pooling, the RoI is cropped from the whole feature map, and is divided into pieces with equal areas according to the output feature map size. However, it&#39;s possible that grids on the borders of different pieces have to be assigned to one piece only. In this case, there may be a little bit of &quot;injustice&quot; among the pieces. In each piece, a global average/maximum pooling is done and the result is only one number in each channel.</p><div align="center" class="figure"><br>    <img src="/images/rcnns/roipool.png" width="40%" alt><br>    Fig. 7 RoI pooling with output size=(2, 2). The black dashed line denotes the original RoI, and the colored area is the actual cropped RoI.<br><br></div><h3 id="Contribution-2"><a href="#Contribution-2" class="headerlink" title="Contribution"></a>Contribution</h3><ul><li>Deeper CNN - <em>VGG16</em> for feature extraction</li><li><em>Multi-task loss</em> &amp; <em>Single-stage training</em></li></ul><h3 id="Known-drawbacks-2"><a href="#Known-drawbacks-2" class="headerlink" title="Known drawbacks"></a>Known drawbacks</h3><ul><li>For region proposal, conventional Selective Search algorithm doesn&#39;t make use of GPU computation power, thus consuming more time</li></ul><h2 id="4-Faster-R-CNN-Region-Proposal-Networks-Speeds-You-Up"><a href="#4-Faster-R-CNN-Region-Proposal-Networks-Speeds-You-Up" class="headerlink" title="4 Faster R-CNN: Region Proposal Networks Speeds You Up"></a>4 Faster R-CNN: Region Proposal Networks Speeds You Up</h2><p>In Fast R-CNN, two of the three main procedures are trained in single-stage, except region proposal. And region proposal is the bottleneck of total detection speed, since GPU with high computation power isn&#39;t utilized here yet. Why not try training a CNN that generates region proposals?</p><h3 id="Overview-3"><a href="#Overview-3" class="headerlink" title="Overview"></a>Overview</h3><p>Simply remove the Selective Search in Fast R-CNN. In place of the SS algorithm, an RPN(Region Proposal Network) is introduced. Given the DCNN features, the RPN generates RoIs with improved speed.</p><h3 id="but-how-on-earth-does-the-RPN-work"><a href="#but-how-on-earth-does-the-RPN-work" class="headerlink" title="...but how on earth does the RPN work?"></a>...but how on earth does the RPN work?</h3><p>This is a question that had been confusing me for so long. </p><p>In a word, it&#39;s a simple CNN taking an image of any size as input, slides a window and outputs \(6k\) numbers each time the window moves. \(k\) is the number of anchors pre-defined - IT DOES NOT MEAN &quot;THOUSAND&quot;. Wait, what is an anchor?</p><p>An anchor is a box size we define first before generating data (for example, \((width=36, height=78)\) for pedestrain, and \((width=50, height=34)\) for dogs?). Though the input image is of size \(n * n\), the anchor can be in any size and any w-h ratio. The prediction of the 6 numbers are based on the anchors we define. When the RPN works, it does NOT predict the possibility that there is an object - BUT the possibility that there is an object that fits in the anchor.</p><p>Besides a classification layer predicting the possibility of there being an object and the possibility of there being nothing but background, a regression layer predicts the relative box coordinates \((t_x, t_y, t_w, t_h)\). For each anchor, its size \((w_a, h_a)\) is given and its position \((w_x, w_y)\) is decided by the center position of the sliding window. The relation between relative coordinates \((t_x, t_y, t_w, t_h)\) and absolute coordinates \((x, y, w, h)\) is:<br>$$t_x=(x-x_a)/w_a\\<br>t_y=(y-y_a)/h_a\\<br>t_w=log(w/w_a)\\<br>t_h=log(h/h_a)$$</p><p>for both prediction and ground truth.</p><div align="center" class="figure"><br>    <img src="/images/rcnns/RPN.png" width="30%" alt><br>    Fig. 8 The original graph demonstration of RPN. Keep in mind that &quot;k&quot; does not mean &quot;thousand&quot;.<br><br></div><p>But there are a great pile of boxes generated by the RPN. Some basic methods have to be taken to select the &quot;good&quot; boxes. Firstly, the boxes with low object scores and high background scores (usually thresholds are set manually) are abandoned. Secondly, using <a href="https://zh.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH" target="_blank" rel="noopener">non-maximum supression</a>, one box for each object target is elected from all boxes that mark the same object.</p><h3 id="Contribution-3"><a href="#Contribution-3" class="headerlink" title="Contribution"></a>Contribution</h3><ul><li><em>Region Proposal Network</em> - high-speed high-quality region proposals</li></ul><h2 id="5-Mask-R-CNN-Detection-and-Instance-Segmentation"><a href="#5-Mask-R-CNN-Detection-and-Instance-Segmentation" class="headerlink" title="5 Mask R-CNN: Detection and Instance Segmentation"></a>5 Mask R-CNN: Detection and Instance Segmentation</h2><p>Though Mask R-CNN is a great work, its idea is rather intuitive - since detection and classification is done, why not add a segmentation head? In this case, some instance-first instance segmentation work would be done!</p><h3 id="Overview-4"><a href="#Overview-4" class="headerlink" title="Overview"></a>Overview</h3><p>Add a small mask fully-convolutional overhead to Faster R-CNN, replace VGG net with more efficient ResNet/FPN(Residual Network / Feature Pyramid Network) and replace RoI pooling with RoI alignment.</p><div align="center" class="figure"><br>    <img src="/images/rcnns/Mask R-CNN.png" width="30%" alt><br>    Fig. 9 The Mask R-CNN framework for instance segmentation. The last convlutional layer is the newly added segmentation layer for each RoI.<br><br></div><h3 id="RoIAlign"><a href="#RoIAlign" class="headerlink" title="RoIAlign"></a>RoIAlign</h3><p>In RoI pooling, quantization will be performed when the RoI coordinates are not integers. For example, when cutting the area \((x_1=11.02, y_1=53.9, x_2=16.2, y_2=58.74)\), actually the area \((x_1=11, y_1=54, x_2=16, y_2=59)\) is what we get (nearest-neighbor).</p><p>But in RoI alignment, the area is exactly \((x_1=11.02, y_1=53.9, x_2=16.2, y_2=58.74)\). Instead of cropping it down, the feature map area is sampled using some sample points. Divide the RoI into \(n*n\)(output size) bins Using bi-linear interpolation, one value would be calculated at each sample point. In the image below is a simple example. In this case we have only one sample point for each pixel in the pooled RoI. Coordinate of the only sample point in the first area is \((12.315, 55.11)\). Calculate the weighted average of the 4 grid points nearby his sample point and we&#39;ll have the value for this pixel in the pooled feature map.</p><div align="center" class="figure"><br>    <img src="/images/rcnns/roialign1.png" width="25%" alt><br>    Fig. 10 RoI alignment with output size=(2, 2) and 1 sample point each bin.<br><br></div><p>It&#39;s obvious that one sample point each bin is far from enough in our example. So using more sample points is wiser.</p><div align="center" class="figure"><br>    <img src="/images/rcnns/roialign2.png" width="25%" alt><br>    Fig. 11 RoI alignment with output size=(2, 2) and 2Ã—2 sample point each bin.<br><br></div><h3 id="Contribution-4"><a href="#Contribution-4" class="headerlink" title="Contribution"></a>Contribution</h3><ul><li><em>RoI Align</em> - improving mask accuracy greatly</li><li>Add a segmentation overhead on Faster R-CNN and achieve accurate instance segmentation</li></ul><h2 id="6-R-CNNs-Proposed-by-Other-Researchers"><a href="#6-R-CNNs-Proposed-by-Other-Researchers" class="headerlink" title="6 R-CNNs Proposed by Other Researchers"></a>6 R-CNNs Proposed by Other Researchers</h2><p>There are several other R-CNNs by other researchers, which are basically variants of the R-CNN architecture.</p><h3 id="Light-Head-R-CNN"><a href="#Light-Head-R-CNN" class="headerlink" title="Light-Head R-CNN"></a>Light-Head R-CNN</h3><p>arXiv: <a href="https://arxiv.org/abs/1711.07264" target="_blank" rel="noopener">https://arxiv.org/abs/1711.07264</a><br>Code(Official, TensorFlow): <a href="https://github.com/zengarden/light_head_rcnn" target="_blank" rel="noopener">https://github.com/zengarden/light_head_rcnn</a></p><h3 id="Cascade-R-CNN"><a href="#Cascade-R-CNN" class="headerlink" title="Cascade R-CNN"></a>Cascade R-CNN</h3><p>arXiv: <a href="https://arxiv.org/abs/1712.00726" target="_blank" rel="noopener">https://arxiv.org/abs/1712.00726</a><br>Code(Official, Caffe): <a href="https://github.com/zhaoweicai/cascade-rcnn" target="_blank" rel="noopener">https://github.com/zhaoweicai/cascade-rcnn</a><br>Code(PyTorch): <a href="https://github.com/guoruoqian/cascade-rcnn_Pytorch" target="_blank" rel="noopener">https://github.com/guoruoqian/cascade-rcnn_Pytorch</a></p><h3 id="Grid-R-CNN"><a href="#Grid-R-CNN" class="headerlink" title="Grid R-CNN"></a>Grid R-CNN</h3><p>arXiv: <a href="https://arxiv.org/abs/1811.12030" target="_blank" rel="noopener">https://arxiv.org/abs/1811.12030</a><br>Code: Not yet</p><h2 id="8-References"><a href="#8-References" class="headerlink" title="8 References"></a>8 References</h2><p>[1] <a href="http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf" target="_blank" rel="noopener">Szegedy, Christian, Alexander Toshev, and Dumitru Erhan. &quot;Deep neural networks for object detection.&quot; Advances in neural information processing systems. 2013.</a></p><p>[2] <a href="https://arxiv.org/pdf/1312.6229.pdf" target="_blank" rel="noopener">Sermanet, Pierre, et al. &quot;Overfeat: Integrated recognition, localization and detection using convolutional networks.&quot; arXiv preprint arXiv:1312.6229 (2013).</a></p><p>[3] <a href="https://arxiv.org/pdf/1311.2524.pdf" target="_blank" rel="noopener">Girshick, Ross, et al. &quot;Rich feature hierarchies for accurate object detection and semantic segmentation.&quot; Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.</a></p><p>[4] <a href="https://arxiv.org/pdf/1406.4729.pdf" target="_blank" rel="noopener">He, Kaiming, et al. &quot;Spatial pyramid pooling in deep convolutional networks for visual recognition.&quot; European conference on computer vision. Springer, Cham, 2014.</a></p><p>[5] <a href="https://arxiv.org/pdf/1504.08083.pdf" target="_blank" rel="noopener">Girshick, Ross. &quot;Fast r-cnn.&quot; Proceedings of the IEEE international conference on computer vision. 2015.</a></p><p>[6] <a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf" target="_blank" rel="noopener">Ren, Shaoqing, et al. &quot;Faster r-cnn: Towards real-time object detection with region proposal networks.&quot; Advances in neural information processing systems. 2015.</a></p><p>[7] <a href="https://arxiv.org/pdf/1703.06870.pdf" target="_blank" rel="noopener">He, Kaiming, et al. &quot;Mask r-cnn.&quot; Computer Vision (ICCV), 2017 IEEE International Conference on. IEEE, 2017.</a></p><p>[8] <a href="https://arxiv.org/pdf/1711.07264.pdf" target="_blank" rel="noopener">Li, Zeming, et al. &quot;Light-head r-cnn: In defense of two-stage object detector.&quot; arXiv preprint arXiv:1711.07264 (2017).</a></p><p>[9] <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2603.pdf" target="_blank" rel="noopener">Cai, Zhaowei, and Nuno Vasconcelos. &quot;Cascade r-cnn: Delving into high quality object detection.&quot; IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Vol. 1. No. 2. 2018.</a></p><p>[10] <a href="https://arxiv.org/abs/1811.12030" target="_blank" rel="noopener">Xin Lu, et al. &quot;Grid R-CNN.&quot; arXiv preprint arXiv:1811.12030 (2018).</a></p><p>[11] <a href="https://dl.dropboxusercontent.com/s/1yisyl5cuxo7g9y/r-cnn-cvpr-supp.pdf?dl=0" target="_blank" rel="noopener">Girshick, Ross, et al. &quot;Rich feature hierarchies for accurate object detection and semantic segmentation.&quot; Proceedings of the IEEE conference on computer vision and pattern recognition - supplementary material. 2014.</a></p>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;p&gt;The R-CNNs are awesome works on object detection, which demonstrated the effectiveness of using region proposals with deep neural networks, and have become a state-of-the-art baseline for the object detection task. In this blog post I&amp;#39;ll make a brief review of the R-CNN family - from R-CNN to Mask R-CNN, and several related works based on the idea of R-CNNs. Implementation and evaluation details are not mentioned here. For those details, please refer to the original papers provided in the &lt;a href=&quot;#8-References&quot;&gt;References&lt;/a&gt; section.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://linkinpark213.com/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://linkinpark213.com/tags/Computer-Vision/"/>
    
      <category term="Reviews" scheme="http://linkinpark213.com/tags/Reviews/"/>
    
      <category term="Object Detection" scheme="http://linkinpark213.com/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[MineSweeping] The Long Struggle of DensePose Installation</title>
    <link href="http://linkinpark213.com/2018/11/18/densepose-minesweeping/"/>
    <id>http://linkinpark213.com/2018/11/18/densepose-minesweeping/</id>
    <published>2018-11-18T08:00:14.000Z</published>
    <updated>2019-10-11T02:06:27.271Z</updated>
    
    <content type="html"><![CDATA[<p>Update in October 2019:</p><p>DensePose has been re-implemented with the brand-new object detection framework <a href="https://github.com/facebookresearch/detectron2" target="_blank" rel="noopener">Detectron2</a>, which is based on PyTorch and much easier to install and use (You don&#39;t have to manually compile Caffe2)<br>I strongly recommend that you check out the new official DensePose code at <a href="https://github.com/facebookresearch/detectron2/tree/master/projects/DensePose" target="_blank" rel="noopener">https://github.com/facebookresearch/detectron2/tree/master/projects/DensePose</a>.</p><p><br><br>DensePose is a great work in real-time human pose estimation, which is based on Caffe2 and Detectron framework. It extracts dense human body 3D surface based on RGB images. The installation instructions are provided <a href="https://github.com/facebookresearch/DensePose/blob/master/INSTALL.md" target="_blank" rel="noopener">here</a>.</p><p>During my installation process, these are the problems that took me some time to tackle. I spent on week to finally figure out solutions to all the issues. So lucky of me not to give up too early...</p><div align="center"><br>    <img src="/images/densepose-ms/facebook.jpg" width="15%" height="15%" alt="Greetings from Facebook AI Research"><br></div><a id="more"></a><p>By the way, <strong>before you suffer too much</strong>, I strongly recommend following the step-by-step <a href="https://github.com/Johnqczhang/densepose_installation/blob/master/README.md" target="_blank" rel="noopener">Caffe2+DensePose installation guide</a> by <a href="https://github.com/Johnqczhang" target="_blank" rel="noopener">@Johnqczhang</a>. If you think you&#39;re almost there, help yourself with the solutions below~</p><h2 id="1-Environment"><a href="#1-Environment" class="headerlink" title="1 Environment"></a>1 Environment</h2><ul><li>System: Ubuntu 18.04</li><li>Linux kernel: 4.15.0-29-generic</li><li>Graphics card: NVIDIA GeForce 1080Ti</li><li>Graphics driver: 410.48</li><li>CUDA: 10.0.130</li><li>cuDNN: 7.3.1</li><li>Caffe2: Built from source</li><li>Python: 2.7.15, based on Anaconda 4.5.11</li></ul><h2 id="2-Problems-amp-Solutions"><a href="#2-Problems-amp-Solutions" class="headerlink" title="2 Problems &amp; Solutions"></a>2 Problems &amp; Solutions</h2><h3 id="2-1-Caffe2-module-not-found"><a href="#2-1-Caffe2-module-not-found" class="headerlink" title="2.1 Caffe2 module not found"></a>2.1 Caffe2 module not found</h3><h4 id="Details"><a href="#Details" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>make</code>.</p><p>Main error message:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Could not find a package configuration file provided by <span class="string">"Caffe2"</span> with any  </span><br><span class="line">of the following names: </span><br><span class="line">    Caffe2Config.cmake </span><br><span class="line">    caffe2-config.cmake</span><br></pre></td></tr></table></figure></p><h4 id="Cause"><a href="#Cause" class="headerlink" title="Cause"></a>Cause</h4><p>Caffe2 build path isn&#39;t known by CMake.</p><h4 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>(Caffe2_DIR <span class="string">"/path/to/pytorch/torch/share/cmake/Caffe2/"</span>)</span><br></pre></td></tr></table></figure></p><p>(Note: <code>set(Caffe2_DIR &quot;/path/to/pytorch/build/&quot;)</code> can also fix this issue but may cause other issues.) </p><h3 id="2-2-Detectron-ops-lib-not-found"><a href="#2-2-Detectron-ops-lib-not-found" class="headerlink" title="2.2 Detectron ops lib not found"></a>2.2 Detectron ops lib not found</h3><h4 id="Details-1"><a href="#Details-1" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>python2 $DENSEPOSE/detectron/tests/test_spatial_narrow_as_op.py</code> after <code>make</code>.</p><p>Main error message:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Detectron ops lib not found; make sure that your Caffe2 version includes Detectron module.</span><br></pre></td></tr></table></figure></p><h4 id="Cause-1"><a href="#Cause-1" class="headerlink" title="Cause"></a>Cause</h4><p>Seems that the Python part of DensePose couldn&#39;t recognize Caffe2.</p><h4 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution"></a>Solution</h4><p>Add <code>/path/to/pytorch/build</code> to <code>PYTHONPATH</code> environment variable. Could be added by directly <code>export PYTHONPATH=$PYTHONPATH:/path/to/pytorch/build</code> instruction or by adding this line to <code>~/.bashrc</code>. Remember to run <code>source ~/.bashrc</code> after the modification.</p><h3 id="2-3-cmake-files-not-found-amp-Unknown-CMake-command-quot-caffe2-interface-library-quot"><a href="#2-3-cmake-files-not-found-amp-Unknown-CMake-command-quot-caffe2-interface-library-quot" class="headerlink" title="2.3 *.cmake files not found &amp; Unknown CMake command &quot;caffe2_interface_library&quot;"></a>2.3 *.cmake files not found &amp; Unknown CMake command &quot;caffe2_interface_library&quot;</h3><h4 id="Details-2"><a href="#Details-2" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p><p>Main error message:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">CMake Error at /path/to/pytorch/build/Caffe2Config.cmake:<span class="number">14</span> (<span class="keyword">include</span>):</span><br><span class="line">  <span class="keyword">include</span> could not find load file:</span><br><span class="line"></span><br><span class="line">    /path/to/pytorch/build/public/utils.cmake</span><br><span class="line">    /path/to/pytorch/build/public/threads.cmake</span><br><span class="line">    /path/to/pytorch/build/public/cuda.cmake</span><br><span class="line">    /path/to/pytorch/build/public/mkl.cmake</span><br><span class="line">    /path/to/pytorch/build/Caffe2Targets.cmake</span><br><span class="line"></span><br><span class="line">Call Stack (most recent call first):</span><br><span class="line">  CMakeLists.txt:<span class="number">8</span> (<span class="keyword">find_package</span>)</span><br><span class="line"></span><br><span class="line">CMake Error at /path/to/pytorch/build/Caffe2Config.cmake:<span class="number">117</span> (caffe2_interface_library):</span><br><span class="line">  Unknown CMake command <span class="string">"caffe2_interface_library"</span>.</span><br><span class="line">Call Stack (most recent call first):</span><br><span class="line">  CMakeLists.txt:<span class="number">8</span> (<span class="keyword">find_package</span>)</span><br></pre></td></tr></table></figure></p><p>(Several <code>*.cmake</code> files, I only showed a few.)</p><h4 id="Cause-2"><a href="#Cause-2" class="headerlink" title="Cause"></a>Cause</h4><p>These files are not in the <code>pytorch/build</code> directory. By searching, I found that they are in the <code>pytorch/torch/share/cmake/Caffe2</code> directory.</p><h4 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>(Caffe2_DIR <span class="string">"/path/to/pytorch/torch/share/cmake/Caffe2/"</span>)</span><br></pre></td></tr></table></figure></p><h3 id="2-4-quot-context-gpu-h-quot-not-found"><a href="#2-4-quot-context-gpu-h-quot-not-found" class="headerlink" title="2.4 &quot;context_gpu.h&quot; not found."></a>2.4 &quot;context_gpu.h&quot; not found.</h3><h4 id="Details-3"><a href="#Details-3" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p><p>I forgot to record the error messages, but it should be obvious that some header files(not just <code>context_gpu.h</code>) are missing.</p><h4 id="Cause-3"><a href="#Cause-3" class="headerlink" title="Cause"></a>Cause</h4><p>This time it&#39;s the include path not recognized...</p><h4 id="Solution-3"><a href="#Solution-3" class="headerlink" title="Solution"></a>Solution</h4><p>Added one line in the beginning of CMakeLists.txt:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">include_directories</span>(<span class="string">"/path/to/pytorch/torch/lib/include"</span>)</span><br></pre></td></tr></table></figure></p><h3 id="2-5-This-file-was-generated-by-a-newer-version-of-protoc-which-is-incompatible-with-your-Protocol-Buffer-headers-Please-update-your-headers"><a href="#2-5-This-file-was-generated-by-a-newer-version-of-protoc-which-is-incompatible-with-your-Protocol-Buffer-headers-Please-update-your-headers" class="headerlink" title="2.5 This file was generated by a newer version of protoc which is incompatible with your Protocol Buffer headers. Please update your headers."></a>2.5 This file was generated by a newer version of protoc which is incompatible with your Protocol Buffer headers. Please update your headers.</h3><h4 id="Details-4"><a href="#Details-4" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p><p>Main error message:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/path/to/pytorch/torch/lib/include/caffe2/proto/caffe2.pb.h:12:2: error: #error This file was generated by a newer version of protoc which is</span><br><span class="line">#error This file was generated by a newer version of protoc which is</span><br><span class="line">^</span><br><span class="line">/path/to/pytorch/torch/lib/include/caffe2/proto/caffe2.pb.h:13:2: error: #error incompatible with your Protocol Buffer headers. Please update</span><br><span class="line">#error incompatible with your Protocol Buffer headers. Please update</span><br><span class="line">^</span><br><span class="line">/path/to/pytorch/torch/lib/include/caffe2/proto/caffe2.pb.h:14:2: error: #error your headers.</span><br><span class="line">#error your headers.</span><br><span class="line">^</span><br></pre></td></tr></table></figure></p><h4 id="Cause-4"><a href="#Cause-4" class="headerlink" title="Cause"></a>Cause</h4><p>If you only have a protobuf higher than v3.6.1, this should not happen. Check if you have multiple protobufs installed from different sources. (In my case, there was a protobuf v3.2.0 installed with <code>apt-get</code> earlier)</p><h4 id="Solution-4"><a href="#Solution-4" class="headerlink" title="Solution"></a>Solution</h4><p>I can&#39;t provide an exact solution. Please try<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">which protoc</span><br></pre></td></tr></table></figure></p><p>and see where protobuf is installed. If this shows the protobuf you installed with Anaconda, remove it completely and try this again. Since DensePose tells you that you have an older version of protobuf, you should be able to locate one. After finding it, remove it or upgrade it to v3.6.1 or higher. I would prefer installing protobuf from source <a href="https://github.com/protocolbuffers/protobuf/releases" target="_blank" rel="noopener">here</a>. It&#39;s not so painful as installing DensePose.</p><h3 id="2-6-quot-mkl-cblas-h-quot-not-found"><a href="#2-6-quot-mkl-cblas-h-quot-not-found" class="headerlink" title="2.6 &quot;mkl_cblas.h&quot; not found."></a>2.6 &quot;mkl_cblas.h&quot; not found.</h3><h4 id="Details-5"><a href="#Details-5" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>make ops</code>.</p><p>I forgot to record the error messages, but it should be obvious too.</p><h4 id="Cause-5"><a href="#Cause-5" class="headerlink" title="Cause"></a>Cause</h4><p>Intel Math Kernel Library was turned on but not found. (Why is it enabled when I didn&#39;t even install it???)</p><h4 id="Solution-5"><a href="#Solution-5" class="headerlink" title="Solution"></a>Solution</h4><p>Install Intel Math Kernel Library <a href="https://software.intel.com/en-us/mkl/choose-download/linux" target="_blank" rel="noopener">here</a> and add <code>/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/include</code> to <code>C_PATH</code> environment variable:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export CPATH=$CPATH:/opt/intel/compilers_and_libraries_2019.1.144/linux/mkl/include</span><br></pre></td></tr></table></figure></p><p>The exact path may vary according to the MKL version and your configuration.<br>Maybe try <code>find / -name mkl_cblas.h</code> to make sure of its location after the installation.</p><p>Adding the path to CMakeLists.txt should also be helpful, but I didn&#39;t test it:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">include_directories(&quot;/opt/intel/compilers_and_libraries_2019.1.144/Linux/mkl/include&quot;)</span><br></pre></td></tr></table></figure></p><h3 id="2-7-GetSingleArgumentâ€™-is-not-a-member-of-â€˜caffe2-PoolPointsInterpOpâ€™"><a href="#2-7-GetSingleArgumentâ€™-is-not-a-member-of-â€˜caffe2-PoolPointsInterpOpâ€™" class="headerlink" title="2.7 GetSingleArgumentâ€™ is not a member of â€˜caffe2::PoolPointsInterpOpâ€™"></a>2.7 GetSingleArgument<float>â€™ is not a member of â€˜caffe2::PoolPointsInterpOp<t, context>â€™</t,></float></h3><h4 id="Details-6"><a href="#Details-6" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>make ops</code>.<br>Main error message:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">/path/to/pytorch/caffe2/operators/accumulate_op.h: In constructor â€˜caffe2::AccumulateOp&lt;T, Context&gt;::AccumulateOp(const caffe2::OperatorDef&amp;, caffe2::Workspace*)â€™:</span><br><span class="line">/path/to/pytorch/caffe2/operators/accumulate_op.h:13:187: error: â€˜GetSingleArgument&lt;<span class="built_in">float</span>&gt;â€™ is not a member of â€˜caffe2::AccumulateOp&lt;T, Context&gt;â€™</span><br><span class="line">   AccumulateOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class="line">                                                                                                                                                                                           ^                        </span><br><span class="line">/path/to/pytorch/caffe2/operators/elementwise_ops.h: In constructor â€˜caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;::BinaryElementwiseWithArgsOp(const caffe2::OperatorDef&amp;, caffe2::Workspace*)â€™:</span><br><span class="line">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:189: error: â€˜GetSingleArgument&lt;bool&gt;â€™ is not a member of â€˜caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;â€™</span><br><span class="line">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class="line">                                                                                                                                                                                             ^                       </span><br><span class="line">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:272: error: â€˜GetSingleArgument&lt;int&gt;â€™ is not a member of â€˜caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;â€™</span><br><span class="line">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br><span class="line">                                                                                                                                                                                                                                                                                ^                      </span><br><span class="line">/path/to/pytorch/caffe2/operators/elementwise_ops.h:106:350: error: â€˜GetSingleArgument&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;â€™ is not a member of â€˜caffe2::BinaryElementwiseWithArgsOp&lt;InputTypes, Context, Functor, OutputTypeMap&gt;â€™</span><br><span class="line">   BinaryElementwiseWithArgsOp(const OperatorDef&amp; operator_def, Workspace* ws)</span><br></pre></td></tr></table></figure></p><h4 id="Cause-6"><a href="#Cause-6" class="headerlink" title="Cause"></a>Cause</h4><p>I&#39;m not sure. Could be that <code>GetSingleArgument()</code> is defined elsewhere?</p><h4 id="Solution-6"><a href="#Solution-6" class="headerlink" title="Solution"></a>Solution</h4><p>Modify <code>/path/to/densepose/detectron/ops/pool_points_interp.h</code>. Change <code>OperatorBase::GetSingleArgument&lt;float&gt;</code> to <code>this-&gt;template GetSingleArgument&lt;float&gt;</code></p><p>(Thanks to badpx@Github: <a href="https://github.com/facebookresearch/DensePose/pull/137/commits/51389c6a02173a25e9429825db452beb5e1cf3be" target="_blank" rel="noopener">https://github.com/facebookresearch/DensePose/pull/137/commits/51389c6a02173a25e9429825db452beb5e1cf3be</a>) </p><h3 id="2-8-fatal-error-caffe2-utils-threadpool-ThreadPool-h-No-such-file-or-directory"><a href="#2-8-fatal-error-caffe2-utils-threadpool-ThreadPool-h-No-such-file-or-directory" class="headerlink" title="2.8 fatal error: caffe2/utils/threadpool/ThreadPool.h: No such file or directory"></a>2.8 fatal error: caffe2/utils/threadpool/ThreadPool.h: No such file or directory</h3><h4 id="Details-7"><a href="#Details-7" class="headerlink" title="Details"></a>Details</h4><p>Occurs when running &quot;make ops&quot;.</p><p>Main error message:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/path/to/pytorch/torch/lib/include/caffe2/core/workspace.h:19:48: fatal error: caffe2/utils/threadpool/ThreadPool.h: No such file or directory</span><br></pre></td></tr></table></figure></p><p>This should only happen when your Caffe2 is installed with Anaconda.</p><h3 id="Cause-7"><a href="#Cause-7" class="headerlink" title="Cause"></a>Cause</h3><p>If your Caffe2 is installed with Anaconda, these files may not be found anywhere in the Caffe2 directory, or in your hard disk at all.</p><h3 id="Solution-7"><a href="#Solution-7" class="headerlink" title="Solution"></a>Solution</h3><p>In <a href="https://github.com/facebookresearch/DensePose/issues/152" target="_blank" rel="noopener">Anikily@Github</a>&#39;s case, downloading Caffe2 source code and add its path to DensePose&#39;s include directories will work:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:pytorch/pytorch.git</span><br></pre></td></tr></table></figure><p>and add one line in the beginning of DensePose/CMakeLists.txt:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">include_directories(&quot;/path/to/pytorch&quot;)</span><br></pre></td></tr></table></figure></p><p>The directory you include here should contain caffe2/utils/threadpool/ThreadPool.h and all the others.</p><p>I don&#39;t think this issue should be solved this way, but I&#39;m sure that these files couldn&#39;t be found anywhere else. If anyone finds a better solution, please comment here to help the others.</p><h3 id="2-9-Undefined-symbol-ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE"><a href="#2-9-Undefined-symbol-ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE" class="headerlink" title="2.9 Undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE"></a>2.9 Undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE</h3><h4 id="Details-8"><a href="#Details-8" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>python detectron/tests/test_zero_even_op.py</code>.</p><p>Main error message:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OSError: /path/to/densepose/build/libcaffe2_detectron_custom_ops_gpu.so: undefined symbol: _ZN6google8protobuf8internal9ArenaImpl28AllocateAlignedAndAddCleanupEmPFvPvE</span><br></pre></td></tr></table></figure></p><h4 id="Cause-8"><a href="#Cause-8" class="headerlink" title="Cause"></a>Cause</h4><p>WTF is this!???<br>As can be seen, this symbol has something to do with Google, and protobuf.<br>I guess this is caused by a different protobuf version. Good news is that a proper version of protobuf was also built with Caffe2, so why not tell this to DensePose?</p><h4 id="Solution-8"><a href="#Solution-8" class="headerlink" title="Solution"></a>Solution</h4><p>In <code>/path/to/densepose/CMakeLists.txt</code>, Add a few lines in the beginning:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">add_library</span>(libprotobuf STATIC IMPORTED) </span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(PROTOBUF_LIB <span class="string">"/path/to/pytorch/torch/lib/libprotobuf.a"</span>) </span><br><span class="line"></span><br><span class="line"><span class="keyword">set_property</span>(TARGET libprotobuf PROPERTY IMPORTED_LOCATION <span class="string">"$&#123;PROTOBUF_LIB&#125;"</span>)</span><br></pre></td></tr></table></figure></p><p>You can find two <code>target_link_libraries</code> lines in this file(they are not adjacent):<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">target_link_libraries</span>(caffe2_detectron_custom_ops caffe2_library) </span><br><span class="line"><span class="keyword">target_link_libraries</span>(caffe2_detectron_custom_ops_gpu caffe2_gpu_library)</span><br></pre></td></tr></table></figure></p><p>Edit the two lines, adding a &quot;libprotobuf&quot; at the end to each of them:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">target_link_libraries</span>(caffe2_detectron_custom_ops caffe2_library libprotobuf) </span><br><span class="line"><span class="keyword">target_link_libraries</span>(caffe2_detectron_custom_ops_gpu caffe2_gpu_library libprotobuf)</span><br></pre></td></tr></table></figure></p><p>Then run <code>make ops</code> again, and <code>python detectron/tests/test_zero_even_op.py</code> again.</p><p>(Thanks to hyounsamk@Github: <a href="https://github.com/facebookresearch/DensePose/issues/119" target="_blank" rel="noopener">https://github.com/facebookresearch/DensePose/issues/119</a>)</p><p>After fixing this issue, my DensePose passed tests and was running flawlessly. If any more issues remain, don&#39;t hesitate to comment here~</p><h3 id="2-10-Undefined-symbol-ZN6caffe219CPUOperatorRegistryB5cxx11Ev"><a href="#2-10-Undefined-symbol-ZN6caffe219CPUOperatorRegistryB5cxx11Ev" class="headerlink" title="2.10 Undefined symbol: _ZN6caffe219CPUOperatorRegistryB5cxx11Ev"></a>2.10 Undefined symbol: _ZN6caffe219CPUOperatorRegistryB5cxx11Ev</h3><h4 id="Details-9"><a href="#Details-9" class="headerlink" title="Details"></a>Details</h4><p>Occurred when running <code>python detectron/tests/test_zero_even_op.py</code>, with Caffe2 installed with Anaconda.</p><p>Main error message:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OSError: /path/to/densepose/build/libcaffe2_detectron_custom_ops_gpu.so: undefined symbol: _ZN6caffe219CPUOperatorRegistryB5cxx11Ev</span><br></pre></td></tr></table></figure></p><h4 id="Cause-9"><a href="#Cause-9" class="headerlink" title="Cause"></a>Cause</h4><p>As can be seen from the messy undefined symbol, this should have something to do with Caffe2 and probably CXX11(oh really???).</p><p>Run <code>ldd -r /path/to/densepose/build/libcaffe2_detectron_custom_ops.so</code> and the one or several undefined symbols with similar names will be shown, which should have been defined in <code>libcaffe2.so</code>. After running <code>strings -a /path/to/pytorch/torch/lib/libcaffe2.so | grep _ZN6caffe219CPUOperator</code>, a few similar symbols (two, in my case) would come up, but are different from the one undefined - <code>&quot;B5cxx11&quot;</code> is missing.</p><p>Why does DensePose want to find a symbol with <code>&quot;B5cxx11&quot;</code>? Who added this suffix?<br>It should be our GCC who did it when compiling DensePose with C++11 standard!</p><p>To find which version of GCC was Caffe2 built by, run <code>strings -a /path/to/pytorch/torch/lib/libcaffe2.so | grep GCC:</code>.<br>In my case, the output is:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GCC: (GNU) 4.9.2 20150212 (Red Hat 4.9.2-6)</span><br></pre></td></tr></table></figure></p><p>Oh? It seems that Caffe2 developers are Red Hat lovers!<br>The Caffe2 installed with Anaconda was built by GCC 4.9.2, which had a slightly different standard on naming symbols.</p><h4 id="Solution-9"><a href="#Solution-9" class="headerlink" title="Solution"></a>Solution</h4><p>The simpliest way out is to turn to GCC 4.9.2 for building DensePose, too.<br>Otherwise, maybe also consider compiling Caffe2/PyTorch from source code?</p><p>(Many thanks to Johnqczhang@Github: <a href="https://github.com/linkinpark213/linkinpark213.github.io/issues/12" target="_blank" rel="noopener">https://github.com/linkinpark213/linkinpark213.github.io/issues/12</a>)</p><div align="center"><br>    <img src="/images/densepose-ms/ican.jpg" width="15%" height="15%" alt="I just can!"><br></div><h2 id="0-Motivation"><a href="#0-Motivation" class="headerlink" title="0 Motivation"></a>0 Motivation</h2><p>Starting from this post, I decide to keep a record (tag: MineSweeping) of the issues I meet while working with environments and also their solutions. </p><p>Doing configurations in order to run others&#39; code may be a difficult task, and is sometimes depressing, since various issues could arise, and the it&#39;s impossible for the authors to keep providing solutions for every user in the community. What&#39;s worse, after fixing some problems with a lot of struggle, one may have to waste the same amount of time on the same issue the next time he/she run it again. That&#39;s why I decide to keep this record: to avoid wasting time twice, while also helping others deal with problems if possible.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Update in October 2019:&lt;/p&gt;
&lt;p&gt;DensePose has been re-implemented with the brand-new object detection framework &lt;a href=&quot;https://github.com/facebookresearch/detectron2&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Detectron2&lt;/a&gt;, which is based on PyTorch and much easier to install and use (You don&amp;#39;t have to manually compile Caffe2)&lt;br&gt;I strongly recommend that you check out the new official DensePose code at &lt;a href=&quot;https://github.com/facebookresearch/detectron2/tree/master/projects/DensePose&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/facebookresearch/detectron2/tree/master/projects/DensePose&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;DensePose is a great work in real-time human pose estimation, which is based on Caffe2 and Detectron framework. It extracts dense human body 3D surface based on RGB images. The installation instructions are provided &lt;a href=&quot;https://github.com/facebookresearch/DensePose/blob/master/INSTALL.md&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;During my installation process, these are the problems that took me some time to tackle. I spent on week to finally figure out solutions to all the issues. So lucky of me not to give up too early...&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;&lt;br&gt;    &lt;img src=&quot;/images/densepose-ms/facebook.jpg&quot; width=&quot;15%&quot; height=&quot;15%&quot; alt=&quot;Greetings from Facebook AI Research&quot;&gt;&lt;br&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://linkinpark213.com/tags/Deep-Learning/"/>
    
      <category term="MineSweeping" scheme="http://linkinpark213.com/tags/MineSweeping/"/>
    
  </entry>
  
  <entry>
    <title>Hello New World!</title>
    <link href="http://linkinpark213.com/2018/07/26/hello-osaka/"/>
    <id>http://linkinpark213.com/2018/07/26/hello-osaka/</id>
    <published>2018-07-26T01:12:52.000Z</published>
    <updated>2018-11-18T05:44:45.857Z</updated>
    
    <content type="html"><![CDATA[<p><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.min.css"></p><p><link rel="stylesheet" type="text/css" href="/css/images//photo-waterfall.css"></p><script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><script src="https://unpkg.com/minigrid@3.1.1/dist/minigrid.min.js"></script><script src="/js/jquery.fancybox.min.js"></script><script src="/js/photo-waterfall.js"></script><p>Here are some photos that I took during my trip to Higashi-Osaka, Nara and Kyoto.</p><a id="more"></a><div class="ImageGrid"></div><script>photo_waterfall.init('/images/hello-osaka/');</script><script src="/js/photo-waterfall-carousel.js"></script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;/css/jquery.fancybox.min.css&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;/css/images//photo-waterfall.css&quot;&gt;&lt;/p&gt;
&lt;script src=&quot;https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;https://unpkg.com/minigrid@3.1.1/dist/minigrid.min.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;/js/jquery.fancybox.min.js&quot;&gt;&lt;/script&gt;
&lt;script src=&quot;/js/photo-waterfall.js&quot;&gt;&lt;/script&gt;


&lt;p&gt;Here are some photos that I took during my trip to Higashi-Osaka, Nara and Kyoto.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Travel Gallery" scheme="http://linkinpark213.com/tags/Travel-Gallery/"/>
    
  </entry>
  
  <entry>
    <title>[PyTorch is better!] A Painless Tensorflow Basic Tutorial - Take ResNet-56 as an Example</title>
    <link href="http://linkinpark213.com/2018/04/29/diy-resnet/"/>
    <id>http://linkinpark213.com/2018/04/29/diy-resnet/</id>
    <published>2018-04-29T13:05:43.000Z</published>
    <updated>2019-03-17T06:27:21.325Z</updated>
    
    <content type="html"><![CDATA[<p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script></p><div lang="en-us"><br>Update in March 2019:<br><br>After TensorFlow developers introduced the APIs of Tensorflow 2.0 on Tensorflow Dev Summit 2019, I have made my decision to turn to PyTorch.<br><div align="center" class="figure"><br><img src="/images/tftutorial/wjz_en.gif" alt="çœŸé¦™ï¼"><br></div><br></div><a id="more"></a><div lang="en-us"><br>TensorFlow is a powerful open-source deep learning framework, supporting various languages including Python. However, its APIs are far too complicated for a beginner in deep learning(especially those who are new to Python). In order to ease the pain of having to understand the mess of various elements in TensorFlow computation graphs, I made this tutorial to help beginners take the first bite of the cake.<br><br>ResNets are one of the greatest works in the deep learning field. Although they look scary with extreme depths, it&#39;s not a hard job to implement one. Now let&#39;s build one of the simplest ResNets - ResNet-56, and train it on the CIFAR-10 dataset.<br><br></div><br><div lang="zh-cn"><br>2019.3 æ›´æ–°:<br><br>Tensorflow Dev Summitä¸Šå¼€å‘è€…ä»‹ç»TF 2.0 APIåï¼Œ æˆ‘å½»åº•ä¸‹å®šäº†æ¢ç”¨PyTorchçš„å†³å¿ƒã€‚<br><div align="center" class="figure"><br><img src="/images/tftutorial/wjz.gif" alt="çœŸé¦™ï¼"><br></div><br></div><br><div lang="zh-cn"><br>TensorFlowæ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€æºæ·±åº¦å­¦ä¹ è½¯ä»¶åº“ï¼Œå®ƒæ”¯æŒåŒ…æ‹¬Pythonåœ¨å†…çš„å¤šç§è¯­è¨€ã€‚ç„¶è€Œï¼Œç”±äºAPIè¿‡äºå¤æ‚ï¼ˆå®é™…ä¸Šè¿˜æœ‰ç‚¹æ··ä¹±ï¼‰ï¼Œå®ƒå¾€å¾€ä½¿å¾—ä¸€ä¸ªæ·±åº¦å­¦ä¹ çš„åˆå­¦è€…ï¼ˆå°¤å…¶æ˜¯ä¸ºæ­¤åˆå­¦Pythonçš„é‚£äº›ï¼‰æœ›è€Œå´æ­¥â€”â€”è€è™åƒå¤©ï¼Œæ— ä»ä¸‹å£ã€‚ä¸ºäº†å‡è½»åˆå­¦è€…ä¸å¾—ä¸å°è¯•ç†è§£TensorFlowä¸­çš„å¤§é‡æ¦‚å¿µçš„ç—›è‹¦ï¼Œæˆ‘213ä»Šå¤©å¸¦å„ä½å°å°æ·±åº¦å­¦ä¹ è¿™ç‰‡å¤©çš„ç¬¬ä¸€å£ã€‚<br>ResNetæ˜¯æ·±åº¦å­¦ä¹ é¢†åŸŸçš„ä¸€ä¸ªé‡ç£…ç‚¸å¼¹ï¼Œå°½ç®¡å®ƒä»¬ï¼ˆResNetæœ‰ä¸åŒå±‚æ•°çš„å¤šä¸ªæ¨¡å‹ï¼‰çš„æ·±åº¦çœ‹ä¸Šå»æœ‰ç‚¹å“äººï¼Œä½†å®é™…ä¸Šå®ç°ä¸€ä¸ªResNetå¹¶ä¸éš¾ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥å®ç°ä¸€ä¸ªè¾ƒä¸ºç®€å•çš„ResNetâ€”â€”ResNet-56ï¼Œå¹¶åœ¨CIFAR-10æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸‹ï¼Œçœ‹çœ‹æ•ˆæœå¦‚ä½•ã€‚<br></div><br><div align="center" class="figure"><br><img src="/images/tftutorial/oyo.gif" alt="Let&#39;s Rock!"><br></div><br><div lang="en-us"><br>First let&#39;s take a look at ResNet-56. It&#39;s proposed by Kaiming He et al., and is designed to confirm the effect of residual networks. It has 56 weighted layers, deep but simple. The structure is shown in the figure below:<br></div><br><div lang="zh-cn"><br>é¦–å…ˆæ¥çœ‹ä¸€ä¸‹ResNet-56è¿™ä¸ªç¥ç»ç½‘ç»œã€‚å®ƒæ˜¯ä½•å‡¯æ˜ç­‰åœ¨ResNetè®ºæ–‡ä¸­æå‡ºçš„ã€ç”¨äºéªŒè¯æ®‹å·®ç½‘ç»œæ•ˆæœçš„ä¸€ä¸ªç›¸å¯¹ç®€å•çš„æ®‹å·®ç½‘ç»œï¼ˆå°½ç®¡å®ƒå¾ˆæ·±ï¼Œæ·±åº¦è¾¾åˆ°äº†56ä¸ªæƒé‡å±‚ï¼‰ã€‚å›¾ç¤ºå¦‚ä¸‹ï¼š<br></div><br><div align="center" class="figure"><br><img src="/images/tftutorial/resnet56.png" alt="ResNet-56" width="80%"><br><br>Fig. 1 The structure of ResNet-56<br></div><br><br><br><div lang="en-us"><br>Seems a little bit long? Don&#39;t worry, let&#39;s do this step by step.<br></div><br><div lang="zh-cn"><br>çœ‹èµ·æ¥æœ‰ç‚¹é•¿äº†æ˜¯ä¸æ˜¯ï¼Ÿåˆ«æ‹…å¿ƒï¼Œæˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ¥åšã€‚<br></div><h2 id="1-Ingredients"><a href="#1-Ingredients" class="headerlink" title="1 Ingredients"></a>1 Ingredients</h2><p>Python 3.6</p><p>TensorFlow 1.4.0</p><p>Numpy 1.13.3</p><p>OpenCV 3.2.0</p><p><a href="https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz" target="_blank" rel="noopener">CIFAR-10 Dataset</a></p><div lang="en-us"><br>Also prepare some basic knowledge on Python programming, digital image processing and convolutional neural networks. If you are already capable of building, training and validating your own neural networks with TensorFlow, you don&#39;t have to read this post.<br></div><br><div lang="zh-cn"><br>å¦å¤–ï¼Œè¯·ç¡®ä¿è‡ªå·±æœ‰ä¸€ç‚¹ç‚¹Pythonç¼–ç¨‹ã€æ•°å­—å›¾åƒå¤„ç†å’Œå·ç§¯ç¥ç»ç½‘ç»œçš„çŸ¥è¯†å‚¨å¤‡ã€‚å¦‚æœä½ å·²ç»å…·å¤‡ç”¨TensorFlowè‡ªè¡Œæ­å»ºç¥ç»ç½‘ç»œå¹¶è¿›è¡Œè®­ç»ƒã€æµ‹è¯•çš„èƒ½åŠ›ï¼Œå°±ä¸å¿…é˜…è¯»æœ¬æ–‡äº†ã€‚<br></div><h2 id="2-Recipe"><a href="#2-Recipe" class="headerlink" title="2 Recipe"></a>2 Recipe</h2><h3 id="2-0-Prepare-the-tools"><a href="#2-0-Prepare-the-tools" class="headerlink" title="2.0 Prepare the tools"></a>2.0 Prepare the tools</h3><div lang="en-us"><br>Prepare(import) the tools for our project, including all that I mentioned above. Like this :P<br></div><br><div lang="zh-cn"><br>å‡†(i)å¤‡(m)æ‰€(p)éœ€(o)å·¥(r)å…·(t)ï¼Œä¸Šä¸€éƒ¨åˆ†å·²æåˆ°è¿‡ã€‚å¦‚ä¸‹ï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> tensor_chain <span class="keyword">import</span> TensorChain</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>Wait... What&#39;s this? TensorChain? Another deep learning framework like TensorFlow?<br><br>Uh, nope. This is my own encapsulation of some TensorFlow APIs, for the sake of easing your pain. You&#39;ll only have to focus on &quot;what&#39;s what&quot; in the beginning. We&#39;ll look into my implementation of this encapsulation later, when you are clear how everything goes. Please <a href="/files/tensor_chain.py">download this file</a> and put it where your code file is, and import it.<br></div><br><div lang="zh-cn"><br>ç­‰ç­‰...æœ€åè¿™ä¸ªæ˜¯ä¸ªä»€ä¹ˆé¬¼ï¼Ÿ TensorChainï¼Ÿå¦ä¸€ä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶å—ï¼Ÿ<br><br>å‘ƒ...å¹¶ä¸æ˜¯ã€‚è¿™ä¸ªæ˜¯æˆ‘å¯¹ä¸€äº›TensorFlow APIçš„å°è£…ï¼Œä¸ºäº†å‡è½»ä½ çš„ç—›è‹¦æ‰åšçš„ã€‚ä½œä¸ºåˆå­¦è€…ï¼Œä½ åªéœ€è¦å…³æ³¨ç”¨TensorFlowæ­å»ºç½‘ç»œæ¨¡å‹çš„è¿™ä¸ªè¿‡ç¨‹ï¼Œåˆ†æ¸…ä¸œè¥¿å—åŒ—ã€‚å›å¤´ç­‰ä½ å¼„æ¸…äº†å¤§ä½“æµç¨‹åï¼Œæˆ‘ä»¬å†æ¥çœ‹è¿™ä¸ªçš„å®ç°ç»†èŠ‚ã€‚è¯·å…ˆä¸‹è½½<a href="/files/tensor_chain.py">è¿™ä¸ªæ–‡ä»¶</a>å¹¶æŠŠå®ƒä¸ä½ çš„ä»£ç æ”¾åœ¨åŒä¸€æ–‡ä»¶å¤¹ä¸‹ï¼Œç„¶åå°±å¯ä»¥importäº†ã€‚<br></div><h3 id="2-1-Decide-the-input"><a href="#2-1-Decide-the-input" class="headerlink" title="2.1 Decide the input"></a>2.1 Decide the input</h3><div lang="en-us"><br>Every neural network requires an input - you always have to identify the details of a question, before asking the computer to solve it. All of the variable, constant in TensorFlow are objects of type <em>tf.Tensors</em>. And the <em>tf.placeholder</em> of our input(s) is a special one. Images in CIFAR-10 dataset are RGB images(3 channels) of 32x32(really small), so our input should shaped like [32, 32, 3]. Also, we want to input a little <em>batch</em> of multiple images. Therefore, our input data should be an array of shape <em>[?, 32, 32, 3]</em>. Unknown dimension size can be marked as None, and it will be clear when we feed the model with the actual images. It&#39;s coded like this:<br></div><br><div lang="zh-cn"><br>æ¯ä¸ªç¥ç»ç½‘ç»œéƒ½éœ€è¦æœ‰è¾“å…¥â€”â€”æ¯•ç«Ÿä½ æƒ³æ‰¾ç”µè„‘è§£å†³ä¸€äº›é—®é¢˜çš„è¯ï¼Œä½ æ€»å¾—å‘Šè¯‰å®ƒé—®é¢˜çš„ä¸€äº›ç»†èŠ‚å§ï¼ŸTensorFlowä¸­æ‰€æœ‰çš„å˜é‡ã€å¸¸é‡éƒ½æ˜¯<em>tf.Tensor</em>ç±»å‹çš„å¯¹è±¡ï¼Œä½œä¸ºè¾“å…¥å†…å®¹çš„å ä½ç¬¦<em>tf.placeholder</em>ä¹Ÿæ˜¯ï¼ˆåªä¸è¿‡æ¯”è¾ƒç‰¹æ®Šè€Œå·²ï¼‰ã€‚CIFAR-10æ•°æ®é›†çš„å›¾åƒéƒ½æ˜¯32x32å°ºå¯¸ï¼ˆå¥½å°å“‡ï¼‰çš„RGBå›¾åƒï¼ˆRGBå³å½©è‰²å›¾åƒçš„ä¸‰ä¸ªé€šé“ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬çš„è¾“å…¥ç»™ç¥ç»ç½‘ç»œçš„å†…å®¹å°†ä¼šåƒæ˜¯[32, 32, 3]è¿™ä¸ªæ ·å­ã€‚å¦å¤–å‘¢ï¼Œæˆ‘ä»¬éœ€è¦è¾“å…¥çš„æ˜¯ä¸€ä¸ªå°<em>batch</em>ï¼ˆæ‰¹ï¼‰çš„å›¾åƒï¼Œå› æ­¤ï¼Œè¾“å…¥ç½‘ç»œçš„å›¾åƒæ•°æ®å°†ä¼šæ˜¯ä¸€ä¸ª<em>[?, 32, 32, 3]</em>çš„æ•°ç»„ï¼ˆä¹Ÿå¯ä»¥æ˜¯numpyæ•°ç»„ï¼‰ã€‚æœªçŸ¥çš„ç»´åº¦å¤§å°ç”¨Noneä»£æŒ‡å°±å¥½ï¼Œæˆ‘ä»¬ä¹‹åç»™æ¨¡å‹å–‚å®é™…å›¾åƒbatchæ—¶ï¼Œå®ƒè‡ªç„¶å°±æ¸…æ¥šäº†ã€‚ä»£ç å¦‚ä¸‹ï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input_tensor = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br><em>Ground truth</em> data also need to be known in supervised learning, so we also have to define a placeholder for the ground truth data:<br></div><br><div lang="zh-cn"><br>ç›‘ç£å­¦ä¹ ä¸­ï¼Œæ­£ç¡®æ ‡æ³¨çš„æ•°æ®ï¼ˆè‹±æ–‡ä¸º<em>ground truth</em>ï¼Œç›®å‰è²Œä¼¼æ²¡æœ‰å¯¹è¿™ä¸ªåè¯çš„åˆç†ç¿»è¯‘ï¼‰ä¹Ÿæ˜¯éœ€è¦è¾“å…¥åˆ°æ¨¡å‹ä¸­çš„ã€‚å› æ­¤å†ç»™ground truthå®šä¹‰ä¸€ä¸ªplaceholderï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ground_truth = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>We want the label data to be in the one-hot encoding format, which means an array of length 10, denoting 10 classes. Only on one position is a &#39;1&#39;, and on other positions are &#39;0&#39;s.<br></div><br><div lang="zh-cn"><br>æˆ‘ä»¬éœ€è¦æ ‡è®°çš„æ•°æ®å‘ˆOne-Hotç¼–ç æ ¼å¼ï¼ˆåˆç§°ä¸ºä¸€ä½æœ‰æ•ˆç¼–ç ï¼‰ï¼Œæ„æ€æ˜¯å¦‚æœæœ‰10ä¸ªç±»åˆ«ï¼Œé‚£ä¹ˆæ•°ç»„é•¿åº¦å°±æ˜¯10ï¼Œæ¯ä¸€ä½ä»£è¡¨ä¸€ä¸ªç±»åˆ«ã€‚åªæœ‰ä¸€ä¸ªä½ç½®ä¸Šæ˜¯1ï¼ˆä»£è¡¨å›¾ç‰‡è¢«åˆ†ä¸ºè¿™ä¸ªç±»ï¼‰ï¼Œå…¶ä»–ä½ä¸Šéƒ½æ˜¯0ã€‚<br></div><h3 id="2-2-Do-some-operations"><a href="#2-2-Do-some-operations" class="headerlink" title="2.2 Do some operations"></a>2.2 Do some operations</h3><div lang="en-us"><br>For now, let&#39;s use our TensorChain to build it fast. Under most circumstances that we may face, the computations are based on the input data or the result of the former computation, so our network(or say, the most of it) look more like a chain than a web. Every time we add some new operation(layer), we add it to our <em>TensorChain</em> object. Just remember to get the <em>output_tensor</em> of this object(denoting the output tensor of the last operation on the chain) when you need to ue native TensorFlow API.<br>The construction function of TensorChain class requires a Tensor object as the parameter, which is also the input tensor of this chain. As we mentioned earlier, all we have to do is add operations. See my ResNet-56 code:<br></div><br><div lang="zh-cn"><br>ç°åœ¨å‘¢ï¼Œæˆ‘ä»¬å…ˆç”¨TensorChainæ¥å¿«é€Ÿç›–æ¥¼ã€‚å› ä¸ºæˆ‘ä»¬é‡åˆ°çš„å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ‰€æœ‰çš„è®¡ç®—éƒ½æ˜¯åœ¨è¾“å…¥æ•°æ®æˆ–è€…è¿™ä¸ªè®¡ç®—çš„å‰ä¸€ä¸ªè®¡ç®—ç»“æœåŸºç¡€ä¸Šè¿›è¡Œçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„ç½‘ç»œï¼ˆè‡³å°‘æ˜¯å®ƒçš„ç»å¤§éƒ¨åˆ†ï¼‰ä¼šçœ‹èµ·æ¥åƒä¸ªé“¾è€Œä¸æ˜¯æ‰€è°“çš„ç½‘ã€‚æ¯æ¬¡æˆ‘ä»¬æ·»åŠ ä¸€ä¸ªæ–°çš„è¿ç®—ï¼ˆå±‚ï¼‰ï¼Œæˆ‘ä»¬ä¼šæŠŠå®ƒåŠ åˆ°è¿™ä¸ªç‹¬ä¸€æ— äºŒçš„TensorChainå¯¹è±¡ã€‚åªè¦è®°å¾—åœ¨ä½¿ç”¨åŸç”ŸTensorFlow APIå‰æŠŠå®ƒçš„<em>output_tensor</em>å±æ€§ï¼ˆä¹Ÿå°±æ˜¯è¿™æ¡é“¾ä¸Šæœ€åä¸€ä¸ªè¿ç®—çš„è¾“å‡ºTensorï¼‰å–å‡ºæ¥å°±å¥½äº†ã€‚<br>TensorChainç±»çš„æ„é€ å‡½æ•°éœ€è¦ä¸€ä¸ªTensorå¯¹è±¡ä½œä¸ºå‚æ•°ï¼Œè¿™ä¸ªå¯¹è±¡ä¹Ÿæ­£æ˜¯è¢«æ‹¿æ¥ä½œä¸ºè¿™ä¸ªé“¾çš„è¾“å…¥å±‚ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€è¯´çš„ï¼Œåªè¦åœ¨è¿™ä¸ªå¯¹è±¡ä¸Šæ·»åŠ è¿ç®—å³å¯ã€‚å†™ä¸ªResNet-56ï¼Œä»£ç å¾ˆç®€å•ï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">chain = TensorChain(input_tensor) \</span><br><span class="line">        .convolution_layer_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>, stride=<span class="number">2</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>, stride=<span class="number">2</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">        .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">        .flatten() \</span><br><span class="line">        .fully_connected_layer(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>This is it? Right, this is it! Isn&#39;t it cool? Didn&#39;t seem that high, huh? That&#39;s because I encapsulated that huge mess of weights and biases, only leaving a few parameters that decide the structure of the network. Later in this pose we&#39;ll talk about the actual work that these functions do.<br></div><br><div lang="zh-cn"><br>å°±è¿™ï¼Ÿæ²¡é”™å‘€ï¼Œå°±è¿™ï¼ç¨³ä¸ç¨³ï¼Ÿä¼¼ä¹çœ‹èµ·æ¥ä¹Ÿæ²¡56å±‚é‚£ä¹ˆé«˜å‘€ï¼Ÿæ¯•ç«Ÿè¿™äº›å‡½æ•°è¢«æˆ‘å°è£…å¾—å¤ªä¸¥å®äº†ï¼Œåªç•™å‡ºå‡ ä¸ªå†³å®šç½‘ç»œç»“æ„çš„å‡ ä¸ªå‚æ•°ä¾›ä¿®æ”¹ã€‚è¿™ç¯‡åšå®¢åè¾¹å°±ä¼šè®²åˆ°è¿™äº›å‡½æ•°ç©¶ç«Ÿå¹²äº†ç‚¹ä»€ä¹ˆäº‹å„¿ã€‚<br></div><h3 id="2-3-Define-the-loss"><a href="#2-3-Define-the-loss" class="headerlink" title="2.3 Define the loss"></a>2.3 Define the loss</h3><div lang="en-us"><br>In supervised learning, you always have to tell the learning target to the model. To tell the model how to optimize, you have to let it know how, how much, on which direction should it change its parameters. This is done by using a loss function. Therefore, we need to define a loss function for our ResNet-56 model(which we designed for this classification problem) so that it will learn and optimize.<br>A commonly used loss function in classification problems is cross entropy. It&#39;s defined below:<br></div><br><div lang="zh-cn"><br>æç›‘ç£å­¦ä¹ ï¼Œæ€»æ˜¯è¦è®©æ¨¡å‹æŒ‰ç…§â€œå‚è€ƒç­”æ¡ˆâ€å»æ”¹çš„ã€‚è¦æ”¹å°±å¾—è®©å®ƒçŸ¥é“æ€ä¹ˆæ”¹ã€æ”¹å¤šå°‘ã€å¾€ä»€ä¹ˆæ–¹å‘æ”¹ï¼Œè¿™ä¹Ÿå°±æ˜¯<em>loss</em>ï¼ˆæŸå¤±å‡½æ•°ï¼‰çš„åŠŸåŠ³ã€‚å› æ­¤ï¼Œåƒæˆ‘ä»¬è¿™ä¸ªæ‹¿æ¥åšåˆ†ç±»é—®é¢˜çš„ResNet-56ï¼Œæˆ‘ä»¬è¦ç»™å®ƒå®šä¹‰ä¸€ä¸ªæŸå¤±å‡½æ•°æ¥è®©å®ƒå­¦ä¹ ã€ä¼˜åŒ–ã€‚<br>åˆ†ç±»é—®é¢˜ä¸Šä¸€ä¸ªå¸¸ç”¨çš„æŸå¤±å‡½æ•°æ˜¯äº¤å‰ç†µã€‚å®šä¹‰å¦‚ä¸‹å¼ï¼š<br></div><br>$$C=-\frac{1}{n}\sum_x{y\ln a+(1-y)\ln(1-a)}$$<br><div lang="en-us"><br>in which \(y\) is the expected(or say correct) output and \(a\) is the actual output.<br>This seems a little bit complicated. But it&#39;s not a hard job to implement, since TensorFlow implemented it already! You can also try and implement it yourself within one line if you want. For now we use the pre-defined cross entropy loss function:<br></div><br><div lang="zh-cn"><br>å…¶ä¸­\(y\)ä¸ºæœŸæœ›è¾“å‡ºï¼ˆæˆ–è€…è¯´å‚è€ƒç­”æ¡ˆï¼‰ï¼Œ\(a\)ä¸ºå®é™…è¾“å‡ºã€‚<br>ç•¥å¤æ‚å‘€...è¿™ä¸ªç”¨ç¨‹åºæ€ä¹ˆå†™ï¼Ÿå…¶å®ä¹Ÿä¸éš¾ã€‚ã€‚ã€‚æ¯•ç«ŸTensorFlowéƒ½å¸®æˆ‘ä»¬å®ç°å¥½å•¦ï¼ï¼ˆæœ‰å…´è¶£çš„è¯ä¹Ÿå¯ä»¥è‡ªå·±å°è¯•ç€å†™ä¸€ä¸‹ï¼ŒåŒæ ·ä¸€è¡Œä»£ç å³å¯æå®šï¼‰ç°åœ¨ä½ åªéœ€è¦æ¥è¿™ä¹ˆä¸€å¥ï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>and it returns a tf.Tensor that denotes an average of cross entropies(don&#39;t forget that this is a batch). As for the &#39;softmax&#39; before the &#39;cross_entropy&#39;, it&#39;s a function that project the data in an array to range 0~1, which allows us to do a comparison between our prediction and the ground truth(in one-hot code). The definition is simple too:\<br></div><br><div lang="zh-cn"><br>å°±å¯ä»¥åˆ›å»ºä¸€ä¸ªè¡¨ç¤ºäº¤å‰ç†µå¹³å‡å€¼ï¼ˆåˆ«å¿˜äº†è¿™å¯æ˜¯ä¸€ä¸ªbatchï¼‰çš„Tensoräº†ã€‚è‡³äºcross_entropyå‰è¾¹çš„é‚£ä¸ª<em>softmax</em>å‘¢ï¼Œå®ƒçš„ä½œç”¨æ˜¯æŠŠè¾“å…¥çš„æ•°ç»„å†…æ•°æ®å½’ä¸€åŒ–ï¼ŒæŠ•å°„åˆ°0~1çš„èŒƒå›´å†…ï¼ˆå®é™…ä¸Šå°±æ˜¯ç›¸å½“äºæŠŠexp(æ•°ç»„å„é¡¹çš„å€¼)çš„å½“åšé¢‘æ•°ï¼Œæ±‚å‡ºä¸€ä¸ªæ¦‚ç‡ï¼‰ï¼Œè¿™æ ·å­æ‰èƒ½è·Ÿå®é™…æ•°æ®åšä¸€ä¸ªæ¯”è¾ƒã€‚å®šä¹‰ä¹Ÿæ¯”è¾ƒç®€å•ï¼š\<br></div><br>$$S_i=\frac{e^{V_i}}{\sum_j{e^{V_j}}}$$<br><div></div><h3 id="2-4-Define-the-train-op"><a href="#2-4-Define-the-train-op" class="headerlink" title="2.4 Define the train op"></a>2.4 Define the train op</h3><div lang="en-us"><br>Now we have the loss function. We&#39;ll have to tell its value to an <em>optimizer</em>, which make our model learn and optimize in order to minimize the loss value. Gradient Descent Optimizer, Adagrad Optimizer, Adam Optimizer and Momentum Optimizer are commonly used optimizers. Here we use an Adam Optimizer for instance. You&#39;re free to try any other one here. When<br></div><br><div lang="zh-cn"><br>ç°åœ¨è¯¯å·®å‡½æ•°å·²ç»æœ‰äº†ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒçš„å€¼å‘Šè¯‰ä¸€ä¸ªä¼˜åŒ–å™¨ï¼ˆ<em>optimizer</em>ï¼‰ï¼Œå¹¶è®©å®ƒå»å°½å¯èƒ½å‘ç€ç¼©å°è¯¯å·®å‡½æ•°å€¼å¾—æ–¹å‘åŠªåŠ›ã€‚è¿™æ ·ï¼Œæ¨¡å‹æ‰èƒ½å»å­¦ä¹ ã€ä¼˜åŒ–ã€‚å¸¸ç”¨çš„ä¼˜åŒ–å™¨åŒ…æ‹¬Gradient Descent Optimizerï¼ŒAdagrad Optimizerï¼ŒAdam Optimizerä»¥åŠMomentum Optimizerç­‰ç­‰ç­‰ç­‰ã€‚é€‰æ‹©ä¼˜åŒ–å™¨æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ç»™å®ƒä¸€ä¸ªåˆå§‹çš„å­¦ä¹ é€Ÿç‡ã€‚è¿™é‡Œæˆ‘ç”¨äº†ä¸€ä¸ª\(10^-3\)ï¼Œå¦‚æœéœ€è¦æé«˜å‡†ç¡®ç‡ï¼Œå¯èƒ½åæœŸå¾®è°ƒè¿˜éœ€è¦è¿›ä¸€æ­¥å‡å°ã€‚ä»£ç å¦‚ä¸‹ï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>)</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>Also, tell the optimizer that what the loss tensor is. The returned object is a train operation.<br></div><br><div lang="zh-cn"><br>å½“ç„¶è¿˜è¦å‘Šè¯‰å®ƒè¦å‡å°çš„æŸå¤±å‡½æ•°æ˜¯å“ªä¸ªTensorï¼Œè¿™ä¸ªå‡½æ•°è¿”å›çš„æ˜¯ä¸€ä¸ªè®­ç»ƒæ“ä½œï¼ˆ<em>train op</em>ï¼Œä¸€ç§ç‰¹æ®Šçš„è¿ç®—ï¼Œæˆ–è€…è¯´æ“ä½œï¼‰ï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train = optimizer.minimize(loss)</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>The neural network is finished. It&#39;s time to grab some data and train it.<br></div><br><div lang="zh-cn"><br>å…¶å®åˆ°è¿™é‡Œä¸ºæ­¢ï¼Œç¥ç»ç½‘ç»œå·²ç»æ­å»ºå¥½äº†ã€‚æ˜¯æ—¶å€™æç‚¹æ•°æ®æ¥è®­ç»ƒå®ƒäº†ã€‚<br></div><h3 id="2-5-Feed-the-model-with-data-and-train-it"><a href="#2-5-Feed-the-model-with-data-and-train-it" class="headerlink" title="2.5 Feed the model with data, and train it!"></a>2.5 Feed the model with data, and train it!</h3><div lang="en-us"><br>Remember how we defined the placeholders? It&#39;s time to fetch some data that fits the placeholders and train it. See how CIFAR-10 dataset can be fetched on its <a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">website</a>.<br></div><br><div lang="zh-cn"><br>è¿˜è®°å¾—æˆ‘ä»¬æ€ä¹ˆå®šä¹‰é‚£äº›placeholderå—ï¼Ÿç°åœ¨æˆ‘ä»¬è¦æŠŠç¬¦åˆå®ƒä»¬å£å¾„çš„æ•°æ®çŒè¿›æ¨¡å‹ã€‚é‚£ä¹ˆæ¥çœ‹ä¸€ä¸‹CIFAR-10æ•°æ®é›†<a href="https://www.cs.toronto.edu/~kriz/cifar.html" target="_blank" rel="noopener">å®˜ç½‘</a>ä¸Šæ˜¯æ€ä¹ˆæè¿°çš„å§ã€‚å®ƒç»™äº†è¿™ä¹ˆä¸€æ®µä»£ç ï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpickle</span><span class="params">(file)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    <span class="keyword">with</span> open(file, <span class="string">'rb'</span>) <span class="keyword">as</span> fo:</span><br><span class="line">        dict = pickle.load(fo, encoding=<span class="string">'bytes'</span>)</span><br><span class="line">    <span class="keyword">return</span> dict</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>The returned value <em>dict</em> is a Python dictionary. Every time we unpickle a file, a dictionary would be returned. Its &#39;data&#39; key leads to 10000 RGB images of size 32x32, which is stored in a [10000, 3072] array(3072=32<em>32</em>3, I guess you know how it&#39;s stored now). The &#39;label&#39; key leads to 10000 values in range 0~9. Obviously we have to reshape the data so as to fit it into the network model:<br></div><br><div lang="zh-cn"><br>è¿”å›å€¼<em>dict</em>æ˜¯ä¸€ä¸ªå­—å…¸ï¼ˆPythonçš„dictç±»å‹ï¼‰ã€‚æ¯è¯»ä¸€ä¸ªbatchæ–‡ä»¶ï¼ˆæ¯”å¦‚data_batch_1ï¼‰ï¼Œå°±ä¼šè¿”å›è¿™æ ·ä¸€ä¸ªå­—å…¸ï¼Œå®ƒçš„â€œdataâ€é”®å€¼æ˜¯10000å¼ 32x32çš„RGBå›¾åƒï¼ˆæ•°ç»„ç»´æ•°å±…ç„¶æ˜¯[10000, 3072]ï¼Œè€Œ3072=10000x32x32x3ï¼å®é™…ä¸Šå°±æ˜¯ç›´æ¥æŠŠæ‰€æœ‰åƒç´ ã€æ‰€æœ‰é€šé“çš„å€¼ç½—åˆ—åœ¨è¿™é‡Œäº†ï¼‰ï¼›â€œlabelâ€é”®å€¼æ˜¯10000ä¸ª0-9ä¹‹é—´çš„æ•´æ•°ï¼ˆä»£è¡¨ç±»åˆ«ï¼‰ã€‚æ˜¾ç„¶ï¼Œä¸ºäº†è®©æ•°æ®èƒ½å¤ŸæˆåŠŸæ”¾è¿›æ¨¡å‹ï¼Œè¿˜éœ€è¦å¯¹å®ƒè¿›è¡Œä¸€ç‚¹å¤„ç†ï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">batch = unpickle(DATA_PATH + <span class="string">'data_batch_&#123;&#125;'</span>.format(i))  <span class="comment"># 'i' is the loop variable</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the image data</span></span><br><span class="line">image_data = np.reshape(batch[<span class="string">b'data'</span>], (<span class="number">10000</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>), <span class="string">'F'</span>).astype(np.float32)   </span><br><span class="line">image_data = image_data / <span class="number">255</span>                            <span class="comment"># Cast range(0, 255) to range(0, 1)</span></span><br><span class="line">image_data = np.transpose(image_data, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>))      <span class="comment"># Exchange row and column</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Read the label data and convert into one-hot code</span></span><br><span class="line">label_data = batch[<span class="string">b'labels'</span>]</span><br><span class="line">new_label_data = np.zeros((<span class="number">10000</span>, <span class="number">10</span>))                   </span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    new_label_data[j][label_data[j]] = <span class="number">1</span></span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>The details for data processing are not covered here. Try doing step-by-step to see the results.<br>The <em>image_data</em> and <em>new_label_data</em> are contain 10000 pieces of data each. Let&#39;s divide them into 100 small batches(100 elements each, including image and label) and feed it into the model. Do this on all the 5 batch files:<br></div><br><div lang="zh-cn"><br>å¤„ç†çš„ç»†èŠ‚ä¸å†èµ˜è¿°ã€‚ä½ å¯ä»¥å°è¯•ä¸€æ­¥ä¸€æ­¥è¿è¡Œæ¥çœ‹çœ‹æ¯ä¸€æ­¥çš„ç»“æœã€‚<br>è¿™æ ·æˆ‘ä»¬æ‹¿åˆ°çš„<em>image_data</em>å’Œ<em>new_label_data</em>éƒ½æ˜¯é•¿åº¦ä¸º10000çš„å¤§batchï¼Œæˆ‘ä»¬æŠŠå®ƒä»¬å„è‡ªåˆ†æˆ100ä»½ï¼Œæ¯æ¬¡å–100ä¸ªå›¾åƒ+æ ‡è®°æ•°æ®æ¥å¡è¿›æ¨¡å‹ã€‚å¯¹å…¨éƒ¨5ä¸ªå¤§batchæ–‡ä»¶æ¥ä¸€éï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    session.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">100</span>): <span class="comment"># 10000 / BATCH_SIZE</span></span><br><span class="line">        <span class="comment"># Divide them and get one part</span></span><br><span class="line">        image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class="line">        label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Feed the model</span></span><br><span class="line">        session.run(train, feed_dict=&#123;</span><br><span class="line">            input_tensor: image_batch,</span><br><span class="line">            ground_truth: label_batch</span><br><span class="line">        &#125;)</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>A <em>session</em> - created with <em>tf.Session()</em> - is required every time we run a TensorFlow model, no matter when we&#39;re training it or evaluating it. The first time you run a model, you&#39;ll need to run <em>session.run(tf.global_variables_initializer())</em> to initialize the values of the TensorFlow variables defined previously.<br>When running <em>session.run()</em>, you must first decide a TensorFlow operation(or a list of operations) that you need. If its result is dependent on some actual data(which means that some data in one or more placeholders flow to this operation), it&#39;s also required that you feed it the actual data by adding a <em>feed_dict</em> parameter. For example, I&#39;m training this ResNet-56 model, in which a loss will be calculated with my <em>ground_truth</em> and the prediction result that comes from the <em>input_tensor</em>. Therefore, I&#39;ll have to give a value for each placeholder given above(format: &quot;placeholder name: corresponding data&quot;), and fold them in one Python dictionary.<br></div><br><div lang="zh-cn"><br>æ¯æ¬¡è¿è¡Œä¸€ä¸ªTensorFlowæ¨¡å‹ï¼ˆæ— è®ºæ˜¯è®­ç»ƒè¿˜æ˜¯æµ‹è¯•ï¼‰æ—¶ï¼Œéƒ½éœ€è¦é€šè¿‡tf.Session()åˆ›å»ºä¸€ä¸ª<em>session</em>ã€‚ç¬¬ä¸€æ¬¡è¿è¡Œæ¨¡å‹ï¼ˆè€Œä¸æ˜¯è½½å…¥ä¹‹å‰ä¿å­˜çš„æ¨¡å‹ï¼‰æ—¶ï¼Œéœ€è¦ä½¿ç”¨<em>session.run(tf.global_variables_initializer())</em>æ¥åˆå§‹åŒ–ä¹‹å‰å®šä¹‰çš„ä¸€äº›å¯è®­ç»ƒçš„TensorFlowå˜é‡ã€‚<br>è¿è¡Œ<em>session.run()</em>æ—¶ï¼Œéœ€è¦æŒ‡å®šä¸€ä¸ªæˆ–ä¸€ç»„ä½ è¦æ‰§è¡Œçš„operationï¼Œä½œä¸ºè¿™ä¸ªå‡½æ•°å”¯ä¸€ä¸€ä¸ªå¿…è¦çš„å‚æ•°ã€‚å¦‚æœå®ƒçš„ç»“æœä¾èµ–äºä¸€äº›å®é™…æ•°æ®ï¼ˆä¹Ÿå°±æ˜¯è¯´åœ¨è®¡ç®—å›¾ä¸­ï¼Œä¸€äº›æ•°æ®ä¼šä»placeholderæµå‘è¿™ä¸ªoperationï¼‰ï¼Œé‚£ä¹ˆå°±éœ€è¦é€šè¿‡å¡«å…¥<em>feed_dict</em>å‚æ•°çš„å€¼æ¥å¡«è£…è®­ç»ƒæˆ–æµ‹è¯•æ•°æ®ã€‚ä»¥æ­¤æ¨¡å‹ä¸ºä¾‹ï¼Œæˆ‘åœ¨è®­ç»ƒå®ƒæ—¶éœ€è¦ç®—è¯¯å·®å‡½æ•°å€¼ï¼Œè¿™éœ€è¦<em>ground_truth</em>æ•°æ®å’Œé¢„æµ‹ç»“æœæ¥è®¡ç®—ï¼Œè€Œé¢„æµ‹ç»“æœåˆéœ€è¦ç”¨è¾“å…¥å›¾åƒ<em>input_tensor</em>æ¥è®¡ç®—å¾—åˆ°ã€‚å› æ­¤ï¼Œæˆ‘éœ€è¦ç»™è¿™ä¸¤ä¸ªå ä½ç¬¦åˆ†åˆ«ç»™å‡ºå¯¹åº”çš„æ•°æ®ï¼ˆæ ¼å¼ï¼šâ€œå ä½ç¬¦åï¼šå¯¹åº”æ•°æ®â€ï¼‰ï¼Œå¹¶æŠŠå®ƒä»¬å°åœ¨åŒä¸€ä¸ªPythonå­—å…¸ä¸­ä½œä¸ºfeed_dictå‚æ•°çš„å€¼ã€‚<br></div><br><div lang="en-us"><br>I&#39;m also interested in the loss function value in each iteration(which means feeding a batch of data and executing one forward-propagation and one back-propagation) in the training process. Therefore, what I&#39;ll fill in the parameter is not just the train op, but also the loss tensor. And the session.run() above should be modified to:<br></div><br><div lang="zh-cn"><br>ç„¶è€Œå‘¢ï¼Œæˆ‘è¿˜æƒ³çœ‹çœ‹æ¯æ¬¡è¿­ä»£ï¼ˆå³æŠŠä¸€ä¸ªbatché€è¿›å»ï¼Œæ‰§è¡Œä¸€æ¬¡æ­£å‘ä¼ æ’­ä¸åå‘ä¼ æ’­è¿™ä¸ªè¿‡ç¨‹ï¼‰ä¸­æŸå¤±å‡½æ•°å˜æˆäº†å¤šå¤§ï¼Œæ¥ç›‘æ§ä¸€ä¸‹è®­ç»ƒçš„æ•ˆæœã€‚è¿™æ ·ï¼Œéœ€è¦session.run()çš„å°±ä¸ä»…æ˜¯é‚£ä¸ªtrainè¿ç®—ï¼Œè¿˜è¦åŠ ä¸Šlossè¿ç®—ã€‚å°†ä¸Šè¾¹çš„session.run()éƒ¨åˆ†æ”¹ä¸ºï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[train_, loss_value] = session.run([train, loss],</span><br><span class="line">    feed_dict=&#123;</span><br><span class="line">        input_tensor: image_batch,</span><br><span class="line">        ground_truth: label_batch</span><br><span class="line">    &#125;)</span><br><span class="line">print(<span class="string">"Loss = &#123;&#125;"</span>.format(loss_value)</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>This is when the return value of session.run() becomes useful. Its value(s) - corresponding to the first parameter of run() - is/are the actual value(s) of the tensor(s) in the first parameter. In our example, <em>loss_value</em> is the actual output of the loss tensor. As for train_, we don&#39;t care what it is. Just add it to match the dimensions.<br></div><br><div lang="zh-cn"><br>è¿™æ—¶å€™ï¼Œsession.run()å‡½æ•°çš„è¿”å›å€¼å°±æœ‰æ„ä¹‰äº†ã€‚å®ƒä¸ç¬¬ä¸€ä¸ªå‚æ•°çš„å†…å®¹ä¸€ä¸€å¯¹åº”ï¼Œåˆ†åˆ«æ˜¯è¯¥å‚æ•°ä¸­å„ä¸ªoperationçš„å®é™…è¾“å‡ºå€¼ã€‚åƒè¿™ä¸ªä¾‹å­é‡Œè¾¹ï¼Œ<em>loss_value</em>æ¥æ”¶çš„å°±æ˜¯lossè¿ç®—çš„è¾“å‡ºå†…å®¹ã€‚è€Œtrainè¿ç®—çš„è¾“å‡ºæˆ‘ä»¬å¹¶ä¸å…³å¿ƒï¼Œä½†æ˜¯ä¸ºäº†ä¿è¯å‚æ•°ç»´åº¦æ•°ä¸è¿”å›å€¼ä¸€è‡´ï¼Œç”¨ä¸€ä¸ªtrain_å˜é‡æ¥æ¥æ”¶è€Œå·²ï¼ˆå®é™…ä¸Šå®ƒçš„å€¼æ˜¯Noneï¼‰ã€‚<br></div><br><div lang="en-us"><br>Actually, one epoch(train the model once with the whole dataset) is not enough for the model to fully optimize. I trained this model for 40 epochs and added some loop variables to display the result. You can see my code and my output below. It&#39;s highly recommended that you train this with a high-performance GPU, or it would be a century before you train your model to a satisfactory degree.<br></div><br><div lang="zh-cn"><br>å®é™…ä¸Šï¼Œä¸€ä¸ªepochï¼ˆæŠŠæ•´ä¸ªæ•°æ®é›†éƒ½åœ¨æ¨¡å‹é‡Œè¿‡ä¸€éçš„å‘¨æœŸï¼‰å¹¶ä¸è¶³ä»¥è®©æ¨¡å‹å……åˆ†å­¦ä¹ ã€‚æˆ‘æŠŠè¿™ä¸ªæ¨¡å‹è®­ç»ƒäº†40ä¸ªepochå¹¶ä¸”åŠ äº†ä¸€äº›å¾ªç¯å˜é‡æ¥è¾“å‡ºç»“æœã€‚æˆ‘çš„ä»£ç å’Œç»“æœå¦‚ä¸‹ã€‚å¼ºçƒˆå»ºè®®ç”¨ä¸€ä¸ªé«˜æ€§èƒ½GPUè®­ç»ƒï¼ˆå¦‚æœæ‰‹å¤´æ²¡æœ‰ï¼Œå¯ä»¥ç§Ÿä¸€ä¸ªGPUæœåŠ¡å™¨ï¼‰ï¼Œä¸ç„¶ç­‰åˆ«äººæŠŠæ¯•è®¾è®ºæ–‡é€—å†™å®Œçš„æ—¶å€™ï¼Œä½ è¿˜åœ¨è®­ç»ƒå°±å¾ˆå°´å°¬äº†ã€‚<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> tensor_chain <span class="keyword">import</span> TensorChain</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpickle</span><span class="params">(file)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(file, <span class="string">'rb'</span>) <span class="keyword">as</span> fo:</span><br><span class="line">        dict = pickle.load(fo, encoding=<span class="string">'bytes'</span>)</span><br><span class="line">    <span class="keyword">return</span> dict</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    input_tensor = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>])</span><br><span class="line">    ground_truth = tf.placeholder(dtype=tf.float32, shape=[<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">    chain = TensorChain(input_tensor) \</span><br><span class="line">            .convolution_layer_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">16</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>, stride=<span class="number">2</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">32</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>, stride=<span class="number">2</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">            .residual_block_2d(<span class="number">3</span>, <span class="number">64</span>) \</span><br><span class="line">            .flatten() \</span><br><span class="line">            .fully_connected_layer(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    prediction = chain.output_tensor</span><br><span class="line">    loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(ground_truth, prediction))</span><br><span class="line"></span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">    train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">        session.run(tf.global_variables_initializer())</span><br><span class="line">        iteration = <span class="number">1</span></span><br><span class="line">        BATCH_SIZE = <span class="number">100</span></span><br><span class="line">        DATA_PATH = <span class="string">'../data/cifar-10-batches-py/'</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">41</span>):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">                data = unpickle(DATA_PATH + <span class="string">'data_batch_&#123;&#125;'</span>.format(i))</span><br><span class="line">                image_data = np.reshape(data[<span class="string">b'data'</span>], (<span class="number">10000</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>), <span class="string">'F'</span>).astype(np.float32)</span><br><span class="line">                image_data = image_data / <span class="number">255</span></span><br><span class="line">                image_data = np.transpose(image_data, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>))</span><br><span class="line">                label_data = data[<span class="string">b'labels'</span>]</span><br><span class="line">                new_label_data = np.zeros((<span class="number">10000</span>, <span class="number">10</span>))</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">                    new_label_data[j][label_data[j]] = <span class="number">1</span></span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(int(<span class="number">10000</span> / BATCH_SIZE)):</span><br><span class="line">                    image_batch = image_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class="line">                    label_batch = new_label_data[j * BATCH_SIZE: j * BATCH_SIZE + BATCH_SIZE]</span><br><span class="line">                    [train_, loss_] = session.run(</span><br><span class="line">                        [train, loss],</span><br><span class="line">                        feed_dict=&#123;</span><br><span class="line">                            input_tensor: image_batch,</span><br><span class="line">                            ground_truth: label_batch</span><br><span class="line">                        &#125;)</span><br><span class="line">                    iteration += <span class="number">1</span></span><br><span class="line">                    print(<span class="string">"Epoch &#123;&#125;, Iteration &#123;&#125;, Loss = &#123;&#125;"</span>.format(epoch, iteration, loss_))</span><br></pre></td></tr></table></figure><br><br><div align="center" class="figure"><br><img src="/images/tftutorial/train.png" alt="Training result" width="40%"><br><br>Fig. 2 Training result: cross entropy has dropped below 0.5<br></div><br><div></div><h3 id="2-6-Conclusion"><a href="#2-6-Conclusion" class="headerlink" title="2.6 Conclusion"></a>2.6 Conclusion</h3><div lang="en-us"><br>In a word, building &amp; training neural network models with TensorFlow involves the following steps:<br><br>1. Decide the <em>input tensor</em><br><br>2. Add operations(<em>op</em>s) based on existing tensors<br><br>3. Define the <em>loss</em> tensor, just like other tensors<br><br>4. Select an <em>optimizer</em> and define the <em>train</em> op<br><br>5. Process <em>data</em> and feed the model with them<br></div><br><div lang="zh-cn"><br>æ€»è€Œè¨€ä¹‹ï¼Œç”¨TensorFlowå»ºç«‹ã€è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹åˆ†ä»¥ä¸‹å‡ æ­¥ï¼š<br><br>1. å®šä¹‰<em>è¾“å…¥</em>Tensor<br><br>2. åœ¨å·²æœ‰çš„Tensorä¸Šæ·»åŠ è¿ç®—ï¼ˆ<em>op</em>ï¼‰<br><br>3. åƒä¹‹å‰æ·»åŠ çš„é‚£äº›è¿ç®—ä¸€æ ·ï¼Œå®šä¹‰<em>æŸå¤±</em>Tensor<br><br>4. é€‰æ‹©ä¸€ä¸ª<em>ä¼˜åŒ–å™¨</em>å¹¶å®šä¹‰<em>è®­ç»ƒ</em>æ“ä½œ<br><br>5. æŠŠ<em>æ•°æ®</em>å¤„ç†ä¸ºåˆé€‚çš„shapeï¼Œå¹¶å–‚è¿›æ¨¡å‹è®­ç»ƒ<br></div><h2 id="3-A-Closer-Look"><a href="#3-A-Closer-Look" class="headerlink" title="3 A Closer Look"></a>3 A Closer Look</h2><div lang="en-us"><br>Wait, it&#39;s too late to leave now!<br>TensorChain saved you from having to deal with a mess of TensorFlow classes and functions. Now it&#39;s time that we take a closer look at how TensorChain is implemented, thus understanding the native TensorFlow APIs.<br></div><br><div lang="zh-cn"><br>åˆ«èµ°å‘¢å–‚ï¼<br>TensorChainè®©ä½ ä¸è‡³äºé¢å¯¹TensorFlowä¸­ä¹±ç³Ÿç³Ÿçš„ç±»å‹å’Œå‡½æ•°è€Œä¸çŸ¥æ‰€æªè¢«æ°´æ·¹æ²¡ã€‚ç°åœ¨æ˜¯æ—¶å€™è¿‘è·ç¦»è§‚å¯Ÿä¸€ä¸‹TensorChainæ˜¯å¦‚ä½•å®ç°çš„ï¼Œä»¥ä¾¿ç†è§£TensorFlowAPIäº†ã€‚<br></div><h3 id="3-1-TensorFlow-variables"><a href="#3-1-TensorFlow-variables" class="headerlink" title="3.1 TensorFlow variables"></a>3.1 TensorFlow variables</h3><div lang="en-us"><br>Let&#39;s begin with TensorFlow variables. Variables in TensorFlow are similar to variables in C, Java or any other strong typed programming languages - they have a type, though not necessarily explicitly decided upon definition. Usually them will change as the training process goes on, getting close to a best value.<br>The most commonly used variables in TensorFlow are weights and biases. I guess that you have seen formulae like:<br></div><br><div lang="zh-cn"><br>å…ˆè¯´TensorFlowçš„å˜é‡ã€‚TensorFlowçš„å˜é‡å’ŒCï¼ŒJavaä»¥åŠå…¶ä»–å¼ºç±»å‹è¯­è¨€ç±»ä¼¼â€”â€”éƒ½æœ‰ä¸€ä¸ªç±»å‹ï¼Œå°½ç®¡ä¸ä¸€å®šåœ¨å®ƒçš„å®šä¹‰æ—¶å°±æ˜¾å¼åœ°å£°æ˜ã€‚é€šå¸¸å®ƒä»¬ä¼šéšç€è®­ç»ƒçš„è¿›è¡Œè€Œä¸æ–­å˜åŒ–ï¼Œè¾¾åˆ°ä¸€ä¸ªæœ€ä½³çš„å€¼é™„è¿‘ã€‚<br>TensorFlowä¸­æœ€å¸¸ç”¨çš„å˜é‡å°±æ˜¯weightså’Œbiasesï¼ˆæƒé‡å’Œåç½®ï¼‰ã€‚æƒ³å¿…ä½ åº”è¯¥è§è¿‡è¿™æ ·çš„å¼å­å§ï¼š<br></div><br>$$y=Wx+b$$<br><div lang="en-us"><br>The \(W\) here is the weight, and the \(b\) here is the bias. When implementing some common network layers, they two are always used as the parameters in the layers. For instance, at the very beginning of our ResNet-56, we had a 3x3 sized convolution layer with 16 channels. Its implementation in TensorChain is:<br></div><br><div lang="zh-cn"><br>è¿™é‡Œ\(W\)å°±æ˜¯æƒé‡ï¼Œ\(b\)å°±æ˜¯åç½®ã€‚åœ¨å®šä¹‰ä¸€äº›å¸¸ç”¨çš„å±‚æ—¶ï¼Œæˆ‘ä»¬å¾€å¾€ä¹Ÿæ˜¯ç”¨è¿™ä¸¤ä¸ªå˜é‡ä½œä¸ºè¿™äº›å±‚ä¸­çš„å‚æ•°ã€‚æ¯”å¦‚è¯´ï¼Œåœ¨æˆ‘ä»¬ResNet-56æœ€å¼€å§‹ï¼Œæˆ‘ä»¬ç”¨åˆ°äº†ä¸€ä¸ª3x3å¤§å°ã€16ä¸ªé€šé“çš„å·ç§¯å±‚ï¼ŒTensorChainä¸­ï¼Œå®ƒçš„å®ç°å¦‚ä¸‹ï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolution_layer_2d</span><span class="params">(self, filter_size: int, num_channels: int, stride: int = <span class="number">1</span>, name: str = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                         disable_log: bool = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Add a 2D convolution layer</span></span><br><span class="line"><span class="string">    :param filter_size: Filter size(width and height) for this operation</span></span><br><span class="line"><span class="string">    :param num_channels: Channel number of this filter</span></span><br><span class="line"><span class="string">    :param stride: Stride for this convolution operation</span></span><br><span class="line"><span class="string">    :param name: The name of the tensor</span></span><br><span class="line"><span class="string">    :param disable_log: Set it True if you don't want this layer to be recorded</span></span><br><span class="line"><span class="string">    :return: This object itself</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    filter = self._weights([filter_size, filter_size, self.num_channels, num_channels], layer_name=name,</span><br><span class="line">                           suffix=<span class="string">'filter'</span>)</span><br><span class="line">    bias = self._bias([num_channels], layer_name=name)</span><br><span class="line">    self.num_channels = num_channels</span><br><span class="line">    self.output_tensor = tf.nn.conv2d(self.output_tensor, filter,</span><br><span class="line">                                      [<span class="number">1</span>, stride, stride, <span class="number">1</span>], <span class="string">'SAME'</span>, name=name)</span><br><span class="line">    self.output_tensor = tf.add(self.output_tensor, bias)</span><br><span class="line">    self._log_layer(</span><br><span class="line">        <span class="string">'2D Convolution layer, filter size = &#123;&#125;x&#123;&#125;, stride = &#123;&#125;, &#123;&#125; channels'</span>.format(filter_size, filter_size,</span><br><span class="line">                                                                                     stride,</span><br><span class="line">                                                                                     num_channels),</span><br><span class="line">        disable=disable_log)</span><br><span class="line">    <span class="keyword">return</span> self</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>See? On line 16, we used a <em>tf.nn.conv2d()</em> function, the parameters of which are <em>input</em>, <em>filter</em>, <em>strides</em>, <em>padding</em>, etc. As can be guessed from the names, this function does a convolution operation with out input and the weights(the convolution <em>filter</em> here). A <em>bias</em> is added to the result as the final output. There are also many people who argue that the bias here is meaningless and should removed. One line of code is sufficient for defining a variable:<br></div><br><div lang="zh-cn"><br>çœ‹è§äº†å§ï¼Ÿ16è¡Œä¸Šï¼Œæˆ‘ä»¬ç”¨äº†ä¸€ä¸ª<em>tf.nn.conv2d()</em>å‡½æ•°ï¼Œå®ƒçš„å‚æ•°æ˜¯<em>input</em>ï¼Œ<em>filter</em>ï¼Œ<em>strides</em>ï¼Œ<em>padding</em>ç­‰ç­‰ã€‚é¡¾åæ€ä¹‰ï¼Œè¿™ä¸ªå‡½æ•°å°±æ˜¯ç”¨æˆ‘ä»¬å®šä¹‰çš„æƒé‡Tensor<em>filter</em>ï¼ˆåœ¨è¿™é‡Œç§°ä¹‹ä¸ºå·ç§¯æ ¸ï¼‰æ¥ä¸è¿™ä¸€å±‚çš„è¾“å…¥inputåšäº†ä¸€æ¬¡è¿ç®—ã€‚è¿ç®—çš„ç»“æœåŠ ä¸Šäº†åç½®Tensor<em>bias</em>ï¼Œä½œä¸ºè¿™ä¸ªå·ç§¯å±‚çš„æœ€ç»ˆè¾“å‡ºã€‚å¾ˆå¤šäººè®¤ä¸ºè¿™é‡Œçš„åç½®biasæ„ä¹‰ä¸æ˜ï¼Œå› æ­¤ä»–ä»¬åœ¨å·ç§¯ä¹‹åæ²¡æœ‰åŠ ä¸Šè¿™æ ·çš„ä¸€ä¸ªbiaså˜é‡ã€‚å®šä¹‰ä¸€ä¸ªå˜é‡åªéœ€è¦è¿™æ ·ä¸€ä¸ªè¯­å¥ï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable(tf.truncated_normal(shape, stddev=sigma), dtype=tf.float32, name=suffix)</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>To define weight or bias variables, create a <em>tf.Variable</em> object. Usually you&#39;ll need to give the <em>initial_value</em> which also decides the shape of this tensor. <em>tf.truncated_normal()</em> and <em>tf.constant()</em> are usually used as the initial values. Also, other APIs - function <em>tf.get_variable()</em> and package <em>tf.initializers</em> are frequently used when using some more methods for initialization. I strongly recommend that you try using these APIs yourself.<br></div><br><div lang="zh-cn"><br>è¦å®šä¹‰æƒé‡æˆ–è€…åç½®å˜é‡ï¼Œè¯·åˆ›å»ºä¸€ä¸ª<em>tf.Variable</em>å¯¹è±¡ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œä½ ä¼šéœ€è¦ç»™å‡º<em>initial_value</em>ï¼ˆTFå˜é‡çš„åˆå§‹å€¼ï¼‰ï¼Œè¿™å°†é¡ºä¾¿å®šä¹‰äº†è¿™ä¸ªå˜é‡çš„shapeï¼ˆå› ä¸ºåˆå§‹å€¼çš„shapeæ˜¯ç¡®å®šçš„ï¼‰ã€‚å¦å¤–ï¼Œä¸€äº›æ–°çš„APIâ€”â€”<em>tf.get_variable()</em>å‡½æ•°å’Œ<em>tf.initializers</em>åŒ…ä¹Ÿå¸¸ç”¨ä¸çš„å‚æ•°åˆå§‹åŒ–ï¼Œä»¥å®ç°æ›´å¤šæ ·çš„åˆå§‹åŒ–æ–¹æ³•ã€‚æˆ‘å¼ºçƒˆå»ºè®®è‡ªå·±åŠ¨æ‰‹å®è·µä¸€ä¸‹ï¼Œè¯•ä¸€è¯•è¿™äº›APIã€‚<br></div><h3 id="3-2-Tensors-and-operations"><a href="#3-2-Tensors-and-operations" class="headerlink" title="3.2 Tensors and operations"></a>3.2 Tensors and operations</h3><div lang="en-us"><br>Going on with the parameters of the <em>tf.nn.conv2d()</em> function. The required parameters also include <em>strides</em> and <em>padding</em>. You should have already learned about what strides mean in convolution, and I&#39;ll only talk about their formats. <em>strides</em> require a 1-D vector with a length of 4, like [1, 2, 2, 1]. The 1st and the 4th number is always 1(in order to match dimensions with the input), while the 2nd and the 3rd means the vertical stride and the horizonal stride.<br>The 4th parameter <em>padding</em> is a little bit different from its definition in convolution operation. It requires &#39;SAME&#39; of &#39;VALID&#39;, denoting &#39;with&#39; or &#39;without&#39; zero paddings. When it&#39;s &#39;SAME&#39;, zero padding is introduced to make the shapes match as needed, equally on every side of the input map.<br></div><br><div lang="zh-cn"><br>æ¥ç€è¯´<em>tf.nn.conv2d()</em>å‡½æ•°çš„å‚æ•°ã€‚éœ€è¦çš„å‚æ•°è¿˜åŒ…æ‹¬<em>strides</em>å’Œ<em>padding</em>ã€‚ä½ åº”è¯¥åœ¨äº†è§£å·ç§¯è¿ç®—æ—¶å­¦è¿‡strideï¼ˆæ­¥å¹…ï¼‰å«ä¹‰ï¼Œæˆ‘åªè¯´ä¸€ä¸‹å®ƒçš„æ ¼å¼å§ã€‚<em>strides</em>å‚æ•°éœ€è¦æ˜¯ä¸€ä¸ª1ç»´ã€é•¿åº¦ä¸º4çš„å‘é‡ã€‚ç¬¬ä¸€ä½å’Œç¬¬å››ä½æ°¸è¿œéƒ½æ˜¯1ï¼Œç¬¬äºŒä½å’Œç¬¬ä¸‰ä½åˆ†åˆ«æ˜¯ç«–ç›´æ–¹å‘å’Œæ°´å¹³æ–¹å‘çš„æ­¥å¹…é•¿ã€‚ç»´æŒè¿™ä¸ªå½¢å¼åªæ˜¯ä¸ºäº†ä¸è¾“å…¥çš„æ•°æ®ç»´åº¦åŒ¹é…ï¼Œå› æ­¤APIçœ‹èµ·æ¥éå¸¸è¹©è„šã€‚<br>ç¬¬å››ä¸ªå‚æ•°<em>padding</em>å’Œå·ç§¯è¿ç®—é‡Œçš„paddingä¸å¤ªç›¸åŒã€‚å®ƒçš„å€¼åªèƒ½æ˜¯&#39;SAME&#39;æˆ–&#39;VALID&#39;ï¼Œåˆ†åˆ«ä»£è¡¨â€œå¸¦â€å’Œâ€œä¸å¸¦â€é›¶è¡¥å…¨ã€‚å¦‚æœæ˜¯&#39;SAME&#39;çš„è¯ï¼Œå‡½æ•°ä¼šå‡åŒ€åœ°åœ¨å›¾åƒçš„ä¸Šä¸‹å·¦å³ä½¿ç”¨é›¶è¡¥å…¨æ¥ä½¿å¾—è¿ç®—ç»“æœä¸ä¹‹å‰å°½å¯èƒ½ä¿æŒä¸€è‡´ã€‚ï¼ˆstride&gt;1æ—¶æœ‰å¯èƒ½è¾“å‡ºå°ºå¯¸ä¸æ˜¯æ­£å¥½ç­‰äºåŸæ¥çš„å°ºå¯¸/strideï¼Œå› ä¸ºè¡¥å…¨é—®é¢˜ï¼‰<br></div><br><div lang="en-us"><br>tf.nn.conv2d() is just an example of TensorFlow <em>operations</em>. Other functions like <em>tf.matmul()</em>, <em>tf.reduce_mean()</em>, <em>tf.global_variables_initializer()</em>, <em>tf.losses.softmax_cross_entropy()</em>, <em>tf.truncated_normal()</em> are all operations. Operation functions return tensors(<em>tf.truncated_normal</em> also return a tensor, a tensor with initializers).<br></div><br><div lang="zh-cn"><br>tf.nn.conv2d()åªæ˜¯TensorFlowè¿ç®—ï¼ˆ<em>operation</em>ï¼‰çš„ä¸€ä¸ªä¾‹å­ã€‚å…¶ä»–ä¾‹å¦‚<em>tf.matmul()</em>ï¼Œ<em>tf.reduce_mean()</em>ï¼Œ<em>tf.nn.relu()</em>ï¼Œ<em>tf.batch_normalization()</em>ï¼Œ<em>tf.global_variables_initializer()</em>ï¼Œ<em>tf.losses.softmax_cross_entropy()</em>ï¼Œ<em>tf.truncated_normal()</em>ä¹‹ç±»çš„å‡½æ•°ä¹Ÿéƒ½æ˜¯TensorFlowçš„è¿ç®—ã€‚TensorFlowçš„è¿ç®—å‡½æ•°ä¼šè¿”å›ä¸€ä¸ªTensorå¯¹è±¡ï¼ˆåŒ…æ‹¬<em>tf.truncated_normal()</em>ä¹Ÿæ˜¯ï¼å®ƒåªä¸è¿‡è¿”å›çš„æ˜¯ä¸€ä¸ªå¸¦åˆå§‹åŒ–å™¨çš„Tensorè€Œå·²ï¼‰ã€‚<br></div><br><div lang="en-us"><br>All the functions in the TensorChain class are based on the most basic TensorFlow operations and variables. After learning about these basic TensorFlow concepts, actually you can already abandon TensorChain, go and try implementing your own neural networks yourself!<br></div><br><div lang="zh-cn"><br>TensorChainç±»ä¸­çš„æ‰€æœ‰æˆå‘˜å‡½æ•°éƒ½æ˜¯åŸºäºæœ€åŸºæœ¬çš„TensorFlowè¿ç®—å’Œå˜é‡çš„ã€‚å®é™…ä¸Šï¼Œäº†è§£äº†è¿™äº›ï¼Œä½ ç°åœ¨å·²ç»å¯ä»¥æŠ›å¼€TensorChainçš„æŸç¼šï¼Œå»å°è¯•å®ç°ä½ è‡ªå·±çš„ç¥ç»ç½‘ç»œäº†ï¼<br></div><h2 id="4-Spices"><a href="#4-Spices" class="headerlink" title="4 Spices"></a>4 Spices</h2><div lang="en-us"><br>I&#39;m not joking just now! But I know that there are a lot of things that you still don&#39;t understand about using TensorFlow - like &quot;how do I visualize my computation graph&quot;, &quot;how do I save/load my model to/from files&quot;, &quot;how do I record some tensors&#39; values while training&quot; or &quot;how do I view the loss curves&quot; - after all TensorFlow APIs are far more complicated than just building those nets. Those are also important techniques in your research. If you&#39;d rather ask me than spending some time experimenting, please go on with reading.<br></div><br><div lang="zh-cn"><br>æˆ‘ï¼Œæˆ‘çœŸæ²¡å¼€ç©ç¬‘ï¼ä½†æ˜¯æˆ‘çŸ¥é“å…³äºå¦‚ä½•ä½¿ç”¨TensorFlowï¼Œä½ è¿˜æœ‰è®¸è®¸å¤šå¤šçš„é—®é¢˜ï¼Œå¥½æ¯”â€œå¦‚ä½•å¯è§†åŒ–åœ°æŸ¥çœ‹æˆ‘çš„è®¡ç®—å›¾ç»“æ„â€ã€â€œå¦‚ä½•å­˜å‚¨/è¯»å–æ¨¡å‹æ–‡ä»¶â€ã€â€œå¦‚ä½•è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­æŸäº›Tensorçš„çœŸå®å€¼â€ã€â€œå¦‚ä½•æŸ¥çœ‹æŸå¤±å‡½æ•°çš„å˜åŒ–æ›²çº¿â€â€”â€”æ¯•ç«ŸTensorFlowçš„APIå¤ªå¤æ‚äº†ï¼Œè¿œæ¯”æ­å»ºç¥ç»ç½‘ç»œé‚£ç‚¹å‡½æ•°å¤æ‚å¾—å¤šã€‚ä¸Šè¾¹è¯´çš„é‚£äº›æ˜¯ä½ ä½¿ç”¨TensorFlowç ”ç©¶è¿‡ç¨‹ä¸­çš„é‡è¦æŠ€å·§ã€‚å¦‚æœä½ æ„¿æ„å¬æˆ‘è®²è€Œä¸æƒ³èŠ±äº›æ—¶é—´å°è¯•çš„è¯ï¼Œè¯·ç»§ç»­è¯»ä¸‹å»ã€‚<br></div><h3 id="4-1-Saving-and-loading-your-model"><a href="#4-1-Saving-and-loading-your-model" class="headerlink" title="4.1 Saving and loading your model"></a>4.1 Saving and loading your model</h3><div lang="en-us"><br>The very first thing that you may want to do - after training a network model with nice outcomes - would be saving it. Saving a model is fairly easy - just use a <em>tf.train.Saver</em> object. See my code below:<br></div><br><div lang="zh-cn"><br>è®­ç»ƒå‡ºä¸€ä¸ªçœ‹èµ·æ¥è¾“å‡ºè¿˜ä¸é”™çš„ç¥ç»ç½‘ç»œæ¨¡å‹åä½ æƒ³åšçš„ç¬¬ä¸€ä»¶äº‹ææ€•å°±æ˜¯æŠŠå®ƒå­˜ä¸‹æ¥äº†å§ï¼Ÿä¿å­˜æ¨¡å‹å…¶å®éå¸¸ç®€å•ï¼šåªè¦ç”¨ä¸€ä¸ª<em>tf.train.Saver</em>ç±»çš„å¯¹è±¡ã€‚ä»£ç ç¤ºä¾‹ï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    <span class="comment"># Train it for some iterations</span></span><br><span class="line">    <span class="comment"># Train it for some iterations</span></span><br><span class="line">    <span class="comment"># Train it for some iterations</span></span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    saver.save(session, <span class="string">'models/model.ckpt'</span>)</span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>I saved my model and variable values to &#39;models/model.ckpt&#39;. But actually, you&#39;ll find 3 files in the &#39;models&#39; directory - <em>model.ckpt.data-00000-of-00001</em>, <em>model.ckpt.meta</em> and <em>model.ckpt.index</em> - none of which is &#39;model.ckpt&#39;! That&#39;s because TensorFlow stores the graph structure separately from variables values. The <em>.meta</em> file describes the saved graph structure; the <em>.index</em> file records the mappings between tensor names and tensor metadata; and the <em>.data-00000-of-00001</em> file - which is always the biggest one - saves all the variable values. If you need the graph data together with the variable values to be loaded, use a Saver to load after creating a session:<br></div><br><div lang="zh-cn"><br>æˆ‘æŠŠæˆ‘çš„æ¨¡å‹å’Œå˜é‡å€¼å­˜åˆ°äº†&#39;models/model.ckpt&#39;æ–‡ä»¶é‡Œã€‚ä½†æ˜¯ï¼å®é™…ä¸Šåœ¨modelsç›®å½•é‡Œä½ ä¼šæ‰¾åˆ°ä¸‰ä¸ªæ–‡ä»¶ï¼š<em>model.ckpt.data-00000-of-00001</em>ï¼Œ<em>model.ckpt.meta</em>å’Œ<em>model.ckpt.index</em>â€”â€”å“ªä¸ªä¹Ÿä¸æ˜¯model.ckptå‘€ï¼Ÿé‚£æ˜¯å› ä¸ºTensorFlowæŠŠè®¡ç®—å›¾çš„ç»“æ„å’Œå›¾ä¸­å„ç§å˜é‡çš„å€¼åˆ†å¼€å­˜æ”¾äº†ã€‚<em>.meta</em>æ–‡ä»¶æè¿°è®¡ç®—å›¾çš„ç»“æ„ï¼›<em>.index</em>æ–‡ä»¶è®°å½•å„ä¸ªTensoråç§°ï¼ˆæ˜¯nameå±æ€§ï¼Œè€Œä¸æ˜¯å˜é‡åï¼‰ä¸Tensorå…ƒä¿¡æ¯ä¹‹é—´çš„æ˜ å°„ï¼›<em>.data-00000-of-00001</em>æ–‡ä»¶å¾€å¾€æ˜¯æœ€å¤§çš„ä¸€ä¸ªï¼Œå®ƒå­˜å‚¨çš„æ˜¯å„ä¸ªTensorFlowå˜é‡çš„å®é™…å€¼ã€‚å¦‚æœè¯»å–æ—¶éœ€è¦æŠŠå›¾ç»“æ„å’Œå˜é‡å€¼éƒ½è¯»è¿›æ¥ï¼Œåœ¨sessionåˆ›å»ºä»¥åï¼ŒåŒæ ·ç”¨ä¸€ä¸ªSaveræ¥è¯»å–å³å¯ï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    saver.restore(session, <span class="string">'models/model.ckpt'</span>)</span><br><span class="line">    <span class="comment"># Then continue doing everything just like the model is just trained</span></span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>Remember that session.run(tf.global_variables_initializer()) shouldn&#39;t be executed, since variables are already initialized with your saved <em>.data-0000-of-00001</em> file.<br>If you only need the graph to be loaded, only use the <em>.meta</em> file:<br></div><br><div lang="zh-cn"><br>è®°ä½ï¼Œè¿™æ—¶å€™å°±ä¸è¦å†å»æ‰§è¡Œsession.run(tf.global_variables_initializer())äº†ï¼Œå› ä¸ºå˜é‡å·²ç»ç”¨å­˜å‚¨çš„checkpointæ–‡ä»¶å†…å®¹åˆå§‹åŒ–è¿‡äº†ã€‚<br>å¦‚æœåªéœ€è¦è¯»å–è®¡ç®—å›¾ç»“æ„ï¼Œåªè¦è¯»å–<em>.meta</em>æ–‡ä»¶ï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    tf.train.import_meta_graph(<span class="string">'models/model.ckpt.meta'</span>)</span><br><span class="line">    <span class="comment"># Then continue doing everything just like the model is just built</span></span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>Function <em>tf.train.import_meta_graph()</em> loads(appends) the graph to your current computation graph. The values of tensors are still uninitialized so you&#39;ll have to execute session.run(tf.global_variables_initializer()) again. The tensors that you defined in the model can be retrieved by their names(property of the Tensor objects, instead of Python variable names). For example:<br></div><br><div lang="zh-cn"><br><em>tf.train.import_meta_graph()</em>å‡½æ•°å°†æ–‡ä»¶é‡Œçš„è®¡ç®—å›¾è¯»åˆ°ï¼ˆæ·»åŠ åˆ°ï¼‰ä½ å½“å‰çš„è®¡ç®—å›¾ä¸­ã€‚å…¶ä¸­æ‰€æœ‰Tensorçš„å€¼ä»æœªåˆå§‹åŒ–ï¼Œæ‰€ä»¥æœ‰å¿…è¦æ‰§è¡Œä¸€ä¸‹session.run(tf.global_variables_initializer())äº†ã€‚ä¹‹å‰å®šä¹‰çš„å˜é‡å¯ä»¥æŒ‰ç…§åç§°å–å›ï¼Œç¤ºä¾‹ï¼š<br></div><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    <span class="comment"># Recover the model here</span></span><br><span class="line"></span><br><span class="line">    graph = tf.get_default_graph()</span><br><span class="line">    image_tensor = graph.get_tensor_by_name(<span class="string">'input_image:0'</span>)</span><br><span class="line">    loss = graph.get_tensor_by_name(<span class="string">'loss:0'</span>)</span><br><span class="line">    train = graph.get_operation_by_name(<span class="string">'train)</span></span><br></pre></td></tr></table></figure><br><br><div lang="en-us"><br>To retrieve normal tensors, you&#39;ll have to append a <em>&#39;:0&#39;</em> to the name of the op. This means getting the associated tensor of the op. <em>train</em> is a little special - we only need the op, so the function is <em>get_operation_by_name()</em> so the &#39;:0&#39; is not necessary.<br></div><br><div lang="zh-cn"><br>è¦å–å›ä¸€èˆ¬çš„Tensorï¼Œéœ€è¦åœ¨Tensorçš„nameå±æ€§å€¼åè¾¹åŠ ä¸€ä¸ª<em>&#39;:0&#39;</em>ï¼Œæ„æ€æ˜¯å–è¿™ä¸ªè¿ç®—å¯¹åº”çš„Tensorã€‚è®­ç»ƒæ“ä½œ<em>train</em>ç•¥æœ‰ä¸åŒâ€”â€”æˆ‘ä»¬è¦çš„å°±åªæ˜¯è¿™ä¸ªopï¼Œæ‰€ä»¥ç”¨çš„å‡½æ•°<em>get_operation_by_name()</em>è·Ÿå…¶ä»–Tensorä¸ä¸€æ ·ï¼Œè€Œä¸”&#39;:0&#39;ä¹Ÿä¸éœ€è¦åŠ ã€‚<br></div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;&lt;/p&gt;
&lt;div lang=&quot;en-us&quot;&gt;&lt;br&gt;Update in March 2019:&lt;br&gt;&lt;br&gt;After TensorFlow developers introduced the APIs of Tensorflow 2.0 on Tensorflow Dev Summit 2019, I have made my decision to turn to PyTorch.&lt;br&gt;&lt;div align=&quot;center&quot; class=&quot;figure&quot;&gt;&lt;br&gt;&lt;img src=&quot;/images/tftutorial/wjz_en.gif&quot; alt=&quot;çœŸé¦™ï¼&quot;&gt;&lt;br&gt;&lt;/div&gt;&lt;br&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://linkinpark213.com/tags/Deep-Learning/"/>
    
      <category term="DIY" scheme="http://linkinpark213.com/tags/DIY/"/>
    
  </entry>
  
  <entry>
    <title>MathJax - Use Math in Hexo, Just Like Tex! (Including Common Issue Solutions)</title>
    <link href="http://linkinpark213.com/2018/04/24/mathjax/"/>
    <id>http://linkinpark213.com/2018/04/24/mathjax/</id>
    <published>2018-04-24T05:43:41.000Z</published>
    <updated>2019-01-03T05:43:17.259Z</updated>
    
    <content type="html"><![CDATA[<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script><p>Sometimes you may want to explain some algorithms or principles with beautiful formulae in your blog. How to do this? Edit them in Microsoft Word, take a screenshot, crop it and put it in the blog post? When you finish your article and find out that you missed a symbol in the pictures - oh man, gotta repeat that again? Stop using those images now! A beautiful math display engine - MathJax allows you to code math like a coder.</p><div style="font-size: 1.2em"><br>$$\mathcal{C}\phi \delta e \mathfrak{M}\alpha th \mathit{I}n \mathcal{H}ex\sigma \mathbb{N}o\omega!$$<br></div><a id="more"></a><h2 id="1-Installation"><a href="#1-Installation" class="headerlink" title="1 Installation"></a>1 Installation</h2><h3 id="1-1-With-npm-For-those-using-Hexo-like-me"><a href="#1-1-With-npm-For-those-using-Hexo-like-me" class="headerlink" title="1.1 With npm (For those using Hexo like me)"></a>1.1 With npm (For those using Hexo like me)</h3><p>First, install <em>hexo-math</em> in your Hexo blog directory.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install hexo-math --save</span><br></pre></td></tr></table></figure></p><p>Then, add <em>math</em> configurations in your <em>_config.yml</em> file.<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">math:</span></span><br><span class="line"><span class="attr">  engine:</span> <span class="string">'mathjax'</span></span><br></pre></td></tr></table></figure></p><p>Finally, also add to your <em>_config.yml</em> file in the <strong>theme directory</strong> these configurations below.<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mathjax:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  per_page:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">  cdn:</span> <span class="string">//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML</span></span><br></pre></td></tr></table></figure></p><h3 id="1-2-Or-by-inserting-a-snippet-in-your-HTML-code"><a href="#1-2-Or-by-inserting-a-snippet-in-your-HTML-code" class="headerlink" title="1.2 Or by inserting a snippet in your HTML code"></a>1.2 Or by inserting a snippet in your HTML code</h3><p>Maybe you don&#39;t have to use math in every blog post. If so, insert the following snippet in your Markdown file also works.<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML'</span> <span class="attr">async</span>&gt;</span><span class="undefined"></span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br></pre></td></tr></table></figure></p><h2 id="2-Usage"><a href="#2-Usage" class="headerlink" title="2 Usage"></a>2 Usage</h2><p>MathJax supports the same grammar that LaTeX does. To learn more about LaTeX, please refer to Chapter 3 of <a href="https://tobi.oetiker.ch/lshort/lshort.pdf" target="_blank" rel="noopener">The Not So Short<br>Introduction to LATEX</a>(CN version also available <a href="http://www.mohu.org/info/lshort-cn.pdf" target="_blank" rel="noopener">here</a>).</p><p>Use a &quot;\\(&quot; and a &quot;\\)&quot; to insert a formula in the line(they decide the boundary of the formula), or two &quot;$$&quot; to insert one that occupy a new line. I&#39;ll give a few examples below.</p><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\\(\mathcal&#123;F&#125;(x)=\mathcal&#123;H&#125;(x)-x\\)</span><br></pre></td></tr></table></figure><p>\(\mathcal{F}(x)=\mathcal{H}(x)-x\)<br><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\\(E=mc^2\\)</span><br></pre></td></tr></table></figure></p><p>\(E=mc^2\)<br><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$$\lim_&#123;n\rightarrow \infty&#125;(1+2^n+3^n)^\frac&#123;1&#125;&#123;x+\sin n&#125;$$</span><br></pre></td></tr></table></figure></p><p>$$\lim_{n\rightarrow \infty}(1+2^n+3^n)^\frac{1}{x+\sin n}$$<br><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$$\mathcal&#123;C&#125;\phi \delta e \mathfrak&#123;M&#125;\alpha th \mathit&#123;I&#125;n \mathcal&#123;H&#125;ex\sigma \mathbb&#123;N&#125;o\omega!$$</span><br></pre></td></tr></table></figure></p><p>$$\mathcal{C}\phi \delta e \mathfrak{M}\alpha th \mathit{I}n \mathcal{H}ex\sigma \mathbb{N}o\omega!$$</p><h2 id="3-Problems-when-using-MathJax-with-Hexo-amp-Solutions"><a href="#3-Problems-when-using-MathJax-with-Hexo-amp-Solutions" class="headerlink" title="3 Problems when using MathJax with Hexo &amp; Solutions"></a>3 Problems when using MathJax with Hexo &amp; Solutions</h2><p>This list will be appended whenever I find any more.</p><h3 id="3-1-Subscript-symbol-quot-quot-gets-mistaken-as-Markdown-emphasize-symbol"><a href="#3-1-Subscript-symbol-quot-quot-gets-mistaken-as-Markdown-emphasize-symbol" class="headerlink" title="3.1 Subscript symbol &quot;_&quot; gets mistaken as Markdown emphasize symbol"></a>3.1 Subscript symbol &quot;_&quot; gets mistaken as Markdown emphasize symbol</h3><p>This is a tough problem. Hexo renderer would first render the .md file into a .html file, and the MathJax script will only work on the .html file. Therefore, when there are multiple subscript symbols, they might be rendered as &lt;em&gt;&lt;/em&gt; tags. </p><p>For example: when you actually need a full-line formula \(x_{i+1}+y_j\), perhaps you&#39;ll get a &quot;$$x<em>{i+1}+y</em>j$$&quot; instead. Look into the HTML code and you&#39;ll understand why.</p><p>My solution for now, is giving up this Markdown emphasize symbol, since both &quot;_&quot; and &quot;*&quot; can be used as emphasize tags, and the alternative symbol &quot;*&quot; will also work if we remove &quot;_&quot;. Using &quot;\_&quot; also works, but it would be frequently used(while &quot;*&quot; isn&#39;t), thus turning our math code into mess code.</p><p>How do we do this? Bravely look into the <em>node_modules</em> directory and find the renderer of the Hexo engine. My renderer is <em>marked</em>, which is the default for Hexo. There is a file named <em>marked.js</em> inside <em>node_modules/marked/lib/</em> directory. You can find two appearances of &quot;em:&quot;. Like this:<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> inline = &#123;</span><br><span class="line">  ...</span><br><span class="line">  em: <span class="regexp">/^\b_((?:[^_]|__)+?)_\b|^\*((?:\*:\*|[\s\S])+?)\*(?!\*)/</span>,</span><br><span class="line">  ...</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>and<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inline.pedantic = merge(&#123;&#125;, inline.normal, &#123;</span><br><span class="line">  ...</span><br><span class="line">  em: <span class="regexp">/^_(?=\S)([\s\S]*?\S)_(?!_)|^\*(?=\S)([\s\S]*?\S)\*(?!\*)/</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p><p>Modify the regular expression after them - remove the one about &quot;_&quot;s and leave the one about &quot;*&quot;s. The new version would be:<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> inline = &#123;</span><br><span class="line">  ...</span><br><span class="line">  em: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br><span class="line">  ...</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>and<br><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inline.pedantic = merge(&#123;&#125;, inline.normal, &#123;</span><br><span class="line">  ...</span><br><span class="line">  em: <span class="regexp">/^\*(?=\S)([\s\S]*?\S)\*(?!\*)/</span></span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></p><p>From now on, you can use &quot;_&quot; as the subscript in MathJax freely. You don&#39;t have to worry about its becoming &lt;em&gt;&lt;/em&gt; tags anymore.</p><h3 id="3-2-Using-quot-amp-quot-for-aligning-multi-line-equations-but-getting-a-quot-Misplaced-amp-quot"><a href="#3-2-Using-quot-amp-quot-for-aligning-multi-line-equations-but-getting-a-quot-Misplaced-amp-quot" class="headerlink" title="3.2 Using &quot;&amp;&quot; for aligning multi-line equations but getting a &quot;Misplaced &amp;&quot;"></a>3.2 Using &quot;&amp;&quot; for aligning multi-line equations but getting a &quot;Misplaced &amp;&quot;</h3><p>For example, in my previous post about ResNet, I tried to use the following code to start a new line in an equation while aligning the lines to the equal sign:<br><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$$\frac&#123;\partial&#123;\mathcal&#123;E&#125;&#125;&#125;&#123;\partial&#123;x<span class="emphasis">_l&#125;&#125; &amp; = \frac&#123;\partial&#123;\mathcal&#123;E&#125;&#125;&#125;&#123;\partial&#123;x_</span>L&#125;&#125;\frac&#123;\partial&#123;x<span class="emphasis">_L&#125;&#125;&#123;\partial&#123;x_</span>l&#125;&#125;\\\\</span><br><span class="line">&amp; = \frac&#123;\partial&#123;\mathcal&#123;E&#125;&#125;&#125;&#123;\partial&#123;x<span class="emphasis">_L&#125;&#125;\Big(1+\frac&#123;\partial&#123;&#125;&#125;&#123;\partial&#123;x_</span>l&#125;&#125;\sum<span class="emphasis">_&#123;i=l&#125;^&#123;L-1&#125;\mathcal&#123;F&#125;(x_</span>i,\mathcal&#123;W&#125;_i)\Big)$$</span><br></pre></td></tr></table></figure></p><p>The &quot;&amp;&quot; symbols were used to align the lines to a certain point. However, the result was a &quot;Misplaced &amp;&quot; prompt.</p><p>By disabling MathJax, I found out that the rendered equation was correct, which means that <strong>the problem isn&#39;t with Hexo renderer</strong>. This was when I realized that although<br><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation&#125;</span><br><span class="line">\end&#123;equation&#125;</span><br></pre></td></tr></table></figure></p><p>are not necessary,<br><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;split&#125;</span><br><span class="line">\end&#123;split&#125;</span><br></pre></td></tr></table></figure></p><p>shouldn&#39;t be removed. Surround the equation with them will work. My code is here:<br><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$$\begin&#123;split&#125;</span><br><span class="line">\frac&#123;\partial&#123;\mathcal&#123;E&#125;&#125;&#125;&#123;\partial&#123;x<span class="emphasis">_l&#125;&#125; &amp; = \frac&#123;\partial&#123;\mathcal&#123;E&#125;&#125;&#125;&#123;\partial&#123;x_</span>L&#125;&#125;\frac&#123;\partial&#123;x<span class="emphasis">_L&#125;&#125;&#123;\partial&#123;x_</span>l&#125;&#125;\\\\</span><br><span class="line">&amp; = \frac&#123;\partial&#123;\mathcal&#123;E&#125;&#125;&#125;&#123;\partial&#123;x<span class="emphasis">_L&#125;&#125;\Big(1+\frac&#123;\partial&#123;&#125;&#125;&#123;\partial&#123;x_</span>l&#125;&#125;\sum<span class="emphasis">_&#123;i=l&#125;^&#123;L-1&#125;\mathcal&#123;F&#125;(x_</span>i,\mathcal&#123;W&#125;_i)\Big)</span><br><span class="line">\end&#123;split&#125;$$</span><br></pre></td></tr></table></figure></p><p>And it runs like:<br>$$\begin{split}<br>\frac{\partial{\mathcal{E}}}{\partial{x_l}} &amp; = \frac{\partial{\mathcal{E}}}{\partial{x_L}}\frac{\partial{x_L}}{\partial{x_l}}\\<br>&amp; = \frac{\partial{\mathcal{E}}}{\partial{x_L}}\Big(1+\frac{\partial{}}{\partial{x_l}}\sum_{i=l}^{L-1}\mathcal{F}(x_i,\mathcal{W}_i)\Big)<br>\end{split}$$</p><h3 id="3-3-To-be-continued"><a href="#3-3-To-be-continued" class="headerlink" title="3.3 To be continued"></a>3.3 To be continued</h3><p>If you encounter other issues while using MathJax with Hexo(with or without a solution), feel free to leave a comment below!</p>]]></content>
    
    <summary type="html">
    
      &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML&quot; async&gt;&lt;/script&gt;

&lt;p&gt;Sometimes you may want to explain some algorithms or principles with beautiful formulae in your blog. How to do this? Edit them in Microsoft Word, take a screenshot, crop it and put it in the blog post? When you finish your article and find out that you missed a symbol in the pictures - oh man, gotta repeat that again? Stop using those images now! A beautiful math display engine - MathJax allows you to code math like a coder.&lt;/p&gt;
&lt;div style=&quot;font-size: 1.2em&quot;&gt;&lt;br&gt;$$\mathcal{C}\phi \delta e \mathfrak{M}\alpha th \mathit{I}n \mathcal{H}ex\sigma \mathbb{N}o\omega!$$&lt;br&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="Blogging" scheme="http://linkinpark213.com/tags/Blogging/"/>
    
  </entry>
  
  <entry>
    <title>A Review of ResNet - Residual Networks</title>
    <link href="http://linkinpark213.com/2018/04/22/resnet/"/>
    <id>http://linkinpark213.com/2018/04/22/resnet/</id>
    <published>2018-04-22T05:55:35.000Z</published>
    <updated>2019-04-21T07:23:02.102Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="0-Introduction"><a href="#0-Introduction" class="headerlink" title="0 Introduction"></a>0 Introduction</h2><p>Deep learning researchers have been constructing skyscrapers in recent years. Especially, VGG nets and GoogLeNet have pushed the depths of convolutional networks to the extreme. But questions remain: if time and money aren&#39;t problems, are deeper networks always performing better? Not exactly.</p><p>When residual networks were proposed, researchers around the world was stunned by its depth. &quot;Jesus Christ! Is this a neural network or the Dubai Tower?&quot; But <strong>don&#39;t be afraid!</strong> These networks are deep but the structures are simple. Interestingly, these networks not only defeated all opponents in the classification, detection, localization challenges in ImageNet 2015, but were also the main innovation in the best paper of CVPR2016.</p><div align="center"><br>    <img src="/images/resnet/network_growth.jpg" width="40%" height="40%" alt="Network Growth"><br></div><a id="more"></a><h2 id="1-The-Crisis-Degradation-of-Deep-Networks"><a href="#1-The-Crisis-Degradation-of-Deep-Networks" class="headerlink" title="1 The Crisis: Degradation of Deep Networks"></a>1 The Crisis: Degradation of Deep Networks</h2><p>VGG nets proved the beneficial of representation depth of convolutional neural networks, at least within a certain range, to be exact. However, when Kaiming He et al. tried to deepen some plain networks, the training error and test error stopped decreasing after the network reached a certain depth(which is not surprising) and soon degraded. This is not an overfitting problem, because training errors also increased; nor is it a gradient vanishing problem, because there are some techniques(e.g. batch normalization[4]) that ease the pain.</p><div align="center" class="figure"><br>    <img src="/images/resnet/downgrade.png" width="60%" height="60%" alt="The Downgrade Problem"><br>    Fig.1 The downgrade problem<br><br></div><p>What seems to be the cause of this degradation? Obviously, deeper neural networks are more difficult to train, but that doesn&#39;t mean deeper neural networks would yield worse results. To explain this problem, Balduzzi et al.[3] identified shattered gradient problem - as depth increases, gradients in standard feedforward networks increasingly resemble white noise. I will write about that later.</p><h2 id="2-A-Closer-Look-at-ResNet-The-Residual-Blocks"><a href="#2-A-Closer-Look-at-ResNet-The-Residual-Blocks" class="headerlink" title="2 A Closer Look at ResNet: The Residual Blocks"></a>2 A Closer Look at ResNet: The Residual Blocks</h2><p>As the old saying goes, &quot;åƒé‡Œä¹‹è¡Œï¼Œå§‹äºè¶³ä¸‹&quot;. Although ResNets are as deep as a thousand layers, they are built with these basic residual blocks(the right part of the figure). </p><div align="center" class="figure"><br>    <img src="/images/resnet/residual_blocks.png" width="50%" height="50%" alt="Comparison between normal weight layers and residual blocks"><br>    Fig.2 Parts of plain networks and a residual block(or residual unit)<br><br></div><h3 id="2-1-Skip-Connections"><a href="#2-1-Skip-Connections" class="headerlink" title="2.1 Skip Connections"></a>2.1 Skip Connections</h3><p>In comparison, basic units of plain network models would look like the one on the left: one ReLU function after a weight layer(usually also with biases), repeated several times. Let&#39;s denote the desired underlying mapping(the ideal mapping) of the two layers as \(\mathcal{H}(x)\), and the real mapping as \(\mathcal{F}(x)\). Clearly, the closer \(\mathcal{F}(x)\) is to \(\mathcal{H}(x)\), the better it fits.</p><p>However, He et al. explicitly let these layers fit a residual mapping instead of the desired underlying mapping. This is implemented with &quot;shortcut connections&quot;, which skip one or more layers, simply performing identity mappings and getting added to the outputs of the stacked weight layers. This way, \(\mathcal{F}(x)\) would not try to fit \(\mathcal{H}(x)\), but \(\mathcal{H}(x)-x\). The whole structure(from the identity mapping branch, to merging the branches by the addition operation) are named &quot;residual blocks&quot;(or &quot;residual units&quot;).</p><p>What&#39;s the point in this? Let&#39;s do a simple analysis. The computation done by the original residual block is: $$y_l=h(x_l)+\mathcal{F}(x_l,\mathcal{W}_l),$$ $$x_{l+1}=f(y_l).$$</p><p>Here are the definitions of symbols:<br>\(x_l\): input features to the \(l\)-th residual block;<br>\(\mathcal{W}_{l}={W_{l,k}|_{1\leq k\leq K}}\): a set of weights(and biases) associated with the \(l\)-th residual unit. \(K\) is the number of layers in this block;<br>\(\mathcal{F}(x,\mathcal{W})\): the residual function, which we talked about earlier. It&#39;s a stack of 2 conv. layers here;<br>\(f(x)\): the activation function. We are using ReLU here;<br>\(h(x)\): identity mapping.</p><p>If \(f(x)\) is also an identity mapping(as if we&#39;re not using any activation function), the first equation would become:<br>$$x_{l+1}=x_l+\mathcal{F}(x_l,\mathcal{W}_l)$$</p><p>Therefore, we can define \(x_L\) recursively of any layer:<br>$$x_L=x_l+\sum_{i=l}^{L-1}\mathcal{F}(x_i,\mathcal{W}_i)$$</p><p>That&#39;s not the end yet! When it comes to the gradients, according to the chain rules of backpropagation, we have a beautiful definition:<br>$$\begin{split}<br>\frac{\partial{\mathcal{E}}}{\partial{x_l}} &amp; = \frac{\partial{\mathcal{E}}}{\partial{x_L}}\frac{\partial{x_L}}{\partial{x_l}}\\<br>&amp; = \frac{\partial{\mathcal{E}}}{\partial{x_L}}\Big(1+\frac{\partial{}}{\partial{x_l}}\sum_{i=l}^{L-1}\mathcal{F}(x_i,\mathcal{W}_i)\Big)<br>\end{split}$$</p><p>What does it mean? It means that the information is directly backpropagated to ANY shallower block. This way, the gradients of a layer never vanish or explode even if the weights are too small or too big.</p><h3 id="2-2-Identity-Mappings"><a href="#2-2-Identity-Mappings" class="headerlink" title="2.2 Identity Mappings"></a>2.2 Identity Mappings</h3><p>It&#39;s important that we use identity mapping here! Just consider doing a simple modification here, for example, \(h(x)=\lambda_lx_l\)(\(\lambda_l\) is a modulating scalar). The definition of \(x_L\) and \(\frac{\partial{\mathcal{E}}}{\partial{x_l}}\) would become:<br>$$x_L=(\prod_{i=l}^{L-1}\lambda_i)x_l+\sum_{i=l}^{L-1}(\prod_{j=i+1}^{L-1}\lambda_j)\mathcal{F}(x_i,\mathcal{W}_i)$$<br>$$\frac{\partial{\mathcal{E}}}{\partial{x_l}}=\frac{\partial{\mathcal{E}}}{\partial{x_L}}\Big((\prod_{i=l}^{L-1}\lambda_i)+\frac{\partial{}}{\partial{x_l}}\sum_{i=l}^{L-1}(\prod_{j=i+1}^{L-1}\lambda_j)\mathcal{F}(x_i,\mathcal{W}_i)\Big)$$</p><p>For extremely deep neural networks where \(L\) is too large, \(\prod_{i=l}^{L-1}\lambda_i\) could be either too small or too large, causing gradient vanishing or gradient explosion. For \(h(x)\) with complex definitions, the gradient could be extremely complicated, thus losing the advantage of the skip connection. Skip connection works best under the condition where the grey channel in Fig. 3 cover no operations (except the addition) and is clean.</p><p>Interestingly, this comfirmed the philosophy of &quot;å¤§é“è‡³ç®€&quot; once again.</p><h3 id="2-3-Post-activation-or-Pre-activation"><a href="#2-3-Post-activation-or-Pre-activation" class="headerlink" title="2.3 Post-activation or Pre-activation?"></a>2.3 Post-activation or Pre-activation?</h3><p>Wait a second... &quot;\(f(x)\) is also an identity mapping&quot; is just our assumption. The activation function is still there!</p><p>Right. There IS an activation function, but it&#39;s moved to somewhere else.  In fact, the original residual block is still a little bit problematic - the output of one residual block is not always the input of the next, since there is a ReLU activation function after the addition(It did NOT REALLY keep the identity mapping to the next block!). Therefore, in[2], He et al. fixed the residual blocks by changing the order of operations.</p><div align="center" class="figure"><br>    <img src="/images/resnet/identity_mapping.png" width="30%" height="30%" alt="New identity mapping"><br>    Fig.3 New identity mapping proposed by He et al.<br><br></div><p>Besides using a simple identity mapping, He et al. also discussed about the position of the activation function and the batch normalization operation. Assuming that we got a special(asymmetric) activation function \(\hat f(x)\), which only affects the path to the next residual unit. Now our definition of \(x_{x+1}\) would become:<br>$$x_{l+1}=x_l+\mathcal{F}(\hat f(x_l),\mathcal{W}_l)$$</p><p>With \(x_l\) still multiplied by 1, information is still fully backpropagated to shallower residual blocks. And the good thing is that using this asymmetric activation function after the addition(partial post-activation) is equivalent to using it beforehand(pre-activation)! This is why He et al. chose to use pre-activation - otherwise it would be necessary to implement that magical activation function \(\hat f(x)\).</p><div align="center" class="figure"><br>    <img src="/images/resnet/pre-activation.png" width="80%" height="80%" alt="Asymmetric after-addition activation"><br>    Fig.4 Using asymmetric after-addition activation is equivalent to constructing a pre-activation residual unit<br><br></div><h2 id="3-ResNet-Architectures"><a href="#3-ResNet-Architectures" class="headerlink" title="3 ResNet Architectures"></a>3 ResNet Architectures</h2><p>Here are the ResNet architectures for ImageNet. Building blocks are shown in brackets, with the numbers of blocks stacked. With the first block of every stack(starting from conv3_x), a downsampling is performed. Each column represents one of the residual networks, and the deepest one has 152 weight layers! Since ResNets were proposed, VGG nets - which were officially called &quot;Very Deep Convolutional Networks&quot; - are not relatively deep anymore. Maybe call them &quot;A Little Bit Deep Convolutional Networks&quot;.</p><div align="center" class="figure"><br>    <img src="/images/resnet/architectures.png" width="80%" height="80%" alt="ResNet architectures for ImageNet"><br>    Table. 1 ResNet architectures for ImageNet.<br><br></div><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h2><h3 id="4-1-Performance-on-ImageNet"><a href="#4-1-Performance-on-ImageNet" class="headerlink" title="4.1 Performance on ImageNet"></a>4.1 Performance on ImageNet</h3><p>He et al. trained ResNet-18 and ResNet-34 on the ImageNet dataset, and also compared them to plain convolutional networks. In Fig. 5, the thin curves denote training error, and the bold ones denote validation error. The figure on the left shows the results of plain convolution networks(in which the 34-layered ones has higher error rates than the 18-layered one), and the figure on the right shows that residual networks perform better than plain ones, while deeper ones perform better than shallow ones.</p><div align="center" class="figure"><br>    <img src="/images/resnet/training.png" width="80%" height="80%" alt="Training ResNet on ImageNet"><br>    Fig. 5 Training ResNet on ImageNet<br><br></div><h3 id="4-2-Effects-of-Different-Shortcut-Connections"><a href="#4-2-Effects-of-Different-Shortcut-Connections" class="headerlink" title="4.2 Effects of Different Shortcut Connections"></a>4.2 Effects of Different Shortcut Connections</h3><p>He et al. also tried various types of shortcut connections to replace the identity mapping, and various positions of activation functions / batch normalization. Experiments show that the original identity mapping and full pre-activation yield the best results.</p><div align="center" class="figure"><br>    <img src="/images/resnet/shortcut-connections.png" width="50%" height="50%" alt="Different shortcuts of residual units"><br>    Fig. 6 Various shortcuts in residual units<br><br>    <img src="/images/resnet/shortcut-connections-experiment.png" width="70%" height="70%" alt="Classification errors with different shortcuts"><br>    Table. 2 Classification error on CIFAR-10 test set with various shortcut connections in residual units<br><br>    <img src="/images/resnet/activations.png" width="70%" height="50%" alt="Different usages of activation in residual units"><br>    Fig. 7 Various usages of activation in residual units<br><br>    <img src="/images/resnet/activations-experiment.png" width="50%" height="50%" alt="Classification errors with different activations"><br>    Table. 3 Classification error on CIFAR-10 test set with various usages of activation in residual units<br><br></div><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p>Residual learning can be crowned as &quot;ONE OF THE GREATEST HITS IN DEEP LEARNING FIELDS&quot;. With a simple identity mapping, it solved the degradation problem of deep neural networks. Now that you have learned about the concept of ResNet, why not give it a try and implement your first residual learning model today?</p><div align="center" class="figure"><br>    <img src="/images/resnet/resnet-yooo.jpg" width="40%" height="40%" alt><br></div><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf" target="_blank" rel="noopener">He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.</a></p><p>[2] <a href="https://arxiv.org/pdf/1603.05027.pdf" target="_blank" rel="noopener">He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks[C]//European Conference on Computer Vision. Springer, Cham, 2016: 630-645.</a></p><p>[3] <a href="https://arxiv.org/pdf/1702.08591.pdf" target="_blank" rel="noopener">Balduzzi D, Frean M, Leary L, et al. The Shattered Gradients Problem: If resnets are the answer, then what is the question?[J]. arXiv preprint arXiv:1702.08591, 2017.</a></p><p>[4] <a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.</a></p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;0-Introduction&quot;&gt;&lt;a href=&quot;#0-Introduction&quot; class=&quot;headerlink&quot; title=&quot;0 Introduction&quot;&gt;&lt;/a&gt;0 Introduction&lt;/h2&gt;&lt;p&gt;Deep learning researchers have been constructing skyscrapers in recent years. Especially, VGG nets and GoogLeNet have pushed the depths of convolutional networks to the extreme. But questions remain: if time and money aren&amp;#39;t problems, are deeper networks always performing better? Not exactly.&lt;/p&gt;
&lt;p&gt;When residual networks were proposed, researchers around the world was stunned by its depth. &amp;quot;Jesus Christ! Is this a neural network or the Dubai Tower?&amp;quot; But &lt;strong&gt;don&amp;#39;t be afraid!&lt;/strong&gt; These networks are deep but the structures are simple. Interestingly, these networks not only defeated all opponents in the classification, detection, localization challenges in ImageNet 2015, but were also the main innovation in the best paper of CVPR2016.&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;&lt;br&gt;    &lt;img src=&quot;/images/resnet/network_growth.jpg&quot; width=&quot;40%&quot; height=&quot;40%&quot; alt=&quot;Network Growth&quot;&gt;&lt;br&gt;&lt;/div&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://linkinpark213.com/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://linkinpark213.com/tags/Computer-Vision/"/>
    
      <category term="Reviews" scheme="http://linkinpark213.com/tags/Reviews/"/>
    
  </entry>
  
  <entry>
    <title>A Review of VGG net - Very Deep Convolutional Neural Networks</title>
    <link href="http://linkinpark213.com/2018/04/21/vgg/"/>
    <id>http://linkinpark213.com/2018/04/21/vgg/</id>
    <published>2018-04-21T07:15:55.000Z</published>
    <updated>2019-04-21T07:23:02.102Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><h2 id="0-Introduction"><a href="#0-Introduction" class="headerlink" title="0 Introduction"></a>0 Introduction</h2><p>Convolutional neural networks(CNN) have enjoyed great success in computer vision research fields in the past few years. A number of attempts are made based on the original CNN architecture to improve its accuracy and performance. In 2014, Karen Simonyan et al. did an investigation on the effect of depth on CNNs&#39; accuracy in large-scale image recognition (thus also proposing a series of very deep CNNs which are usually called VGG nets). The result confirmed the importance of CNN depth in visual representations.</p><h2 id="1-Background-VGG-net-39-s-ancestors"><a href="#1-Background-VGG-net-39-s-ancestors" class="headerlink" title="1 Background: VGG net&#39;s ancestors"></a>1 Background: VGG net&#39;s ancestors</h2><p>Before introducing VGG net, let&#39;s take a glance at prior convolutional neural networks. </p><h3 id="1-1-LeNet-The-Origin"><a href="#1-1-LeNet-The-Origin" class="headerlink" title="1.1 LeNet: The Origin"></a>1.1 LeNet: The Origin</h3><p>Basic neural network structures(for example, multi-layer perceptron) learn patterns on 1D vectors, which cannot cope with 2D features in images well. In 1986, Lecun et al. proposed a convolution network model called LeNet-5. Its structure is fairly simple: two convolution layers, two subsampling layers and a few fully connected layers. This network was used to solve a number recognition problem. (If you need to learn more about the convolution operation, please refer to Google or <em>Digital Image Processing</em> by Rafael C. Gonzalez)</p><a id="more"></a><div align="center" class="figure"><br>    <img src="/images/vgg/lenet.png" width="80%" height="60%" alt="LeNet"><br><br>    Fig. 1 Architecture of LeNet<br><br></div><h3 id="1-2-AlexNet-The-Powerful-Convolution"><a href="#1-2-AlexNet-The-Powerful-Convolution" class="headerlink" title="1.2 AlexNet: The Powerful Convolution"></a>1.2 AlexNet: The Powerful Convolution</h3><p>In 2012, Alex Krizhevsky et al. won the first place in ILSVRC-2012(ImageNet Large-Scale Visual Recognition Challenge 2012) and achieved the highest top-5 error rate of 15.3% with a convolutional network model, while the second-best entry only achieved 26.2%. The network, namely AlexNet, was trained on two GTX580 3GB GPUs in parallel. Since a single GTX580 GPU has only 3GB memory, the maximum size of networks is limited. This model proved the effectiveness of CNNs under complicated circumstances and the power of GPUs. So what if the network can go deeper? Will the top-5 error rate get even lower?</p><div align="center" class="figure"><br><img src="/images/vgg/alexnet.png" alt="AlexNet"><br><br>Fig. 2 Architecture of AlexNet<br></div><h2 id="2-Main-Contributions-of-VGG-Nets"><a href="#2-Main-Contributions-of-VGG-Nets" class="headerlink" title="2 Main Contributions of VGG Nets"></a>2 Main Contributions of VGG Nets</h2><p>Here comes our hero - VGG nets. By the way, VGG is not the name of the network, but the name of the authors&#39; group - <em>Visual Geometry Group</em>, from Department of Engineering Science, University of Oxford. The networks they proposed were therefore named after the group. The main contributions of VGG nets are: 1. more but smaller convolution filters; 2. great depth of networks.</p><h3 id="2-1-Stacks-of-Smaller-Convolution-Filters"><a href="#2-1-Stacks-of-Smaller-Convolution-Filters" class="headerlink" title="2.1 Stacks of Smaller Convolution Filters"></a>2.1 Stacks of Smaller Convolution Filters</h3><p>Rather than using relatively large receptive fields in the first convolution layers, Simonyan et al. selected very small 3x3 receptive fields throughout the whole net, which are convolved with the input at every pixel with a stride of 1. As is shown in the figures below, a stack of two 3x3 convolution layers has an effective receptive field of 5x5. We can also conclude that a stack of three 3x3 convolution filters has an effective receptive field of 7x7.</p><div align="center" class="figure"><br>    <img src="/images/vgg/conv1.png" width="50%" height="50%" alt="Conv5x5"><br><br>    Fig. 3 A convolution layer with one 5x5 conv. filter has a receptive field of 5x5<br><br>    <img src="/images/vgg/conv2.png" width="60%" height="60%" alt="Conv3x3x2"><br><br>    Fig. 4 A convolution layer stack with two 3x3 conv. filter also has a effective receptive field of 5x5<br><br></div><p>Now that we&#39;re clear that stacks of small-kernel convolution layers have equal sized receptive fields, why are they the better choice? Well, the first advantage is incorporating more rectification layers instead of a single one, since every convolution layer includes an activation function(usually ReLU). More rectification brings more non-linearity, and more non-linearity makes the decision function more discriminative and fit better. Also, when the receptive field isn&#39;t too large, a stack of 3x3 convolution filters have fewer parameters to train. Assuming the number of input and output channels of a convolution layer stack are equal(let&#39;s call it C) and the receptive field is 5x5, we have \(2*3*3*C*C=18C^2\) instead of \(5*5*C*C=25C^2\) parameters here. Similarly, when the receptive field is 7x7, we have \(3*3*3*C*C=27C^2\) instead of \(7*7*C*C=49C^2\). When the field gets even larger? A function with \(O(n)\) complexity only has greater advantage against an \(O(n^2)\) when \(n\) grows.</p><h3 id="2-2-Deep-Dark-Fantasy"><a href="#2-2-Deep-Dark-Fantasy" class="headerlink" title="2.2 Deep Dark Fantasy"></a>2.2 Deep Dark Fantasy</h3><p>Cliches time. Just like any blogger mentioning VGG nets would do, here are the network structures proposed by Simonyan et al.</p><div align="center" class="figure"><br>    <img src="/images/vgg/vggnets.png" width="60%" height="60%" alt="VGG Nets"><br><br>    Table. 1 VGG nets of various depths<br><br></div><p>Look at the table column-by-column. Each column(A, A-LRN, B, C, D, E) corresponds to one network structure. As you can see, their networks grew from 11 layers(in net A) to 19 layers(in net E). Each time something is added to the previous net, it would appear bold. Clearly, LRN(Local Response Normalization) didn&#39;t work well in this case(actually, A-LRN net performed worse than A, while consuming much more memory and computation time), and was thus removed. </p><p>What&#39;s worth mentioning are the 1x1 convolution layers appearing in network C. This is a way to increase non-linearity(also by introducing activation functions) of the decision function while also keeping the size of the receptive fields unchanged.</p><h2 id="3-Training-amp-Evaluation"><a href="#3-Training-amp-Evaluation" class="headerlink" title="3 Training &amp; Evaluation"></a>3 Training &amp; Evaluation</h2><p>Bad initialization could stall learning due to the instability of gradient deep networks. Therefore, the authors first trained the network A, which is shallow enough to be trained with random initialization. Then, the next networks (B to E) are initialized with the pre-trained models, and only weights of the new layers are randomly initialized.</p><p>In spite of the larger number of parameters and the greater depth of our nets compared to AlexNet, the nets required less epochs to converge due to the implicit regularization imposed by greater depth, smaller convolution filter sizes and the pre-initialization of certain layers. They also generalize well to other datasets, achieving state-of-the-art performances. Results of VGG nets in comparison against other models in ILSVRC are shown in the table below.</p><div align="center" class="figure"><br>    <img src="/images/vgg/VGG-performance-comparison.png" width="70%" height="70%" alt="VGG net results"><br><br>    Table. 2 VGG net performance, in comparison with the state of the art in ILSVRC classification<br><br></div><p>In conclusion, the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. </p><p>You might ask: Why not even deeper, with more powerful GPUs(the authors used Titan Black), we can absolutely train deeper networks that perform better! Not exactly. Problems arose as the networks get too deep, and this is where ResNet comes in.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="https://arxiv.org/pdf/1409.1556/" target="_blank" rel="noopener">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.</a></p>]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;0-Introduction&quot;&gt;&lt;a href=&quot;#0-Introduction&quot; class=&quot;headerlink&quot; title=&quot;0 Introduction&quot;&gt;&lt;/a&gt;0 Introduction&lt;/h2&gt;&lt;p&gt;Convolutional neural networks(CNN) have enjoyed great success in computer vision research fields in the past few years. A number of attempts are made based on the original CNN architecture to improve its accuracy and performance. In 2014, Karen Simonyan et al. did an investigation on the effect of depth on CNNs&amp;#39; accuracy in large-scale image recognition (thus also proposing a series of very deep CNNs which are usually called VGG nets). The result confirmed the importance of CNN depth in visual representations.&lt;/p&gt;
&lt;h2 id=&quot;1-Background-VGG-net-39-s-ancestors&quot;&gt;&lt;a href=&quot;#1-Background-VGG-net-39-s-ancestors&quot; class=&quot;headerlink&quot; title=&quot;1 Background: VGG net&amp;#39;s ancestors&quot;&gt;&lt;/a&gt;1 Background: VGG net&amp;#39;s ancestors&lt;/h2&gt;&lt;p&gt;Before introducing VGG net, let&amp;#39;s take a glance at prior convolutional neural networks. &lt;/p&gt;
&lt;h3 id=&quot;1-1-LeNet-The-Origin&quot;&gt;&lt;a href=&quot;#1-1-LeNet-The-Origin&quot; class=&quot;headerlink&quot; title=&quot;1.1 LeNet: The Origin&quot;&gt;&lt;/a&gt;1.1 LeNet: The Origin&lt;/h3&gt;&lt;p&gt;Basic neural network structures(for example, multi-layer perceptron) learn patterns on 1D vectors, which cannot cope with 2D features in images well. In 1986, Lecun et al. proposed a convolution network model called LeNet-5. Its structure is fairly simple: two convolution layers, two subsampling layers and a few fully connected layers. This network was used to solve a number recognition problem. (If you need to learn more about the convolution operation, please refer to Google or &lt;em&gt;Digital Image Processing&lt;/em&gt; by Rafael C. Gonzalez)&lt;/p&gt;
    
    </summary>
    
    
      <category term="Deep Learning" scheme="http://linkinpark213.com/tags/Deep-Learning/"/>
    
      <category term="Computer Vision" scheme="http://linkinpark213.com/tags/Computer-Vision/"/>
    
      <category term="Reviews" scheme="http://linkinpark213.com/tags/Reviews/"/>
    
  </entry>
  
  <entry>
    <title>Smartypants is NOT SO SMART</title>
    <link href="http://linkinpark213.com/2018/03/20/smartypants/"/>
    <id>http://linkinpark213.com/2018/03/20/smartypants/</id>
    <published>2018-03-20T02:00:22.000Z</published>
    <updated>2018-11-18T05:44:45.857Z</updated>
    
    <content type="html"><![CDATA[<p>When blogging with Hexo, every time I type a single quotation mark(also called apostrophe) like this:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;</span><br></pre></td></tr></table></figure></p><p>, Hexo would convert it to a symbol like this<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">â€™</span><br></pre></td></tr></table></figure></p><p>You would say that this is also an apotrophe, but it really looks UNBEARABLE in the articles. It&#39;s been a problem bothering me for more than a month.(I&#39;m not saying that this is the reason for not updating my blog, but I don&#39;t mind if you think so!)</p><a id="more"></a><p><img src="/images/smartypants/apostrophe.png" alt="apostrophe"></p><p>Therefore, I Googled about this problem and tried to find other victims. According to their sharings, this problem is caused by <em>marked</em> -- the default markdown renderer of Hexo.The <em>&quot;smatrypants&quot;</em> function of marked was turned on by default.</p><p>Now take a look at the introduction of <em>smartypants</em> on the <em>hexo-renderer-marked</em> page:</p><blockquote><p><em>smartypants</em> - Use &quot;smart&quot; typograhic punctuation for things like quotes and dashes.</p></blockquote><p>C&#39;mon, seriously? </p><p>There are a few bloggers who solved this by adding the code below to the _config.yml file in the blog directory. </p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">mark:</span></span><br><span class="line"><span class="attr">  smartypants:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>This worked for most victims (perhaps all of them), but not for me. I have no idea why those config wasn&#39;t working, so if anyone finds out the reason, please contact me by e-mail.</p><p>If you&#39;re sure <em>smartypants</em> is causing the problem, and the solution above didn&#39;t work for you either, maybe you can try my solution.</p><p>Since <em>hexo-renderer-marked</em> is installed in the blog&#39;s <em>node_modules</em> directory(may also be in your Node.js directory if installed globally), isn&#39;t it possible that we change its own configurations? I looked at the <em>index.js</em> file in the <em>node_modules/hexo-renderer-marked/</em> directory. There you are, smartypants!</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hexo.config.marked = assign(&#123;</span><br><span class="line">  gfm: <span class="literal">true</span>,</span><br><span class="line">  pedantic: <span class="literal">false</span>,</span><br><span class="line">  sanitize: <span class="literal">false</span>,</span><br><span class="line">  tables: <span class="literal">true</span>,</span><br><span class="line">  breaks: <span class="literal">true</span>,</span><br><span class="line">  smartLists: <span class="literal">true</span>,</span><br><span class="line">  smartypants: <span class="literal">true</span>,</span><br><span class="line">  modifyAnchors: <span class="string">''</span>,</span><br><span class="line">  autolink: <span class="literal">true</span></span><br><span class="line">&#125;, hexo.config.marked);</span><br></pre></td></tr></table></figure><p>Now you know what to do.</p><p>Aaaaaaaaaaaaaaaand many thanks to Xizi Wu, the artist of my new avatar! I love it!<br><img src="/images/long_nobg.png" alt="Harper Long by Xizi Wu"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;When blogging with Hexo, every time I type a single quotation mark(also called apostrophe) like this:&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;apos;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;, Hexo would convert it to a symbol like this&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;â€™&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;You would say that this is also an apotrophe, but it really looks UNBEARABLE in the articles. It&amp;#39;s been a problem bothering me for more than a month.(I&amp;#39;m not saying that this is the reason for not updating my blog, but I don&amp;#39;t mind if you think so!)&lt;/p&gt;
    
    </summary>
    
    
      <category term="Blogging" scheme="http://linkinpark213.com/tags/Blogging/"/>
    
  </entry>
  
  <entry>
    <title>Cheers for the 8th International linkinpark213 Day!</title>
    <link href="http://linkinpark213.com/2018/02/13/linkinpark213-day/"/>
    <id>http://linkinpark213.com/2018/02/13/linkinpark213-day/</id>
    <published>2018-02-13T09:40:21.000Z</published>
    <updated>2018-11-18T06:04:41.608Z</updated>
    
    <content type="html"><![CDATA[<p>Cheers for the 8th International linkinpark213 Day!</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="string">'Harper'</span> <span class="string">'Sweet'</span> <span class="string">'Kobayashi'</span> <span class="string">'Kawasaki'</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"I'm <span class="variable">$i</span>, cheers!"</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h2 id="What-is-linkinpark213-Day"><a href="#What-is-linkinpark213-Day" class="headerlink" title="What is linkinpark213 Day?"></a>What is linkinpark213 Day?</h2><p>International linkinpark213 Day is a global anniversary set up by Harper Long in 2011 A.D., celebrated on February 13th every year. This anniversary is officially written as &#39;linkinpark213 Day&#39;, the first letter of which is a lower-case. The establishment of the anniversary dates back to the early 10s in the 21st century.</p><p>Till now, the population for this anniversary has already reached 1e-6 million, and the distribution also expanded from a small county to the whole middle-China area, including Hebei, Henan, Shanxi and Shaanxi province. There will also be some Japanese resident who plan to celebrate this day in 2019.</p><a id="more"></a><h2 id="Why-13th-Feb"><a href="#Why-13th-Feb" class="headerlink" title="Why 13th Feb?"></a>Why 13th Feb?</h2><p>According to the modern Chinese habit of writing, &#39;13th Feb&#39; is usually written as &#39;2.13&#39;. Also, &#39;13&#39; and &#39;B&#39; look similar and are often regarded to be equal. In conclusion, &#39;13 Feb&#39; can be transformed to &#39;2B&#39;, which is a common word in Chinese. Although the word is sometimes classified as &quot;offensive&quot;, it reflects feelings of optimism, bravery and entertainment.</p><h2 id="How-do-people-celebrate-the-day"><a href="#How-do-people-celebrate-the-day" class="headerlink" title="How do people celebrate the day?"></a>How do people celebrate the day?</h2><p>When the anniversary was first set up, there was no officially specified ways of celebration. People gathered, held parties and enjoyed spending time together. </p><p>On the 3th linkinpark213 Day, a proposal by a high school Chinese teacher was taken as the official way of celebration -- on each linkinpark213 Day, a number of people participate in the Pigeon-Flying Competition funded by Harper Long. This way of celebration prevailed until now, and all participants except Harper Long won the competition every year.</p><p>Pigeon-Flying is a broadly-accepted traditional Chinese custom, the exact origin of which is too ancient to be revealed. Modern scholars tend to believe that this custom became well-known no later than 206 A.D. in China. In modern times, pigeon-flying is an activity involving &quot;making a promise&quot; and &quot;not keeping it&quot;. According to some folk stories, this activity was firstly named when it happened to a pigeon keeper when he forgot to keep a promise that he made with a friend. </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Promise promise = <span class="keyword">new</span> Promise();</span><br><span class="line">    promise.make(<span class="string">"I will come!"</span>);</span><br><span class="line">    System.exit(<span class="number">0</span>);</span><br><span class="line">    promise.keep();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>(Please notice that pigeon-flying mentioned here is not the same activity as what happens every Olympics since 1896 A.D.)</p><p>Join our celebration today! You can easily participate in the Pigeon-Flying Competition by not participating.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Cheers for the 8th International linkinpark213 Day!&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;Harper&#39;&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;Sweet&#39;&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;Kobayashi&#39;&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&#39;Kawasaki&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;do&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;I&#39;m &lt;span class=&quot;variable&quot;&gt;$i&lt;/span&gt;, cheers!&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;done&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;What-is-linkinpark213-Day&quot;&gt;&lt;a href=&quot;#What-is-linkinpark213-Day&quot; class=&quot;headerlink&quot; title=&quot;What is linkinpark213 Day?&quot;&gt;&lt;/a&gt;What is linkinpark213 Day?&lt;/h2&gt;&lt;p&gt;International linkinpark213 Day is a global anniversary set up by Harper Long in 2011 A.D., celebrated on February 13th every year. This anniversary is officially written as &amp;#39;linkinpark213 Day&amp;#39;, the first letter of which is a lower-case. The establishment of the anniversary dates back to the early 10s in the 21st century.&lt;/p&gt;
&lt;p&gt;Till now, the population for this anniversary has already reached 1e-6 million, and the distribution also expanded from a small county to the whole middle-China area, including Hebei, Henan, Shanxi and Shaanxi province. There will also be some Japanese resident who plan to celebrate this day in 2019.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Other" scheme="http://linkinpark213.com/tags/Other/"/>
    
  </entry>
  
</feed>
